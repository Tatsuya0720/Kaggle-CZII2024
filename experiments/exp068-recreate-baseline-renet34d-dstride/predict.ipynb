{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from metric import visualize_epoch_results\n",
    "from src.utils import save_images\n",
    "from src.metric import score, create_cls_pos, create_cls_pos_sikii, create_df\n",
    "from src.inference import inference, inference2pos\n",
    "from src.kaggle_notebook_metric import compute_lb\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../inputs/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TS_73_6</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>268.662</td>\n",
       "      <td>4730.318</td>\n",
       "      <td>916.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TS_73_6</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>238.946</td>\n",
       "      <td>4853.061</td>\n",
       "      <td>909.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TS_73_6</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>83.114</td>\n",
       "      <td>5729.560</td>\n",
       "      <td>1219.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TS_73_6</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>582.143</td>\n",
       "      <td>2769.968</td>\n",
       "      <td>1076.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TS_73_6</td>\n",
       "      <td>apo-ferritin</td>\n",
       "      <td>510.389</td>\n",
       "      <td>2157.244</td>\n",
       "      <td>362.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>1124</td>\n",
       "      <td>TS_6_6</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2609.876</td>\n",
       "      <td>4569.876</td>\n",
       "      <td>1169.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>1125</td>\n",
       "      <td>TS_6_6</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>2213.287</td>\n",
       "      <td>4135.017</td>\n",
       "      <td>1286.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>1126</td>\n",
       "      <td>TS_6_6</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>3303.905</td>\n",
       "      <td>5697.825</td>\n",
       "      <td>789.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>1127</td>\n",
       "      <td>TS_6_6</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>1008.748</td>\n",
       "      <td>5949.213</td>\n",
       "      <td>1077.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>1128</td>\n",
       "      <td>TS_6_6</td>\n",
       "      <td>virus-like-particle</td>\n",
       "      <td>5749.052</td>\n",
       "      <td>3911.392</td>\n",
       "      <td>275.342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1129 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index experiment        particle_type         x         y         z\n",
       "0         0    TS_73_6         apo-ferritin   268.662  4730.318   916.115\n",
       "1         1    TS_73_6         apo-ferritin   238.946  4853.061   909.898\n",
       "2         2    TS_73_6         apo-ferritin    83.114  5729.560  1219.524\n",
       "3         3    TS_73_6         apo-ferritin   582.143  2769.968  1076.364\n",
       "4         4    TS_73_6         apo-ferritin   510.389  2157.244   362.438\n",
       "...     ...        ...                  ...       ...       ...       ...\n",
       "1124   1124     TS_6_6  virus-like-particle  2609.876  4569.876  1169.759\n",
       "1125   1125     TS_6_6  virus-like-particle  2213.287  4135.017  1286.851\n",
       "1126   1126     TS_6_6  virus-like-particle  3303.905  5697.825   789.744\n",
       "1127   1127     TS_6_6  virus-like-particle  1008.748  5949.213  1077.303\n",
       "1128   1128     TS_6_6  virus-like-particle  5749.052  3911.392   275.342\n",
       "\n",
       "[1129 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_gt_df(base_dir, exp_names):\n",
    "    result_df = None\n",
    "    particle_names = CFG.particles_name\n",
    "\n",
    "    for exp_name in exp_names:\n",
    "        for particle in particle_names:\n",
    "            np_corrds = read_info_json(\n",
    "                base_dir=base_dir, exp_name=exp_name, particle_name=particle\n",
    "            )  # (n, 3)\n",
    "            # 各行にexp_nameとparticle_name追加\n",
    "            particle_df = pd.DataFrame(np_corrds, columns=[\"z\", \"y\", \"x\"])\n",
    "            particle_df[\"experiment\"] = exp_name\n",
    "            particle_df[\"particle_type\"] = particle\n",
    "\n",
    "            if result_df is None:\n",
    "                result_df = particle_df\n",
    "            else:\n",
    "                result_df = pd.concat([result_df, particle_df], axis=0).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "    result_df = result_df.reset_index()\n",
    "    result_df = result_df[[\"index\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", CFG.train_exp_names)\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "def combine_models(\n",
    "    models: list[torch.nn.Module],\n",
    "    model_weights: list[float],\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    How:\n",
    "        モデルのパラメータを、渡されたそれぞれの重みに応じて重み付き平均して1つのモデルにまとめる。\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    models : list[torch.nn.Module]\n",
    "        重み付き平均したいモデルたち。\n",
    "    model_weights : list[float]\n",
    "        上記モデルたちに対応した重み。\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        重み付き平均されたモデル。\n",
    "    \"\"\"\n",
    "    # Why not: model_weightsの長さとmodelsの数が一致しない場合はエラーを出す以外に、\n",
    "    #         例外を投げる方法もあるが、そのままにする。\n",
    "    assert len(models) == len(model_weights), \"モデルと重みの数が一致していません。\"\n",
    "\n",
    "    # Why not: PyTorchのModuleを新規に作り直す方法もあるが、最初のモデルをコピーして\n",
    "    #         そこに重みを上書きする形が簡単。\n",
    "    # ここでは models[0] のstate_dictをコピーして初期化し、それに各モデルを加算していく\n",
    "    base_state_dict = models[0].state_dict()\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    # 合計重み(正規化のため)を計算\n",
    "    weight_sum = sum(model_weights)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for key in base_state_dict.keys():\n",
    "            # まず最初のモデルのパラメータ*重み で初期化\n",
    "            avg_param = base_state_dict[key] * model_weights[0]\n",
    "\n",
    "            # 残りのモデルを加算\n",
    "            for i in range(1, len(models)):\n",
    "                avg_param += models[i].state_dict()[key] * model_weights[i]\n",
    "\n",
    "            # 重み和で割って正規化\n",
    "            avg_param /= weight_sum\n",
    "            new_state_dict[key] = avg_param\n",
    "\n",
    "    # 新しいモデル(同じアーキテクチャ)を作り、new_state_dictをロード\n",
    "    # ここでは例としてmodels[0]と同じクラスを使う\n",
    "    combined_model = type(models[0])(\n",
    "        encoder=models[0].encoder,  # Unet3Dのコンストラクタ引数を流用\n",
    "        num_domains=5,\n",
    "    )\n",
    "    combined_model.load_state_dict(new_state_dict)\n",
    "\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# 例としてtimmのエンコーダを作成\n",
    "# (Unet3Dはユーザー定義クラスで、同一クラス・同一初期化が必要)\n",
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model01 = Unet3D(encoder=encoder, num_domains=5)\n",
    "model02 = Unet3D(encoder=encoder, num_domains=5)\n",
    "model03 = Unet3D(encoder=encoder, num_domains=5)\n",
    "model04 = Unet3D(encoder=encoder, num_domains=5)\n",
    "model05 = Unet3D(encoder=encoder, num_domains=5)\n",
    "\n",
    "# モデルのロード\n",
    "model01.load_state_dict(torch.load(\"./TS_6_4/best_model.pth\"))\n",
    "model02.load_state_dict(torch.load(\"./TS_6_6/best_model.pth\"))\n",
    "model03.load_state_dict(torch.load(\"./TS_73_6/best_model.pth\"))\n",
    "model04.load_state_dict(torch.load(\"./TS_86_3/best_model.pth\"))\n",
    "model05.load_state_dict(torch.load(\"./TS_99_9/best_model.pth\"))\n",
    "\n",
    "# 各モデルに対する重み\n",
    "model01_m = 1.0\n",
    "model02_m = 1.0  # 01\n",
    "model03_m = 1.0  # 01\n",
    "model04_m = 1.0  # 01\n",
    "model05_m = 1.0  # 01\n",
    "\n",
    "# モデルをまとめる\n",
    "models = [model01, model02, model03, model04, model05]\n",
    "model_weights = [model01_m, model02_m, model03_m, model04_m, model05_m]\n",
    "\n",
    "combined_model = combine_models(models, model_weights).cuda()\n",
    "torch.save(combined_model.state_dict(), \"./combined_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = combined_model\n",
    "model = model01.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model soup\n",
    "# import timm\n",
    "\n",
    "\n",
    "# def copy_params(model1, model2):\n",
    "#     model1.load_state_dict(model2.state_dict())\n",
    "#     return model1\n",
    "\n",
    "\n",
    "# def sum_model_params(modelA, modelB):\n",
    "#     \"\"\"modelA + modelB\"\"\"\n",
    "#     sdA = modelA.state_dict()\n",
    "#     sdB = modelB.state_dict()\n",
    "#     for key in sdA:\n",
    "#         sdA[key] = sdA[key] + sdB[key]\n",
    "#     modelA.load_state_dict(sdA)\n",
    "#     return modelA\n",
    "\n",
    "\n",
    "# def multi_model_params(model, a):\n",
    "#     \"\"\"a * model\"\"\"\n",
    "#     sd = model.state_dict()\n",
    "#     for key in sd:\n",
    "#         sd[key] = sd[key] * a\n",
    "#     model.load_state_dict(sd)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# model_name_paths = [\n",
    "#     \"./TS_6_4/best_model.pth\",\n",
    "#     \"./TS_6_6/best_model.pth\",\n",
    "#     # \"./TS_69_2/best_model.pth\",\n",
    "#     \"./TS_73_6/best_model.pth\",\n",
    "#     \"./TS_86_3/best_model.pth\",\n",
    "#     \"./TS_99_9/best_model.pth\",\n",
    "# ]\n",
    "\n",
    "# encoder = timm.create_model(\n",
    "#     model_name=CFG.model_name,\n",
    "#     pretrained=True,\n",
    "#     in_chans=3,\n",
    "#     num_classes=0,\n",
    "#     global_pool=\"\",\n",
    "#     features_only=True,\n",
    "# )\n",
    "\n",
    "# model01 = Unet3D(encoder=encoder, num_domains=5)\n",
    "# model02 = Unet3D(encoder=encoder, num_domains=5)\n",
    "# model03 = Unet3D(encoder=encoder, num_domains=5)\n",
    "# model04 = Unet3D(encoder=encoder, num_domains=5)\n",
    "# model05 = Unet3D(encoder=encoder, num_domains=5)\n",
    "\n",
    "# model01_m = 0\n",
    "# model02_m = 0\n",
    "# model03_m = 10\n",
    "# model04_m = 2\n",
    "# model05_m = 0\n",
    "\n",
    "# model01.load_state_dict(torch.load(model_name_paths[0]))\n",
    "# model02.load_state_dict(torch.load(model_name_paths[1]))\n",
    "# model03.load_state_dict(torch.load(model_name_paths[2]))\n",
    "# model04.load_state_dict(torch.load(model_name_paths[3]))\n",
    "# model05.load_state_dict(torch.load(model_name_paths[4]))\n",
    "\n",
    "# model01 = multi_model_params(model01, model01_m)\n",
    "# model02 = multi_model_params(model02, model02_m)\n",
    "# model03 = multi_model_params(model03, model03_m)\n",
    "# model04 = multi_model_params(model04, model04_m)\n",
    "# model05 = multi_model_params(model05, model05_m)\n",
    "\n",
    "# model01 = sum_model_params(model01, model02)\n",
    "# model01 = sum_model_params(model01, model03)\n",
    "# model01 = sum_model_params(model01, model04)\n",
    "# model01 = sum_model_params(model01, model05)\n",
    "# model = multi_model_params(\n",
    "#     model01, 1 / (model01_m + model02_m + model03_m + model04_m + model05_m)\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# # del model01, model02, model03, model04  # , model05\n",
    "\n",
    "# # model = multi_model_params(model, 1 / len(model_name_paths)).to(\"cuda\")\n",
    "# # model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "\n",
    "\n",
    "# # test\n",
    "# # model.load_state_dict(torch.load(\"./TS_69_2/best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model03.load_state_dict(torch.load(model_name_paths[2]))\n",
    "# model03 = multi_model_params(model03, model03_m)\n",
    "# model03.encoder.layer1[0].conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# encoder = timm.create_model(\n",
    "#     model_name=CFG.model_name,\n",
    "#     pretrained=True,\n",
    "#     in_chans=3,\n",
    "#     num_classes=0,\n",
    "#     global_pool=\"\",\n",
    "#     features_only=True,\n",
    "# )\n",
    "# model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "# model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "# inferenced_array = inference(model, exp_name, train=False)\n",
    "# 0.7303962244998289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "exp_names = CFG.valid_exp_names  # [\"TS_6_4\", \"TS_5_4\", \"TS_69_2\"]\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "constant = 0.25\n",
    "sikii = {\n",
    "    \"apo-ferritin\": constant,\n",
    "    \"beta-amylase\": constant,\n",
    "    \"beta-galactosidase\": constant,\n",
    "    \"ribosome\": constant,\n",
    "    \"thyroglobulin\": constant,\n",
    "    \"virus-like-particle\": constant,\n",
    "}\n",
    "\n",
    "pred_dict = {}\n",
    "\n",
    "# for exp_name in tqdm(CFG.train_exp_names):\n",
    "for exp_name in tqdm(exp_names):  # 5つのデータで試す\n",
    "    # inferenced_array = inference(model, exp_name, train=False)\n",
    "    inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "        model,\n",
    "        exp_name,\n",
    "        train=False,\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "    )\n",
    "    pred_dict[exp_name] = inferenced_array\n",
    "    # pred_df = inference2pos(\n",
    "    #     pred_segmask=inferenced_array, exp_name=exp_name, sikii_dict=sikii\n",
    "    # )\n",
    "\n",
    "    # all_pred.append(pred_df)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df = pd.concat(all_pred, axis=0).reset_index(drop=True)\n",
    "# pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "# pred_df = pred_df.drop_duplicates(\n",
    "#     subset=[\"experiment\", \"x\", \"y\", \"z\"], keep=\"first\"\n",
    "# ).reset_index(drop=True)\n",
    "# pred_df = pred_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "# pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", exp_names)\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sikii値とexp_namesを入れるとスコアを出力する関数\n",
    "\n",
    "\n",
    "def compute_score(sikii_list, inferenced_array, exp_name):\n",
    "    apo_ferritin = sikii_list[0]\n",
    "    beta_amylase = sikii_list[1]\n",
    "    beta_galactosidase = sikii_list[2]\n",
    "    ribosome = sikii_list[3]\n",
    "    thyroglobulin = sikii_list[4]\n",
    "    virus_like_particle = sikii_list[5]\n",
    "\n",
    "    sikii_dict = {\n",
    "        \"apo-ferritin\": apo_ferritin,\n",
    "        \"beta-amylase\": beta_amylase,\n",
    "        \"beta-galactosidase\": beta_galactosidase,\n",
    "        \"ribosome\": ribosome,\n",
    "        \"thyroglobulin\": thyroglobulin,\n",
    "        \"virus-like-particle\": virus_like_particle,\n",
    "    }\n",
    "\n",
    "    all_pred = []\n",
    "\n",
    "    pred_df = inference2pos(\n",
    "        pred_segmask=inferenced_array, exp_name=exp_name, sikii_dict=sikii_dict\n",
    "    )\n",
    "\n",
    "    all_pred.append(pred_df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df = pd.concat(all_pred, axis=0).reset_index(drop=True)\n",
    "    pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(\n",
    "        subset=[\"experiment\", \"x\", \"y\", \"z\"], keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    pred_df = pred_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", [exp_name])\n",
    "\n",
    "    result_df, lb_score = compute_lb(\n",
    "        pred_df, \"../../inputs/train/overlay/ExperimentRuns/\", [exp_name]\n",
    "    )\n",
    "\n",
    "    return lb_score\n",
    "\n",
    "\n",
    "def reduce_computation_sikii_search(\n",
    "    inferenced_array: np.ndarray, exp_name: str, threshold_candidates: list[float]\n",
    ") -> tuple[list[float], float]:\n",
    "    \"\"\"\n",
    "    # How\n",
    "    6つのしきい値が互いに独立してスコアに貢献しているという前提で、\n",
    "    1次元ずつ最適なしきい値を探す手法を実装する.\n",
    "\n",
    "    1. 初期の best_thresholds (全要素 0.5 など適当な値) を用意\n",
    "    2. i=0 から i=5 まで順番に:\n",
    "       - threshold_candidates をすべて試し、他は固定したまま i 番目だけ変化させてスコアを計算\n",
    "       - 最良スコアが得られる候補値を確定し、best_thresholds[i] とする\n",
    "    3. 全部決まったら最終的なスコアを計算して返す\n",
    "\n",
    "    これにより、全組み合わせ (product) を回すよりも計算量が大幅に減少する.\n",
    "    \"\"\"\n",
    "    # Why not: 6値独立であるという前提が満たされていない場合、近似解になる可能性あり\n",
    "    best_thresholds = [0.5] * 6  # 適当な初期値でOK\n",
    "\n",
    "    for i in tqdm(range(6)):\n",
    "        best_local_score = -float(\"inf\")\n",
    "        best_local_value = None\n",
    "\n",
    "        for candidate in threshold_candidates:\n",
    "            current_thresholds = best_thresholds[:]  # 現在のベストを複製\n",
    "            current_thresholds[i] = candidate\n",
    "            try:\n",
    "                score = compute_score(current_thresholds, inferenced_array, exp_name)\n",
    "            except:\n",
    "                score = -float(\"inf\")\n",
    "            if score > best_local_score:\n",
    "                best_local_score = score\n",
    "                best_local_value = candidate\n",
    "\n",
    "        # i番目のしきい値を最適値に更新\n",
    "        best_thresholds[i] = best_local_value\n",
    "\n",
    "    final_score = compute_score(best_thresholds, inferenced_array, exp_name)\n",
    "    return best_thresholds, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_score([0.25, 0.25, 0.25, 0.25, 0.25, 0.25], exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均値\n",
    "from sklearn.metrics import *\n",
    "from scipy.optimize import minimize\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "exp_name = CFG.valid_exp_names[0]\n",
    "\n",
    "inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "    model,\n",
    "    exp_name,\n",
    "    train=False,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmx(x):\n",
    "    # x: (cls, depth, height, width)\n",
    "    x = np.exp(x)\n",
    "    x = x / x.sum(axis=0)\n",
    "    return x\n",
    "\n",
    "\n",
    "inferenced_array = softmx(inferenced_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KappaOPtimizer = minimize(\n",
    "#     compute_score,\n",
    "#     x0=[0.90, 0.90, 0.90, 0.90, 0.90, 0.90],\n",
    "#     args=(inferenced_array, exp_name),\n",
    "#     bounds=[(0.0, 0.95) for _ in range(6)],\n",
    "#     method=\"nelder-mead\",\n",
    "#     options={\"maxiter\": 50},\n",
    "# )\n",
    "\n",
    "\n",
    "best_thresholds, final_score = reduce_computation_sikii_search(\n",
    "    inferenced_array,\n",
    "    exp_name,\n",
    "    [\n",
    "        0.05,\n",
    "        0.1,\n",
    "        0.15,\n",
    "        0.2,\n",
    "        0.25,\n",
    "        0.3,\n",
    "        0.35,\n",
    "        0.4,\n",
    "        0.45,\n",
    "        0.5,\n",
    "        0.55,\n",
    "        0.6,\n",
    "        0.65,\n",
    "        0.7,\n",
    "        0.75,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 130\n",
    "cls = 5\n",
    "# inferenced_array[cls, depth]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(inferenced_array[cls, depth], cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 121\n",
    "cls = 5\n",
    "# inferenced_array[cls, depth]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(inferenced_array[cls, depth], cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_score(best_thresholds, inferenced_array, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apo_ferritin = best_thresholds[0]\n",
    "beta_amylase = best_thresholds[1]\n",
    "beta_galactosidase = best_thresholds[2]\n",
    "ribosome = best_thresholds[3]\n",
    "thyroglobulin = best_thresholds[4]\n",
    "virus_like_particle = best_thresholds[5]\n",
    "\n",
    "sikii_dict = {\n",
    "    \"apo-ferritin\": apo_ferritin,\n",
    "    \"beta-amylase\": beta_amylase,\n",
    "    \"beta-galactosidase\": beta_galactosidase,\n",
    "    \"ribosome\": ribosome,\n",
    "    \"thyroglobulin\": thyroglobulin,\n",
    "    \"virus-like-particle\": virus_like_particle,\n",
    "}\n",
    "\n",
    "sikii_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
