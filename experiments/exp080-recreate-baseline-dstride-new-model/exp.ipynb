{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from src.kaggle_notebook_metric import compute_lb, extract_particle_results\n",
    "from src.inference import inference, inference2pos, create_gt_df\n",
    "from metric import visualize_epoch_results\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_name = os.path.join(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "param = {\n",
    "    \"model\": CFG.model_name,\n",
    "    \"resolution\": CFG.resolution,\n",
    "    \"augmentation_prob\": CFG.augmentation_prob,\n",
    "    \"slice\": CFG.slice_,\n",
    "    \"epochs\": CFG.epochs,\n",
    "    \"lr\": CFG.lr,\n",
    "    \"batch_size\": CFG.batch_size,\n",
    "    \"weight_decay\": CFG.weight_decay,\n",
    "    \"num_workers\": CFG.num_workers,\n",
    "    \"augment_data_ratio\": CFG.augment_data_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# sikii値とexp_namesを入れるとスコアを出力する関数\n",
    "\n",
    "\n",
    "def compute_score(sikii_list, inferenced_array, exp_name):\n",
    "    apo_ferritin = sikii_list[0]\n",
    "    beta_amylase = sikii_list[1]\n",
    "    beta_galactosidase = sikii_list[2]\n",
    "    ribosome = sikii_list[3]\n",
    "    thyroglobulin = sikii_list[4]\n",
    "    virus_like_particle = sikii_list[5]\n",
    "\n",
    "    sikii_dict = {\n",
    "        \"apo-ferritin\": apo_ferritin,\n",
    "        \"beta-amylase\": beta_amylase,\n",
    "        \"beta-galactosidase\": beta_galactosidase,\n",
    "        \"ribosome\": ribosome,\n",
    "        \"thyroglobulin\": thyroglobulin,\n",
    "        \"virus-like-particle\": virus_like_particle,\n",
    "    }\n",
    "\n",
    "    all_pred = []\n",
    "\n",
    "    pred_df = inference2pos(\n",
    "        pred_segmask=inferenced_array, exp_name=exp_name, sikii_dict=sikii_dict\n",
    "    )\n",
    "\n",
    "    all_pred.append(pred_df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df = pd.concat(all_pred, axis=0).reset_index(drop=True)\n",
    "    pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(\n",
    "        subset=[\"experiment\", \"x\", \"y\", \"z\"], keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    pred_df = pred_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", [exp_name])\n",
    "\n",
    "    result_df, lb_score = compute_lb(\n",
    "        pred_df, \"../../inputs/train/overlay/ExperimentRuns/\", [exp_name]\n",
    "    )\n",
    "\n",
    "    return lb_score\n",
    "\n",
    "\n",
    "def reduce_computation_sikii_search(\n",
    "    inferenced_array: np.ndarray, exp_name: str, threshold_candidates: list[float]\n",
    ") -> tuple[list[float], float]:\n",
    "    \"\"\"\n",
    "    # How\n",
    "    6つのしきい値が互いに独立してスコアに貢献しているという前提で、\n",
    "    1次元ずつ最適なしきい値を探す手法を実装する.\n",
    "\n",
    "    1. 初期の best_thresholds (全要素 0.5 など適当な値) を用意\n",
    "    2. i=0 から i=5 まで順番に:\n",
    "       - threshold_candidates をすべて試し、他は固定したまま i 番目だけ変化させてスコアを計算\n",
    "       - 最良スコアが得られる候補値を確定し、best_thresholds[i] とする\n",
    "    3. 全部決まったら最終的なスコアを計算して返す\n",
    "\n",
    "    これにより、全組み合わせ (product) を回すよりも計算量が大幅に減少する.\n",
    "    \"\"\"\n",
    "    # Why not: 6値独立であるという前提が満たされていない場合、近似解になる可能性あり\n",
    "    best_thresholds = [0.5] * 6  # 適当な初期値でOK\n",
    "\n",
    "    for i in tqdm(range(6)):\n",
    "        best_local_score = -float(\"inf\")\n",
    "        best_local_value = None\n",
    "\n",
    "        for candidate in threshold_candidates:\n",
    "            current_thresholds = best_thresholds[:]  # 現在のベストを複製\n",
    "            current_thresholds[i] = candidate\n",
    "            score = compute_score(current_thresholds, inferenced_array, exp_name)\n",
    "            if score > best_local_score:\n",
    "                best_local_score = score\n",
    "                best_local_value = candidate\n",
    "\n",
    "        # i番目のしきい値を最適値に更新\n",
    "        best_thresholds[i] = best_local_value\n",
    "\n",
    "    final_score = compute_score(best_thresholds, inferenced_array, exp_name)\n",
    "    return best_thresholds, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([6, 16, 320, 320])\n",
      "Augmented shape: torch.Size([6, 16, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# 回転\n",
    "# 3Dテンソルの各軸に対して指定した角度で回転する関数\n",
    "def rotate_3d(tomogram, segmentation_map, angle):\n",
    "    \"\"\"Rotates the 3D tensors tomogram and segmentation_map around the Z-axis.\"\"\"\n",
    "    rotated_tomogram = TF.rotate(tomogram, angle, expand=False)\n",
    "    rotated_segmentation_map = TF.rotate(segmentation_map, angle, expand=False)\n",
    "    return rotated_tomogram, rotated_segmentation_map\n",
    "\n",
    "\n",
    "# 平行移動\n",
    "# 指定された範囲でランダムに平行移動\n",
    "def translate_3d(tomogram, segmentation_map, max_shift):\n",
    "    \"\"\"Translates the 3D tensors by a random shift within max_shift.\"\"\"\n",
    "    shift_x = random.randint(-max_shift, max_shift)\n",
    "    shift_y = random.randint(-max_shift, max_shift)\n",
    "    translated_tomogram = TF.affine(\n",
    "        tomogram, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    translated_segmentation_map = TF.affine(\n",
    "        segmentation_map, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    return translated_tomogram, translated_segmentation_map\n",
    "\n",
    "\n",
    "# フリップ\n",
    "# 縦横（上下左右）ランダムフリップ\n",
    "def flip_3d(tomogram, segmentation_map):\n",
    "    \"\"\"Randomly flips the 3D tensors along height or width.\"\"\"\n",
    "    if random.random() > 0.5:  # Horizontal flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-1])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-1])\n",
    "    if random.random() > 0.5:  # Vertical flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-2])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-2])\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# クロッピング\n",
    "# 入力テンソルを中心またはランダムクロップで切り取る\n",
    "def crop_3d(tomogram, segmentation_map, crop_size):\n",
    "    \"\"\"Crops the 3D tensors to the specified crop_size.\"\"\"\n",
    "    _, depth, height, width = tomogram.size()\n",
    "    crop_d, crop_h, crop_w = crop_size\n",
    "\n",
    "    if crop_h > height or crop_w > width:\n",
    "        raise ValueError(\"Crop size cannot be larger than the original size.\")\n",
    "\n",
    "    start_h = random.randint(0, height - crop_h)  # Random starting position for height\n",
    "    start_w = random.randint(0, width - crop_w)  # Random starting position for width\n",
    "\n",
    "    cropped_tomogram = tomogram[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "    cropped_segmentation_map = segmentation_map[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "\n",
    "    return cropped_tomogram, cropped_segmentation_map\n",
    "\n",
    "\n",
    "# Mixup\n",
    "# 2つのサンプルを線形補間して混合\n",
    "def mixup(tomogram, segmentation_map, alpha=0.4):\n",
    "    \"\"\"Applies mixup augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size = tomogram.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_tomogram = lam * tomogram + (1 - lam) * tomogram[index, :]\n",
    "    mixed_segmentation_map = (\n",
    "        lam * segmentation_map + (1 - lam) * segmentation_map[index, :]\n",
    "    )\n",
    "\n",
    "    return mixed_tomogram, mixed_segmentation_map\n",
    "\n",
    "\n",
    "# Cutmix\n",
    "# ランダム領域を切り取って別のサンプルに貼り付け\n",
    "def cutmix(tomogram, segmentation_map, alpha=1.0):\n",
    "    \"\"\"Applies cutmix augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size, depth, height, width = tomogram.size()\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    cx = random.randint(0, width)\n",
    "    cy = random.randint(0, height)\n",
    "    cw = int(width * (1 - lam))\n",
    "    ch = int(height * (1 - lam))\n",
    "\n",
    "    x1 = max(cx - cw // 2, 0)\n",
    "    x2 = min(cx + cw // 2, width)\n",
    "    y1 = max(cy - ch // 2, 0)\n",
    "    y2 = min(cy + ch // 2, height)\n",
    "\n",
    "    tomogram[:, :, y1:y2, x1:x2] = tomogram[index, :, y1:y2, x1:x2]\n",
    "    segmentation_map[:, :, y1:y2, x1:x2] = segmentation_map[index, :, y1:y2, x1:x2]\n",
    "\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# データ拡張の組み合わせ適用\n",
    "def augment_data(\n",
    "    tomogram,\n",
    "    segmentation_map,\n",
    "    crop_size=(16, 256, 256),\n",
    "    max_shift=10,\n",
    "    rotation_angle=30,\n",
    "    p=0.5,\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "):\n",
    "    \"\"\"Applies a combination of rotation, translation, flipping, cropping, mixup, and cutmix to the inputs with probabilities.\"\"\"\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = rotate_3d(\n",
    "            tomogram,\n",
    "            segmentation_map,\n",
    "            angle=random.uniform(-rotation_angle, rotation_angle),\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = translate_3d(\n",
    "            tomogram, segmentation_map, max_shift=max_shift\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = flip_3d(tomogram, segmentation_map)\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = crop_3d(\n",
    "            tomogram, segmentation_map, crop_size=crop_size\n",
    "        )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = mixup(\n",
    "    #         tomogram, segmentation_map, alpha=mixup_alpha\n",
    "    #     )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = cutmix(\n",
    "    #         tomogram, segmentation_map, alpha=cutmix_alpha\n",
    "    #     )\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# バッチサイズ6, 深さ16, 高さ320, 幅320のランダムテンソル\n",
    "tomogram = torch.rand((6, 16, 320, 320))\n",
    "segmentation_map = torch.randint(0, 2, (6, 16, 320, 320))  # ラベルは0または1\n",
    "\n",
    "# データ拡張の適用\n",
    "aug_tomogram, aug_segmentation_map = augment_data(tomogram, segmentation_map, p=0.7)\n",
    "print(\"Original shape:\", tomogram.shape)\n",
    "print(\"Augmented shape:\", aug_tomogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatuya\u001b[0m (\u001b[33mlatent-walkers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp080-recreate-baseline-dstride-new-model/wandb/run-20250203_000930-0tp2suk4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/0tp2suk4' target=\"_blank\">exp080-recreate-baseline-dstride-new-model_TS_73_6</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/0tp2suk4' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/0tp2suk4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0tp2suk4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39570494a2e47048648ee56914f9690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.016 MB uploaded\\r'), FloatProgress(value=0.2692375938960194, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp080-recreate-baseline-dstride-new-model_TS_73_6</strong> at: <a href='https://wandb.ai/latent-walkers/czii2024/runs/0tp2suk4' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/0tp2suk4</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250203_000930-0tp2suk4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0tp2suk4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b59b8fd9f74f95a32d2ed527fb7db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112948299999993, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp080-recreate-baseline-dstride-new-model/wandb/run-20250203_000931-d4kyq6mf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/d4kyq6mf' target=\"_blank\">exp080-recreate-baseline-dstride-new-model_TS_99_9</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/d4kyq6mf' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/d4kyq6mf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:d4kyq6mf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b34304b213448d87ca973f89631f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.016 MB uploaded\\r'), FloatProgress(value=0.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp080-recreate-baseline-dstride-new-model_TS_99_9</strong> at: <a href='https://wandb.ai/latent-walkers/czii2024/runs/d4kyq6mf' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/d4kyq6mf</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250203_000931-d4kyq6mf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:d4kyq6mf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f005120cb424df7bd482f41b8d0c42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113557099999918, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp080-recreate-baseline-dstride-new-model/wandb/run-20250203_001023-uz8iwl9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/uz8iwl9b' target=\"_blank\">exp080-recreate-baseline-dstride-new-model_TS_6_4</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/uz8iwl9b' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/uz8iwl9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [01:13<00:00, 10.21it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "Epoch 1/10 [Training]: 100%|██████████| 376/376 [1:01:54<00:00,  9.88s/it, loss=0.8880]  \n",
      "Epoch 1/10 [Validation]: 100%|██████████| 1/1 [00:08<00:00,  8.49s/it, loss=0.8287]\n",
      "100%|██████████| 6/6 [03:45<00:00, 37.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8880 valid-beta4-score:0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|██████████| 376/376 [1:03:02<00:00, 10.06s/it, loss=0.7819]  \n",
      "Epoch 2/10 [Validation]: 100%|██████████| 1/1 [00:10<00:00, 10.41s/it, loss=0.6991]\n",
      "100%|██████████| 6/6 [03:29<00:00, 34.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7819 valid-beta4-score:0.4690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|██████████| 376/376 [7:56:38<00:00, 76.06s/it, loss=0.7069]     \n",
      "Epoch 3/10 [Validation]: 100%|██████████| 1/1 [00:03<00:00,  3.80s/it, loss=0.8416]\n",
      "100%|██████████| 6/6 [03:29<00:00, 34.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7069 valid-beta4-score:0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|██████████| 376/376 [22:42<00:00,  3.62s/it, loss=0.6721] \n",
      "Epoch 4/10 [Validation]: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it, loss=0.8581]\n",
      "100%|██████████| 6/6 [03:41<00:00, 36.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6721 valid-beta4-score:0.6155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|██████████| 376/376 [3:00:27<00:00, 28.80s/it, loss=0.6502]   \n",
      "Epoch 5/10 [Validation]: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it, loss=0.5758]\n",
      "100%|██████████| 6/6 [03:44<00:00, 37.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6502 valid-beta4-score:0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|██████████| 376/376 [16:01<00:00,  2.56s/it, loss=0.6172]\n",
      "Epoch 6/10 [Validation]: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it, loss=0.5938]\n",
      "100%|██████████| 6/6 [03:36<00:00, 36.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6172 valid-beta4-score:0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]:  88%|████████▊ | 331/376 [5:47:16<01:48,  2.42s/it, loss=0.5960]   "
     ]
    }
   ],
   "source": [
    "for vaild_exp_name in [\"TS_73_6\", \"TS_99_9\", \"TS_6_4\", \"TS_69_2\", \"TS_86_3\", \"TS_6_6\"]:\n",
    "    wandb.init(\n",
    "        project=\"czii2024\", name=f\"{notebook_name}_{vaild_exp_name}\", config=param\n",
    "    )\n",
    "\n",
    "    if vaild_exp_name == \"TS_73_6\" or vaild_exp_name == \"TS_99_9\":\n",
    "        continue\n",
    "\n",
    "    vaild_exp_name = [vaild_exp_name]\n",
    "    train_exp_name = CFG.train_exp_names.copy()\n",
    "    train_exp_name.remove(vaild_exp_name[0])\n",
    "\n",
    "    # valid_exp_name[0]の名前でディレクトリを作成\n",
    "    os.makedirs(f\"./{vaild_exp_name[0]}\", exist_ok=True)\n",
    "\n",
    "    train_dataset = EziiDataset(\n",
    "        exp_names=train_exp_name,\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "        particles_name=CFG.particles_name,\n",
    "        resolution=CFG.resolution,\n",
    "        zarr_type=CFG.train_zarr_types,\n",
    "        train=True,\n",
    "        augmentation=True,\n",
    "        slice=True,\n",
    "        pre_read=True,\n",
    "    )\n",
    "    valid_dataset = EziiDataset(\n",
    "        exp_names=vaild_exp_name,\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "        particles_name=CFG.particles_name,\n",
    "        resolution=CFG.resolution,\n",
    "        zarr_type=CFG.valid_zarr_types,\n",
    "        augmentation=False,\n",
    "        train=True,\n",
    "        slice=True,\n",
    "        pre_read=True,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "\n",
    "    encoder = timm.create_model(\n",
    "        model_name=CFG.model_name,\n",
    "        pretrained=True,\n",
    "        in_chans=3,\n",
    "        num_classes=0,\n",
    "        global_pool=\"\",\n",
    "        features_only=True,\n",
    "    )\n",
    "    model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    criterion = DiceLoss()\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=10,\n",
    "        num_training_steps=CFG.epochs * len(train_loader),\n",
    "        # * batch_size,\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    seg_loss = SegmentationLoss(criterion)\n",
    "    padf = PadToSize(CFG.resolution)\n",
    "\n",
    "    best_model = None\n",
    "    best_constant = 0\n",
    "    best_score = -100\n",
    "    best_particle_score = {}\n",
    "\n",
    "    grand_train_loss = []\n",
    "    grand_valid_loss = []\n",
    "    grand_train_score = []\n",
    "    grand_valid_score = []\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        with tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\"\n",
    "        ) as tq:\n",
    "            for data in tq:\n",
    "                normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "                segmentation_map = data[\"segmentation_map\"]\n",
    "                zarr_embedding_idx = data[\"zarr_type_embedding_idx\"]\n",
    "\n",
    "                normalized_tomogram = padf(normalized_tomogram)\n",
    "                segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                # データ拡張\n",
    "                normalized_tomogram, segmentation_map = augment_data(\n",
    "                    normalized_tomogram, segmentation_map, p=CFG.augmentation_prob\n",
    "                )\n",
    "                normalized_tomogram = normalized_tomogram.cuda()\n",
    "                segmentation_map = segmentation_map.long().cuda()\n",
    "                zarr_embedding_idx = zarr_embedding_idx.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast():\n",
    "                    pred = model(\n",
    "                        preprocess_tensor(normalized_tomogram), zarr_embedding_idx\n",
    "                    )\n",
    "                    loss = seg_loss(pred, segmentation_map)\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                # 確率予測\n",
    "                prob_pred = torch.softmax(pred, dim=1)\n",
    "                tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "        del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        with tqdm(\n",
    "            valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\"\n",
    "        ) as tq:\n",
    "            with torch.no_grad():\n",
    "                for data in tq:\n",
    "                    normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "                    segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "                    zarr_embedding_idx = data[\"zarr_type_embedding_idx\"].cuda()\n",
    "\n",
    "                    normalized_tomogram = padf(normalized_tomogram)\n",
    "                    segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                    with autocast():\n",
    "                        pred = model(\n",
    "                            preprocess_tensor(normalized_tomogram), zarr_embedding_idx\n",
    "                        )\n",
    "                        loss = seg_loss(pred, segmentation_map)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "                    # 確率予測\n",
    "                    prob_pred = torch.softmax(pred, dim=1)\n",
    "                    tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "        del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # # ############### validation ################\n",
    "        train_nshuffle_original_tomogram = defaultdict(list)\n",
    "        train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "        train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        valid_original_tomogram = defaultdict(list)\n",
    "        valid_pred_tomogram = defaultdict(list)\n",
    "        valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        train_mean_scores = []\n",
    "        valid_mean_scores = []\n",
    "\n",
    "        # モデルの保存\n",
    "        make_dir_ = (\n",
    "            f\"../../../../../../../../mnt/d/kaggle-tmp-models/czii2024/{notebook_name}/\"\n",
    "        )\n",
    "        os.makedirs(make_dir_, exist_ok=True)\n",
    "        torch.save(model.state_dict(), make_dir_ + f\"model_{epoch}.pth\")\n",
    "\n",
    "        # ############### validation ################\n",
    "        train_nshuffle_original_tomogram = defaultdict(list)\n",
    "        train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "        train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        valid_original_tomogram = defaultdict(list)\n",
    "        valid_pred_tomogram = defaultdict(list)\n",
    "        valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        train_mean_scores = []\n",
    "        valid_mean_scores = []\n",
    "\n",
    "        train_inferenced_array = {}\n",
    "        train_pred_array = []\n",
    "        train_gt_array = []\n",
    "        valid_inferenced_array = {}\n",
    "        valid_gt_array = []\n",
    "\n",
    "        # for exp_name in tqdm(CFG.train_exp_names):\n",
    "        for exp_name in vaild_exp_name:  # 5つのデータで試す\n",
    "            # inferenced_array = inference(model, exp_name, train=False)\n",
    "            inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "                model, exp_name, train=False\n",
    "            )\n",
    "            valid_inferenced_array[exp_name] = inferenced_array\n",
    "            base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "            gt_df = create_gt_df(base_dir, [exp_name])\n",
    "            valid_gt_array.append(gt_df)\n",
    "\n",
    "        valid_gt_array = pd.concat(valid_gt_array)\n",
    "\n",
    "        b_constant = 0\n",
    "        b_score = -100\n",
    "        b_particle_score = {}\n",
    "\n",
    "        try:\n",
    "            best_thresholds, final_score = reduce_computation_sikii_search(\n",
    "                inferenced_array,\n",
    "                exp_name,\n",
    "                [\n",
    "                    0.05,\n",
    "                    0.1,\n",
    "                    0.15,\n",
    "                    0.2,\n",
    "                    0.25,\n",
    "                    0.3,\n",
    "                    0.35,\n",
    "                    0.4,\n",
    "                    0.45,\n",
    "                    0.5,\n",
    "                    0.55,\n",
    "                    0.6,\n",
    "                    0.65,\n",
    "                    0.7,\n",
    "                    0.75,\n",
    "                ],\n",
    "            )\n",
    "        except:\n",
    "            best_thresholds = [0.5] * 6\n",
    "            final_score = -50\n",
    "\n",
    "        b_score = final_score\n",
    "        b_particle_constant = {\n",
    "            \"apo-ferritin\": best_thresholds[0],\n",
    "            \"beta-amylase\": best_thresholds[1],\n",
    "            \"beta-galactosidase\": best_thresholds[2],\n",
    "            \"ribosome\": best_thresholds[3],\n",
    "            \"thyroglobulin\": best_thresholds[4],\n",
    "            \"virus-like-particle\": best_thresholds[5],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            valid_pred_array = []\n",
    "            for exp_name in [vaild_exp_name[0]]:\n",
    "                pred_df = inference2pos(\n",
    "                    pred_segmask=valid_inferenced_array[exp_name],\n",
    "                    exp_name=exp_name,\n",
    "                    sikii_dict=b_particle_constant,\n",
    "                )\n",
    "                valid_pred_array.append(pred_df)\n",
    "\n",
    "            valid_pred_array = pd.concat(valid_pred_array)\n",
    "\n",
    "            if len(valid_pred_array) != 0:\n",
    "                result_df, score_ = compute_lb(\n",
    "                    valid_pred_array,\n",
    "                    \"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "                    vaild_exp_name,\n",
    "                )\n",
    "                particle_score = extract_particle_results(result_df)\n",
    "\n",
    "                b_score = score_\n",
    "                b_particle_score = particle_score\n",
    "        except:\n",
    "            b_score = -50\n",
    "            b_particle_score = {}\n",
    "\n",
    "        import gc\n",
    "        import torch.cuda as cuda\n",
    "\n",
    "        # del valid_pred_array, valid_gt_array\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "\n",
    "        # print(\"constant\", b_constant, \"score\", b_score)\n",
    "\n",
    "        # wandb-log\n",
    "        train_info = {\n",
    "            \"01_epoch\": epoch,\n",
    "            \"02_train_loss\": np.mean(train_loss),\n",
    "            \"03_valid_loss\": np.mean(valid_loss),\n",
    "            # \"train_score\": np.mean(train_mean_scores),\n",
    "            \"04_valid_best_score\": b_score,\n",
    "        }\n",
    "        train_info = {**train_info, **b_particle_score}\n",
    "        train_info = {**train_info, **b_particle_constant}\n",
    "        wandb.log(train_info)\n",
    "\n",
    "        # score-update\n",
    "        if b_score > best_score:\n",
    "            best_score = b_score\n",
    "            # best_score = np.mean(valid_mean_scores)\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, f\"./{vaild_exp_name[0]}/best_model.pth\")\n",
    "\n",
    "        print(\n",
    "            f\"train-epoch-loss:{np.mean(train_loss):.4f}\",\n",
    "            # f\"valid-epoch-loss:{np.mean(valid_loss):.4f}\",\n",
    "            # f\"train-beta4-score:{np.mean(train_mean_scores):.4f}\",\n",
    "            f\"valid-beta4-score:{b_score:.4f}\",\n",
    "        )\n",
    "\n",
    "        grand_train_loss.append(np.mean(train_loss))\n",
    "        # grand_valid_loss.append(np.mean(valid_loss))\n",
    "        # grand_train_score.append(np.mean(train_mean_scores))\n",
    "        grand_valid_score.append(b_score)\n",
    "\n",
    "    del model, optimizer, criterion, scheduler, scaler, seg_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lossとvalid_lossのプロット\n",
    "\n",
    "plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scoreとvalid_scoreのプロット\n",
    "plt.plot(grand_train_score, label=\"train_score\")\n",
    "plt.plot(grand_valid_score, label=\"valid_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
