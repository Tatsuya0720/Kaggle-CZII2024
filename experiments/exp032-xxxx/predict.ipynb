{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import UNet_2D, aug\n",
    "from src.utils import save_images\n",
    "from src.metric import score, create_cls_pos, create_cls_pos_sikii, create_df\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../inputs/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TS_5_4', 'denoised'), ('TS_5_4', 'ctfdeconvolved'), ('TS_5_4', 'wbp'), ('TS_5_4', 'isonetcorrected'), ('TS_73_6', 'denoised'), ('TS_73_6', 'ctfdeconvolved'), ('TS_73_6', 'wbp'), ('TS_73_6', 'isonetcorrected'), ('TS_99_9', 'denoised'), ('TS_99_9', 'ctfdeconvolved'), ('TS_99_9', 'wbp'), ('TS_99_9', 'isonetcorrected'), ('TS_6_4', 'denoised'), ('TS_6_4', 'ctfdeconvolved'), ('TS_6_4', 'wbp'), ('TS_6_4', 'isonetcorrected'), ('TS_69_2', 'denoised'), ('TS_69_2', 'ctfdeconvolved'), ('TS_69_2', 'wbp'), ('TS_69_2', 'isonetcorrected')]\n",
      "[('TS_86_3', 'denoised'), ('TS_6_6', 'denoised')]\n",
      "[('TS_6_4', 'denoised'), ('TS_5_4', 'denoised'), ('TS_69_2', 'denoised')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    # exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_dataset = EziiDataset(\n",
    "    exp_names=[\"TS_6_4\", \"TS_5_4\", \"TS_69_2\"],\n",
    "    base_dir=\"../../inputs/test\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "for row in tqdm(valid_loader):\n",
    "    normalized_tomogram = row[\"normalized_tomogram\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadToSize(nn.Module):\n",
    "    def __init__(self, resolution):\n",
    "        super().__init__()\n",
    "        if resolution == \"0\":\n",
    "            self.size = 640\n",
    "        elif resolution == \"1\":\n",
    "            self.size = 320\n",
    "        elif resolution == \"2\":\n",
    "            self.size = 160\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.pad(x, (0, 0, self.size - x.shape[-1], self.size - x.shape[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4293923941643343: : 184it [00:04, 40.05it/s]                   \n"
     ]
    }
   ],
   "source": [
    "model = UNet_2D().to(\"cuda\")\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor([0.5, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    ")\n",
    "# criterion = DiceLoss()\n",
    "\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "batch_size = 4\n",
    "\n",
    "valid_loss = []\n",
    "valid_pred_tomogram = defaultdict(list)\n",
    "valid_gt_tomogram = defaultdict(list)\n",
    "model.eval()\n",
    "tq = tqdm(range(len(valid_loader) * normalized_tomogram.shape[0]))\n",
    "for data in valid_loader:\n",
    "    exp_name = data[\"exp_name\"][0]\n",
    "    tomogram = data[\"normalized_tomogram\"].to(\"cuda\")\n",
    "    segmentation_map = data[\"segmentation_map\"].to(\"cuda\").long()\n",
    "\n",
    "    for i in range(tomogram.shape[1]):\n",
    "        input_ = tomogram[:, i].unsqueeze(0)\n",
    "        gt = segmentation_map[:, i]\n",
    "\n",
    "        input_ = PadToSize(CFG.resolution)(input_)\n",
    "        gt = PadToSize(CFG.resolution)(gt)\n",
    "        output = model(input_)\n",
    "        output = nn.functional.softmax(output, dim=1)\n",
    "        loss = criterion(output, gt)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "        tq.set_description(f\"Loss: {np.mean(valid_loss)}\")\n",
    "        tq.update(1)\n",
    "\n",
    "        output = drop_padding(output, CFG.resolution)\n",
    "\n",
    "        valid_pred_tomogram[exp_name].append(output.cpu().detach().numpy())\n",
    "        valid_gt_tomogram[exp_name].append(gt.cpu().detach().numpy())\n",
    "tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_df(base_dir, exp_names):\n",
    "    result_df = None\n",
    "    particle_names = CFG.particles_name\n",
    "\n",
    "    for exp_name in exp_names:\n",
    "        for particle in particle_names:\n",
    "            np_corrds = read_info_json(\n",
    "                base_dir=base_dir, exp_name=exp_name, particle_name=particle\n",
    "            )  # (n, 3)\n",
    "            # 各行にexp_nameとparticle_name追加\n",
    "            particle_df = pd.DataFrame(np_corrds, columns=[\"z\", \"y\", \"x\"])\n",
    "            particle_df[\"experiment\"] = exp_name\n",
    "            particle_df[\"particle_type\"] = particle\n",
    "\n",
    "            if result_df is None:\n",
    "                result_df = particle_df\n",
    "            else:\n",
    "                result_df = pd.concat([result_df, particle_df], axis=0).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "    result_df = result_df.reset_index()  # index\texperiment\tparticle_type\tx\ty\tz\n",
    "    result_df = result_df[[\"index\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", CFG.valid_exp_names)\n",
    "gt_df = gt_df[gt_df[\"particle_type\"] != \"beta-amylase\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df = pd.read_csv(\"../../inputs/train_submission.csv\")\n",
    "\n",
    "\n",
    "def calc_score(initial_sikii):\n",
    "    all_pred_df = None\n",
    "\n",
    "    for exp_name in CFG.valid_exp_names:\n",
    "        pred_tomogram = valid_pred_tomogram[exp_name]\n",
    "        pred_tomogram = np.array(pred_tomogram)  # (92, 1, 7, 315, 315)\n",
    "        pred_tomogram = pred_tomogram.squeeze(1)  # (92, 7, 315, 315)\n",
    "\n",
    "        pred_cls_pos, pred_Ascale_pos = create_cls_pos_sikii(\n",
    "            pred_tomogram, sikii_dict=initial_sikii\n",
    "        )\n",
    "        pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "        # pred_df = create_df(pred_cls_pos, exp_name)\n",
    "\n",
    "        if all_pred_df is None:\n",
    "            all_pred_df = pred_df\n",
    "        else:\n",
    "            all_pred_df = pd.concat([all_pred_df, pred_df], axis=0).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "    pred_df = all_pred_df[all_pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=[\"x\", \"y\", \"z\"], keep=\"first\").reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    pred_df = pred_df.reset_index()\n",
    "\n",
    "    score_ = score(\n",
    "        pred_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1, beta=4\n",
    "    )\n",
    "\n",
    "    return score_\n",
    "\n",
    "\n",
    "def calc_score_by_exp(initial_sikii):\n",
    "    exp_scores = {}\n",
    "\n",
    "    for exp_name in CFG.valid_exp_names:\n",
    "        gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", [exp_name])\n",
    "\n",
    "        pred_tomogram = valid_pred_tomogram[exp_name]\n",
    "        pred_tomogram = np.array(pred_tomogram)  # (92, 1, 7, 315, 315)\n",
    "        pred_tomogram = pred_tomogram.squeeze(1)  # (92, 7, 315, 315)\n",
    "\n",
    "        pred_cls_pos, pred_Ascale_pos = create_cls_pos_sikii(\n",
    "            pred_tomogram, sikii_dict=initial_sikii\n",
    "        )\n",
    "        pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "\n",
    "        pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "        pred_df = pred_df.drop_duplicates(\n",
    "            subset=[\"x\", \"y\", \"z\"], keep=\"first\"\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        pred_df = pred_df.reset_index()\n",
    "\n",
    "        score_ = score(\n",
    "            pred_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1, beta=4\n",
    "        )\n",
    "\n",
    "        exp_scores[exp_name] = score_\n",
    "\n",
    "    return exp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3702277379631658"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant = 0.35\n",
    "\n",
    "initial_sikii = {\n",
    "    \"apo-ferritin\": constant,\n",
    "    \"beta-amylase\": constant,\n",
    "    \"beta-galactosidase\": constant,\n",
    "    \"ribosome\": constant,\n",
    "    \"thyroglobulin\": constant,\n",
    "    \"virus-like-particle\": constant,\n",
    "}\n",
    "\n",
    "score_ = calc_score(initial_sikii)\n",
    "score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TS_86_3': 0.5155326189007919, 'TS_6_6': 0.5345474069874836}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant = 0.5666666666666667\n",
    "\n",
    "initial_sikii = {\n",
    "    \"apo-ferritin\": constant,\n",
    "    \"beta-amylase\": constant,\n",
    "    \"beta-galactosidase\": constant,\n",
    "    \"ribosome\": constant,\n",
    "    \"thyroglobulin\": constant,\n",
    "    \"virus-like-particle\": constant,\n",
    "}\n",
    "\n",
    "score_ = calc_score_by_exp(initial_sikii)\n",
    "score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_sikii' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best_sikii = 0\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# best_score = -np.inf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         best_sikii = sikii\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     print(sikii, score_)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbest_sikii\u001b[49m, best_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_sikii' is not defined"
     ]
    }
   ],
   "source": [
    "best_sikii = 0\n",
    "best_score = -np.inf\n",
    "\n",
    "for sikii in np.linspace(0.3, 0.7, 100):\n",
    "    initial_sikii = {\n",
    "        \"apo-ferritin\": sikii,\n",
    "        \"beta-amylase\": sikii,\n",
    "        \"beta-galactosidase\": sikii,\n",
    "        \"ribosome\": sikii,\n",
    "        \"thyroglobulin\": sikii,\n",
    "        \"virus-like-particle\": sikii,\n",
    "    }\n",
    "    score_ = calc_score(initial_sikii)\n",
    "    if score_ > best_score:\n",
    "        best_score = score_\n",
    "        best_sikii = sikii\n",
    "    print(sikii, score_)\n",
    "\n",
    "best_sikii, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1840it [00:29, 63.00it/s]                      \n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_pred_tomogram = defaultdict(list)\n",
    "valid_gt_tomogram = defaultdict(list)\n",
    "model.eval()\n",
    "tq = tqdm(range(len(train_loader) * normalized_tomogram.shape[0]))\n",
    "for data in train_loader:\n",
    "    exp_name = data[\"exp_name\"][0]\n",
    "    tomogram = data[\"normalized_tomogram\"].to(\"cuda\")\n",
    "    segmentation_map = data[\"segmentation_map\"].to(\"cuda\").long()\n",
    "\n",
    "    for i in range(tomogram.shape[1]):\n",
    "        input_ = tomogram[:, i].unsqueeze(0)\n",
    "        gt = segmentation_map[:, i]\n",
    "\n",
    "        input_ = PadToSize(CFG.resolution)(input_)\n",
    "        gt = PadToSize(CFG.resolution)(gt)\n",
    "        output = model(input_)\n",
    "        output = nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        tq.update(1)\n",
    "\n",
    "        output = drop_padding(output, CFG.resolution)\n",
    "\n",
    "        valid_pred_tomogram[exp_name].append(output.cpu().detach().numpy())\n",
    "        valid_gt_tomogram[exp_name].append(gt.cpu().detach().numpy())\n",
    "tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(initial_sikii):\n",
    "    all_pred_df = None\n",
    "\n",
    "    for exp_name in CFG.train_exp_names:\n",
    "        pred_tomogram = valid_pred_tomogram[exp_name]\n",
    "        pred_tomogram = np.array(pred_tomogram)  # (92, 1, 7, 315, 315)\n",
    "        pred_tomogram = pred_tomogram.squeeze(1)  # (92, 7, 315, 315)\n",
    "\n",
    "        pred_cls_pos, pred_Ascale_pos = create_cls_pos_sikii(\n",
    "            pred_tomogram, sikii_dict=initial_sikii\n",
    "        )\n",
    "        pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "        # pred_df = create_df(pred_cls_pos, exp_name)\n",
    "\n",
    "        if all_pred_df is None:\n",
    "            all_pred_df = pred_df\n",
    "        else:\n",
    "            all_pred_df = pd.concat([all_pred_df, pred_df], axis=0).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "    pred_df = all_pred_df[all_pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=[\"x\", \"y\", \"z\"], keep=\"first\").reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    pred_df = pred_df.reset_index()\n",
    "\n",
    "    score_ = score(\n",
    "        pred_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1, beta=4\n",
    "    )\n",
    "\n",
    "    return score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", CFG.train_exp_names)\n",
    "gt_df = gt_df[gt_df[\"particle_type\"] != \"beta-amylase\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 0.3534642243245894\n",
      "0.41250000000000003 0.3633357267104117\n",
      "0.42500000000000004 0.3683684864295206\n",
      "0.4375 0.37240028369233763\n",
      "0.45 0.378698423683563\n",
      "0.4625 0.3855536346532354\n",
      "0.475 0.3851317411342358\n",
      "0.4875 0.38739060577678897\n",
      "0.5 0.3940074160254892\n",
      "0.5125 0.39966732042505076\n",
      "0.525 0.4059922209302399\n",
      "0.5375 0.41111633642684453\n",
      "0.55 0.4149057778718746\n",
      "0.5625 0.4166629709990723\n",
      "0.575 0.4200151894119474\n",
      "0.5874999999999999 0.41221797587577946\n",
      "0.6 0.4219170897705994\n",
      "0.6125 0.42937237746217577\n",
      "0.625 0.43422966984334505\n",
      "0.6375 0.4390913151810357\n",
      "0.6499999999999999 0.4434392255622401\n",
      "0.6625 0.4388315231214689\n",
      "0.6749999999999999 0.4423836595504814\n",
      "0.6875 0.45234293202416415\n"
     ]
    }
   ],
   "source": [
    "best_sikii = 0\n",
    "best_score = -np.inf\n",
    "\n",
    "for sikii in np.linspace(0.4, 0.7, 25):\n",
    "    initial_sikii = {\n",
    "        \"apo-ferritin\": sikii,\n",
    "        \"beta-amylase\": sikii,\n",
    "        \"beta-galactosidase\": sikii,\n",
    "        \"ribosome\": sikii,\n",
    "        \"thyroglobulin\": sikii,\n",
    "        \"virus-like-particle\": sikii,\n",
    "    }\n",
    "    score_ = calc_score(initial_sikii)\n",
    "    if score_ > best_score:\n",
    "        best_score = score_\n",
    "        best_sikii = sikii\n",
    "    print(sikii, score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sikii, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_tomogram = defaultdict(list)\n",
    "model.eval()\n",
    "tq = tqdm(range(len(test_loader) * normalized_tomogram.shape[0]))\n",
    "for data in test_loader:\n",
    "    exp_name = data[\"exp_name\"][0]\n",
    "    tomogram = data[\"normalized_tomogram\"].to(\"cuda\")\n",
    "\n",
    "    for i in range(tomogram.shape[1]):\n",
    "        input_ = tomogram[:, i].unsqueeze(0)\n",
    "\n",
    "        input_ = PadToSize(CFG.resolution)(input_)\n",
    "        output = model(input_)\n",
    "        output = nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        tq.update(1)\n",
    "\n",
    "        output = drop_padding(output, CFG.resolution)\n",
    "\n",
    "        valid_pred_tomogram[exp_name].append(output.cpu().detach().numpy())\n",
    "tq.close()\n",
    "\n",
    "all_pred_df = None\n",
    "\n",
    "for exp_name in [\"TS_6_4\", \"TS_5_4\", \"TS_69_2\"]:\n",
    "    pred_tomogram = valid_pred_tomogram[exp_name]\n",
    "    pred_tomogram = np.array(pred_tomogram)  # (92, 1, 7, 315, 315)\n",
    "    pred_tomogram = pred_tomogram.squeeze(1)  # (92, 7, 315, 315)\n",
    "\n",
    "    pred_cls_pos, pred_Ascale_pos = create_cls_pos_sikii(\n",
    "        pred_tomogram, sikii_dict=initial_sikii\n",
    "    )\n",
    "    pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "    # pred_df = create_df(pred_cls_pos, exp_name)\n",
    "\n",
    "    if all_pred_df is None:\n",
    "        all_pred_df = pred_df\n",
    "    else:\n",
    "        all_pred_df = pd.concat([all_pred_df, pred_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "pred_df = all_pred_df[all_pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=[\"x\", \"y\", \"z\"], keep=\"first\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "pred_df = pred_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
