{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from src.kaggle_notebook_metric import compute_lb, extract_particle_results\n",
    "from src.inference import inference, inference2pos, create_gt_df\n",
    "from metric import visualize_epoch_results\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_name = os.path.join(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "param = {\n",
    "    \"model\": CFG.model_name,\n",
    "    \"resolution\": CFG.resolution,\n",
    "    \"augmentation_prob\": CFG.augmentation_prob,\n",
    "    \"slice\": CFG.slice_,\n",
    "    \"epochs\": CFG.epochs,\n",
    "    \"lr\": CFG.lr,\n",
    "    \"batch_size\": CFG.batch_size,\n",
    "    \"weight_decay\": CFG.weight_decay,\n",
    "    \"num_workers\": CFG.num_workers,\n",
    "    \"augment_data_ratio\": CFG.augment_data_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# sikii値とexp_namesを入れるとスコアを出力する関数\n",
    "\n",
    "\n",
    "def compute_score(sikii_list, inferenced_array, exp_name):\n",
    "    apo_ferritin = sikii_list[0]\n",
    "    beta_amylase = sikii_list[1]\n",
    "    beta_galactosidase = sikii_list[2]\n",
    "    ribosome = sikii_list[3]\n",
    "    thyroglobulin = sikii_list[4]\n",
    "    virus_like_particle = sikii_list[5]\n",
    "\n",
    "    sikii_dict = {\n",
    "        \"apo-ferritin\": apo_ferritin,\n",
    "        \"beta-amylase\": beta_amylase,\n",
    "        \"beta-galactosidase\": beta_galactosidase,\n",
    "        \"ribosome\": ribosome,\n",
    "        \"thyroglobulin\": thyroglobulin,\n",
    "        \"virus-like-particle\": virus_like_particle,\n",
    "    }\n",
    "\n",
    "    all_pred = []\n",
    "\n",
    "    pred_df = inference2pos(\n",
    "        pred_segmask=inferenced_array, exp_name=exp_name, sikii_dict=sikii_dict\n",
    "    )\n",
    "\n",
    "    all_pred.append(pred_df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df = pd.concat(all_pred, axis=0).reset_index(drop=True)\n",
    "    pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(\n",
    "        subset=[\"experiment\", \"x\", \"y\", \"z\"], keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    pred_df = pred_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", [exp_name])\n",
    "\n",
    "    result_df, lb_score = compute_lb(\n",
    "        pred_df, \"../../inputs/train/overlay/ExperimentRuns/\", [exp_name]\n",
    "    )\n",
    "\n",
    "    return lb_score\n",
    "\n",
    "\n",
    "def reduce_computation_sikii_search(\n",
    "    inferenced_array: np.ndarray, exp_name: str, threshold_candidates: list[float]\n",
    ") -> tuple[list[float], float]:\n",
    "    \"\"\"\n",
    "    # How\n",
    "    6つのしきい値が互いに独立してスコアに貢献しているという前提で、\n",
    "    1次元ずつ最適なしきい値を探す手法を実装する.\n",
    "\n",
    "    1. 初期の best_thresholds (全要素 0.5 など適当な値) を用意\n",
    "    2. i=0 から i=5 まで順番に:\n",
    "       - threshold_candidates をすべて試し、他は固定したまま i 番目だけ変化させてスコアを計算\n",
    "       - 最良スコアが得られる候補値を確定し、best_thresholds[i] とする\n",
    "    3. 全部決まったら最終的なスコアを計算して返す\n",
    "\n",
    "    これにより、全組み合わせ (product) を回すよりも計算量が大幅に減少する.\n",
    "    \"\"\"\n",
    "    # Why not: 6値独立であるという前提が満たされていない場合、近似解になる可能性あり\n",
    "    best_thresholds = [0.5] * 6  # 適当な初期値でOK\n",
    "\n",
    "    for i in tqdm(range(6)):\n",
    "        best_local_score = -float(\"inf\")\n",
    "        best_local_value = None\n",
    "\n",
    "        for candidate in threshold_candidates:\n",
    "            current_thresholds = best_thresholds[:]  # 現在のベストを複製\n",
    "            current_thresholds[i] = candidate\n",
    "            score = compute_score(current_thresholds, inferenced_array, exp_name)\n",
    "            if score > best_local_score:\n",
    "                best_local_score = score\n",
    "                best_local_value = candidate\n",
    "\n",
    "        # i番目のしきい値を最適値に更新\n",
    "        best_thresholds[i] = best_local_value\n",
    "\n",
    "    final_score = compute_score(best_thresholds, inferenced_array, exp_name)\n",
    "    return best_thresholds, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([6, 16, 320, 320])\n",
      "Augmented shape: torch.Size([6, 16, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# 回転\n",
    "# 3Dテンソルの各軸に対して指定した角度で回転する関数\n",
    "def rotate_3d(tomogram, segmentation_map, angle):\n",
    "    \"\"\"Rotates the 3D tensors tomogram and segmentation_map around the Z-axis.\"\"\"\n",
    "    rotated_tomogram = TF.rotate(tomogram, angle, expand=False)\n",
    "    rotated_segmentation_map = TF.rotate(segmentation_map, angle, expand=False)\n",
    "    return rotated_tomogram, rotated_segmentation_map\n",
    "\n",
    "\n",
    "# 平行移動\n",
    "# 指定された範囲でランダムに平行移動\n",
    "def translate_3d(tomogram, segmentation_map, max_shift):\n",
    "    \"\"\"Translates the 3D tensors by a random shift within max_shift.\"\"\"\n",
    "    shift_x = random.randint(-max_shift, max_shift)\n",
    "    shift_y = random.randint(-max_shift, max_shift)\n",
    "    translated_tomogram = TF.affine(\n",
    "        tomogram, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    translated_segmentation_map = TF.affine(\n",
    "        segmentation_map, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    return translated_tomogram, translated_segmentation_map\n",
    "\n",
    "\n",
    "# フリップ\n",
    "# 縦横（上下左右）ランダムフリップ\n",
    "def flip_3d(tomogram, segmentation_map):\n",
    "    \"\"\"Randomly flips the 3D tensors along height or width.\"\"\"\n",
    "    if random.random() > 0.5:  # Horizontal flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-1])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-1])\n",
    "    if random.random() > 0.5:  # Vertical flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-2])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-2])\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# クロッピング\n",
    "# 入力テンソルを中心またはランダムクロップで切り取る\n",
    "def crop_3d(tomogram, segmentation_map, crop_size):\n",
    "    \"\"\"Crops the 3D tensors to the specified crop_size.\"\"\"\n",
    "    _, depth, height, width = tomogram.size()\n",
    "    crop_d, crop_h, crop_w = crop_size\n",
    "\n",
    "    if crop_h > height or crop_w > width:\n",
    "        raise ValueError(\"Crop size cannot be larger than the original size.\")\n",
    "\n",
    "    start_h = random.randint(0, height - crop_h)  # Random starting position for height\n",
    "    start_w = random.randint(0, width - crop_w)  # Random starting position for width\n",
    "\n",
    "    cropped_tomogram = tomogram[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "    cropped_segmentation_map = segmentation_map[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "\n",
    "    return cropped_tomogram, cropped_segmentation_map\n",
    "\n",
    "\n",
    "# Mixup\n",
    "# 2つのサンプルを線形補間して混合\n",
    "def mixup(tomogram, segmentation_map, alpha=0.4):\n",
    "    \"\"\"Applies mixup augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size = tomogram.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_tomogram = lam * tomogram + (1 - lam) * tomogram[index, :]\n",
    "    mixed_segmentation_map = (\n",
    "        lam * segmentation_map + (1 - lam) * segmentation_map[index, :]\n",
    "    )\n",
    "\n",
    "    return mixed_tomogram, mixed_segmentation_map\n",
    "\n",
    "\n",
    "# Cutmix\n",
    "# ランダム領域を切り取って別のサンプルに貼り付け\n",
    "def cutmix(tomogram, segmentation_map, alpha=1.0):\n",
    "    \"\"\"Applies cutmix augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size, depth, height, width = tomogram.size()\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    cx = random.randint(0, width)\n",
    "    cy = random.randint(0, height)\n",
    "    cw = int(width * (1 - lam))\n",
    "    ch = int(height * (1 - lam))\n",
    "\n",
    "    x1 = max(cx - cw // 2, 0)\n",
    "    x2 = min(cx + cw // 2, width)\n",
    "    y1 = max(cy - ch // 2, 0)\n",
    "    y2 = min(cy + ch // 2, height)\n",
    "\n",
    "    tomogram[:, :, y1:y2, x1:x2] = tomogram[index, :, y1:y2, x1:x2]\n",
    "    segmentation_map[:, :, y1:y2, x1:x2] = segmentation_map[index, :, y1:y2, x1:x2]\n",
    "\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# データ拡張の組み合わせ適用\n",
    "def augment_data(\n",
    "    tomogram,\n",
    "    segmentation_map,\n",
    "    crop_size=(16, 256, 256),\n",
    "    max_shift=10,\n",
    "    rotation_angle=30,\n",
    "    p=0.5,\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "):\n",
    "    \"\"\"Applies a combination of rotation, translation, flipping, cropping, mixup, and cutmix to the inputs with probabilities.\"\"\"\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = rotate_3d(\n",
    "            tomogram,\n",
    "            segmentation_map,\n",
    "            angle=random.uniform(-rotation_angle, rotation_angle),\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = translate_3d(\n",
    "            tomogram, segmentation_map, max_shift=max_shift\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = flip_3d(tomogram, segmentation_map)\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = crop_3d(\n",
    "            tomogram, segmentation_map, crop_size=crop_size\n",
    "        )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = mixup(\n",
    "    #         tomogram, segmentation_map, alpha=mixup_alpha\n",
    "    #     )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = cutmix(\n",
    "    #         tomogram, segmentation_map, alpha=cutmix_alpha\n",
    "    #     )\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# バッチサイズ6, 深さ16, 高さ320, 幅320のランダムテンソル\n",
    "tomogram = torch.rand((6, 16, 320, 320))\n",
    "segmentation_map = torch.randint(0, 2, (6, 16, 320, 320))  # ラベルは0または1\n",
    "\n",
    "# データ拡張の適用\n",
    "aug_tomogram, aug_segmentation_map = augment_data(tomogram, segmentation_map, p=0.7)\n",
    "print(\"Original shape:\", tomogram.shape)\n",
    "print(\"Augmented shape:\", aug_tomogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatuya\u001b[0m (\u001b[33mlatent-walkers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp076-recreate-baseline-renet34d-dstride/wandb/run-20250201_114358-nup0rovb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/nup0rovb' target=\"_blank\">exp076-recreate-baseline-renet34d-dstride_TS_73_6</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/nup0rovb' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/nup0rovb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_exp_name ['TS_73_6', 'TS_99_9']\n",
      "valid_exp_name ['TS_6_4', 'TS_69_2', 'TS_86_3', 'TS_6_6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [00:58<00:00,  9.60it/s]\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n",
      "Epoch 1/15 [Training]: 100%|██████████| 280/280 [02:51<00:00,  1.63it/s, loss=0.8739]\n",
      "Epoch 1/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.49it/s, loss=0.8469]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8739 valid-beta4-score:0.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Training]: 100%|██████████| 280/280 [02:45<00:00,  1.70it/s, loss=0.7604]\n",
      "Epoch 2/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s, loss=0.8545]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7604 valid-beta4-score:0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Training]: 100%|██████████| 280/280 [02:32<00:00,  1.84it/s, loss=0.6884]\n",
      "Epoch 3/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.75it/s, loss=0.7560]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6884 valid-beta4-score:0.5371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Training]: 100%|██████████| 280/280 [03:01<00:00,  1.54it/s, loss=0.6348]\n",
      "Epoch 4/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s, loss=0.7436]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6348 valid-beta4-score:0.5537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Training]: 100%|██████████| 280/280 [02:42<00:00,  1.72it/s, loss=0.6146]\n",
      "Epoch 5/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s, loss=0.8065]\n",
      "100%|██████████| 6/6 [01:01<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6146 valid-beta4-score:0.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Training]: 100%|██████████| 280/280 [02:42<00:00,  1.72it/s, loss=0.5662]\n",
      "Epoch 6/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s, loss=0.6809]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5662 valid-beta4-score:0.5671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Training]: 100%|██████████| 280/280 [02:42<00:00,  1.73it/s, loss=0.5358]\n",
      "Epoch 7/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s, loss=0.7024]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5358 valid-beta4-score:0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Training]: 100%|██████████| 280/280 [02:41<00:00,  1.74it/s, loss=0.5321]\n",
      "Epoch 8/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s, loss=0.6927]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5321 valid-beta4-score:0.6152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Training]: 100%|██████████| 280/280 [02:43<00:00,  1.71it/s, loss=0.5284]\n",
      "Epoch 9/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.73it/s, loss=0.5187]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5284 valid-beta4-score:0.7114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Training]: 100%|██████████| 280/280 [02:36<00:00,  1.78it/s, loss=0.5148]\n",
      "Epoch 10/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s, loss=0.7037]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5148 valid-beta4-score:0.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Training]: 100%|██████████| 280/280 [02:44<00:00,  1.70it/s, loss=0.5090]\n",
      "Epoch 11/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.87it/s, loss=0.8218]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5090 valid-beta4-score:0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Training]: 100%|██████████| 280/280 [02:44<00:00,  1.71it/s, loss=0.5009]\n",
      "Epoch 12/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.73it/s, loss=0.6653]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5009 valid-beta4-score:0.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Training]: 100%|██████████| 280/280 [02:34<00:00,  1.81it/s, loss=0.5216]\n",
      "Epoch 13/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.78it/s, loss=0.5079]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5216 valid-beta4-score:0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Training]: 100%|██████████| 280/280 [02:42<00:00,  1.72it/s, loss=0.5024]\n",
      "Epoch 14/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.82it/s, loss=0.6979]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5024 valid-beta4-score:0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Training]: 100%|██████████| 280/280 [02:40<00:00,  1.74it/s, loss=0.5026]\n",
      "Epoch 15/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s, loss=0.6096]\n",
      "100%|██████████| 6/6 [00:59<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5026 valid-beta4-score:0.7008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nup0rovb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e46ff0711e4690b6b92759a6ca88b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>01_epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>02_train_loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>03_valid_loss</td><td>██▆▆▇▄▅▅▁▅▇▄▁▅▃</td></tr><tr><td>04_valid_best_score</td><td>▁▄▅▆▅▆▆▇██▆▇█▇█</td></tr><tr><td>apo-ferritin</td><td>▁▁▁███▁███▁████</td></tr><tr><td>apoo_ferritin_f4</td><td>▁▅███▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>apoo_ferritin_p</td><td>▁▄▇█▆█▅▆▇▆▅▆▇▆▇</td></tr><tr><td>apoo_ferritin_r</td><td>▁▅███▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>beta-amylase</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>beta-galactosidase</td><td>▁▂▅█████▂█▂████</td></tr><tr><td>beta_amylase_f4</td><td>▃▁▅▆▃▃▇▆█▇▆▇▇▆▇</td></tr><tr><td>beta_amylase_p</td><td>▂▁▄▄▂▂▆▅█▆▅▆▆▅▅</td></tr><tr><td>beta_amylase_r</td><td>▆▁▆▇▇██▇▇▇▆▇▇▇▇</td></tr><tr><td>beta_galactosidase_f4</td><td>▁▂▆▇▁▅▇▅██▃▇▇▆▇</td></tr><tr><td>beta_galactosidase_p</td><td>▁▃▇█▁▄▆▄█▇▃▆▆▅▇</td></tr><tr><td>beta_galactosidase_r</td><td>▇▂▅▆▁██▇▇▇▄██▇█</td></tr><tr><td>ribosome</td><td>███▁███████████</td></tr><tr><td>ribosome_f4</td><td>▁█▆▆▇▇▅▆▇▇▆▆█▇▇</td></tr><tr><td>ribosome_p</td><td>▁█▅▄▄▄▂▃▅▅▃▄▆▅▆</td></tr><tr><td>ribosome_r</td><td>▁▇▆▄█▇▇▇▇█▇▆▇▇▇</td></tr><tr><td>thyroglobulin</td><td>▄▁▁▄█████████▄█</td></tr><tr><td>thyroglobulin_f4</td><td>▁▇▇▇▄▆▇▆██▆▇█▇█</td></tr><tr><td>thyroglobulin_p</td><td>▁▆▇▅▃▃▇▄█▆▄▅▇▅▆</td></tr><tr><td>thyroglobulin_r</td><td>▁▆▄▇▅█▄▇▅▇▇▇▇▇▇</td></tr><tr><td>virus-like-particle</td><td>▁▁▁▁█████▅█████</td></tr><tr><td>virus_like_particle_f4</td><td>▂▁▁▁▅▄▅▇███████</td></tr><tr><td>virus_like_particle_p</td><td>▁▁▁▁▂▁▂▃▇▇▇▆█▆▆</td></tr><tr><td>virus_like_particle_r</td><td>▅▁▁▁███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>01_epoch</td><td>14</td></tr><tr><td>02_train_loss</td><td>0.50263</td></tr><tr><td>03_valid_loss</td><td>0.60959</td></tr><tr><td>04_valid_best_score</td><td>0.70081</td></tr><tr><td>apo-ferritin</td><td>0.3</td></tr><tr><td>apoo_ferritin_f4</td><td>0.8018</td></tr><tr><td>apoo_ferritin_p</td><td>0.4264</td></tr><tr><td>apoo_ferritin_r</td><td>0.84848</td></tr><tr><td>beta-amylase</td><td>0.05</td></tr><tr><td>beta-galactosidase</td><td>0.3</td></tr><tr><td>beta_amylase_f4</td><td>0.31892</td></tr><tr><td>beta_amylase_p</td><td>0.02963</td></tr><tr><td>beta_amylase_r</td><td>0.81818</td></tr><tr><td>beta_galactosidase_f4</td><td>0.53125</td></tr><tr><td>beta_galactosidase_p</td><td>0.07161</td></tr><tr><td>beta_galactosidase_r</td><td>0.8871</td></tr><tr><td>ribosome</td><td>0.3</td></tr><tr><td>ribosome_f4</td><td>0.74163</td></tr><tr><td>ribosome_p</td><td>0.29301</td></tr><tr><td>ribosome_r</td><td>0.82011</td></tr><tr><td>thyroglobulin</td><td>0.3</td></tr><tr><td>thyroglobulin_f4</td><td>0.68752</td></tr><tr><td>thyroglobulin_p</td><td>0.14866</td></tr><tr><td>thyroglobulin_r</td><td>0.88889</td></tr><tr><td>virus-like-particle</td><td>0.3</td></tr><tr><td>virus_like_particle_f4</td><td>0.92469</td></tr><tr><td>virus_like_particle_p</td><td>0.52846</td></tr><tr><td>virus_like_particle_r</td><td>0.97015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp076-recreate-baseline-renet34d-dstride_TS_73_6</strong> at: <a href='https://wandb.ai/latent-walkers/czii2024/runs/nup0rovb' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/nup0rovb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250201_114358-nup0rovb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nup0rovb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7801807674ff4ae983ef9f627ee89a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113271666302656, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp076-recreate-baseline-renet34d-dstride/wandb/run-20250201_130058-37xbuauq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/37xbuauq' target=\"_blank\">exp076-recreate-baseline-renet34d-dstride_TS_6_4</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/37xbuauq' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/37xbuauq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_exp_name ['TS_6_4', 'TS_69_2']\n",
      "valid_exp_name ['TS_73_6', 'TS_99_9', 'TS_86_3', 'TS_6_6']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [01:23<00:00,  6.73it/s]\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.84s/it]\n",
      "Epoch 1/15 [Training]: 100%|██████████| 280/280 [02:52<00:00,  1.62it/s, loss=0.8619]\n",
      "Epoch 1/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s, loss=0.8119]\n",
      "100%|██████████| 6/6 [00:58<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8619 valid-beta4-score:0.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Training]: 100%|██████████| 280/280 [02:38<00:00,  1.76it/s, loss=0.7626]\n",
      "Epoch 2/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.73it/s, loss=0.8236]\n",
      "100%|██████████| 6/6 [00:53<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7626 valid-beta4-score:0.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Training]: 100%|██████████| 280/280 [02:54<00:00,  1.60it/s, loss=0.6870]\n",
      "Epoch 3/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.79it/s, loss=0.7672]\n",
      "100%|██████████| 6/6 [00:55<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6870 valid-beta4-score:0.4757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Training]: 100%|██████████| 280/280 [02:41<00:00,  1.74it/s, loss=0.6554]\n",
      "Epoch 4/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.82it/s, loss=0.7611]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6554 valid-beta4-score:0.4002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Training]: 100%|██████████| 280/280 [02:43<00:00,  1.71it/s, loss=0.6340]\n",
      "Epoch 5/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s, loss=0.6671]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6340 valid-beta4-score:0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Training]: 100%|██████████| 280/280 [02:47<00:00,  1.67it/s, loss=0.6048]\n",
      "Epoch 6/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.67it/s, loss=0.7620]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6048 valid-beta4-score:0.4880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Training]: 100%|██████████| 280/280 [02:55<00:00,  1.60it/s, loss=0.6015]\n",
      "Epoch 7/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.52it/s, loss=0.7060]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6015 valid-beta4-score:0.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Training]: 100%|██████████| 280/280 [02:41<00:00,  1.73it/s, loss=0.5941]\n",
      "Epoch 8/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.59it/s, loss=0.7040]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5941 valid-beta4-score:0.5138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Training]: 100%|██████████| 280/280 [02:35<00:00,  1.80it/s, loss=0.5966]\n",
      "Epoch 9/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.49it/s, loss=0.6969]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5966 valid-beta4-score:0.5633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Training]: 100%|██████████| 280/280 [02:46<00:00,  1.68it/s, loss=0.5749]\n",
      "Epoch 10/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.64it/s, loss=0.6904]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5749 valid-beta4-score:0.5293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Training]: 100%|██████████| 280/280 [02:41<00:00,  1.74it/s, loss=0.5791]\n",
      "Epoch 11/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.70it/s, loss=0.7407]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5791 valid-beta4-score:0.5603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Training]: 100%|██████████| 280/280 [02:45<00:00,  1.69it/s, loss=0.5733]\n",
      "Epoch 12/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.69it/s, loss=0.7108]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5733 valid-beta4-score:0.5182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Training]: 100%|██████████| 280/280 [02:49<00:00,  1.65it/s, loss=0.5655]\n",
      "Epoch 13/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s, loss=0.7046]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5655 valid-beta4-score:0.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Training]: 100%|██████████| 280/280 [02:38<00:00,  1.77it/s, loss=0.5759]\n",
      "Epoch 14/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.57it/s, loss=0.7959]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5759 valid-beta4-score:0.5498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Training]: 100%|██████████| 280/280 [02:39<00:00,  1.75it/s, loss=0.5695]\n",
      "Epoch 15/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.61it/s, loss=0.7620]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5695 valid-beta4-score:0.5465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:37xbuauq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969e141fe1aa488d9034fea9c98497d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>01_epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>02_train_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>03_valid_loss</td><td>▇█▅▅▁▅▃▃▂▂▄▃▃▇▅</td></tr><tr><td>04_valid_best_score</td><td>▁▅▆▅▇▇█▇█▇█▇███</td></tr><tr><td>apo-ferritin</td><td>▁▂████▂███▂████</td></tr><tr><td>apoo_ferritin_f4</td><td>▁▇█▆█▇████▇▇▇▇█</td></tr><tr><td>apoo_ferritin_p</td><td>▁█▆▃▇▅▆▆▇▇▆▆▅▆▆</td></tr><tr><td>apoo_ferritin_r</td><td>▁▇█▇███████▇▇▇█</td></tr><tr><td>beta-amylase</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>beta-galactosidase</td><td>▁▁▅▅█▁▁█▅▂█▁███</td></tr><tr><td>beta_amylase_f4</td><td>▁▂▃▄▄▅▇▅█▆▆▆▇▇▆</td></tr><tr><td>beta_amylase_p</td><td>▁▂▂▃▃▄█▄█▅▆▆▆▆▆</td></tr><tr><td>beta_amylase_r</td><td>▄▁▆█▇▇▆▇██▆▇██▇</td></tr><tr><td>beta_galactosidase_f4</td><td>▁▄▄▄▆▅█▇▇▇█▆█▇▇</td></tr><tr><td>beta_galactosidase_p</td><td>▁▄▃▃▅▄█▆▇▆█▅▇█▇</td></tr><tr><td>beta_galactosidase_r</td><td>▆▁█▄▇▇█▇▆█▇▇▇▇▇</td></tr><tr><td>ribosome</td><td>██▄██▁█████████</td></tr><tr><td>ribosome_f4</td><td>▃██▁▅▆▇▆▇▅▆▇▇▇▆</td></tr><tr><td>ribosome_p</td><td>▄█▆▁▄▃▅▄▆▃▄▄▅▅▄</td></tr><tr><td>ribosome_r</td><td>▂██▁▅▆▇▆▇▆▇▇▇▇▇</td></tr><tr><td>thyroglobulin</td><td>▂▁██▅█▅█▂██▅███</td></tr><tr><td>thyroglobulin_f4</td><td>▁▄▆▇█▇█▇█▇█▇██▇</td></tr><tr><td>thyroglobulin_p</td><td>▁▄▄▅▆▅█▄▆▅▇▅▆▆▅</td></tr><tr><td>thyroglobulin_r</td><td>▁▃█▇██▇████████</td></tr><tr><td>virus-like-particle</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>virus_like_particle_f4</td><td>█▁▁▄▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>virus_like_particle_p</td><td>█▁▁▇▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>virus_like_particle_r</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>01_epoch</td><td>14</td></tr><tr><td>02_train_loss</td><td>0.56955</td></tr><tr><td>03_valid_loss</td><td>0.76203</td></tr><tr><td>04_valid_best_score</td><td>0.54648</td></tr><tr><td>apo-ferritin</td><td>0.3</td></tr><tr><td>apoo_ferritin_f4</td><td>0.81727</td></tr><tr><td>apoo_ferritin_p</td><td>0.40472</td></tr><tr><td>apoo_ferritin_r</td><td>0.87288</td></tr><tr><td>beta-amylase</td><td>0.05</td></tr><tr><td>beta-galactosidase</td><td>0.3</td></tr><tr><td>beta_amylase_f4</td><td>0.2895</td></tr><tr><td>beta_amylase_p</td><td>0.0264</td></tr><tr><td>beta_amylase_r</td><td>0.76786</td></tr><tr><td>beta_galactosidase_f4</td><td>0.51643</td></tr><tr><td>beta_galactosidase_p</td><td>0.07126</td></tr><tr><td>beta_galactosidase_r</td><td>0.84722</td></tr><tr><td>ribosome</td><td>0.3</td></tr><tr><td>ribosome_f4</td><td>0.64545</td></tr><tr><td>ribosome_p</td><td>0.19832</td></tr><tr><td>ribosome_r</td><td>0.75132</td></tr><tr><td>thyroglobulin</td><td>0.3</td></tr><tr><td>thyroglobulin_f4</td><td>0.66488</td></tr><tr><td>thyroglobulin_p</td><td>0.11957</td></tr><tr><td>thyroglobulin_r</td><td>0.92994</td></tr><tr><td>virus-like-particle</td><td>0.05</td></tr><tr><td>virus_like_particle_f4</td><td>0.0</td></tr><tr><td>virus_like_particle_p</td><td>0.0</td></tr><tr><td>virus_like_particle_r</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp076-recreate-baseline-renet34d-dstride_TS_6_4</strong> at: <a href='https://wandb.ai/latent-walkers/czii2024/runs/37xbuauq' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/37xbuauq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250201_130058-37xbuauq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:37xbuauq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783805fae3444c893a8a4be386874d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111326178910935, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp076-recreate-baseline-renet34d-dstride/wandb/run-20250201_141759-ba7kkqb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/ba7kkqb8' target=\"_blank\">exp076-recreate-baseline-renet34d-dstride_TS_86_3</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/ba7kkqb8' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/ba7kkqb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_exp_name ['TS_86_3', 'TS_6_6']\n",
      "valid_exp_name ['TS_73_6', 'TS_99_9', 'TS_6_4', 'TS_69_2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [01:12<00:00,  7.69it/s]\n",
      "100%|██████████| 4/4 [02:10<00:00, 32.62s/it]\n",
      "Epoch 1/15 [Training]: 100%|██████████| 280/280 [03:01<00:00,  1.54it/s, loss=0.8665]\n",
      "Epoch 1/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s, loss=0.8340]\n",
      "100%|██████████| 6/6 [00:57<00:00,  9.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8665 valid-beta4-score:0.1317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Training]: 100%|██████████| 280/280 [02:53<00:00,  1.61it/s, loss=0.7388]\n",
      "Epoch 2/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.50it/s, loss=0.7696]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7388 valid-beta4-score:0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Training]: 100%|██████████| 280/280 [09:06<00:00,  1.95s/it, loss=0.6973]\n",
      "Epoch 3/15 [Validation]: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s, loss=0.6919]\n",
      "100%|██████████| 6/6 [00:52<00:00,  8.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6973 valid-beta4-score:0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Training]: 100%|██████████| 280/280 [03:02<00:00,  1.54it/s, loss=0.6782]\n",
      "Epoch 4/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s, loss=0.6853]\n",
      "100%|██████████| 6/6 [00:54<00:00,  9.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6782 valid-beta4-score:0.4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Training]: 100%|██████████| 280/280 [02:47<00:00,  1.67it/s, loss=0.6599]\n",
      "Epoch 5/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.50it/s, loss=0.7459]\n",
      "100%|██████████| 6/6 [00:55<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6599 valid-beta4-score:0.5373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Training]: 100%|██████████| 280/280 [03:23<00:00,  1.38it/s, loss=0.6371]\n",
      "Epoch 6/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.27it/s, loss=0.7547]\n",
      "100%|██████████| 6/6 [00:55<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6371 valid-beta4-score:0.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Training]: 100%|██████████| 280/280 [02:56<00:00,  1.59it/s, loss=0.6102]\n",
      "Epoch 7/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s, loss=0.7332]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6102 valid-beta4-score:0.5652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Training]: 100%|██████████| 280/280 [02:49<00:00,  1.65it/s, loss=0.6000]\n",
      "Epoch 8/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s, loss=0.8329]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6000 valid-beta4-score:0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Training]: 100%|██████████| 280/280 [02:45<00:00,  1.69it/s, loss=0.5978]\n",
      "Epoch 9/15 [Validation]: 100%|██████████| 4/4 [00:01<00:00,  2.49it/s, loss=0.6940]\n",
      "100%|██████████| 6/6 [00:56<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5978 valid-beta4-score:0.5572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Training]: 100%|██████████| 280/280 [03:25<00:00,  1.37it/s, loss=0.5798]\n",
      "Epoch 10/15 [Validation]: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s, loss=0.6945]\n",
      " 33%|███▎      | 2/6 [00:19<00:39,  9.89s/it]"
     ]
    }
   ],
   "source": [
    "for train_exp_name in [\n",
    "    [\"TS_73_6\", \"TS_99_9\"],\n",
    "    [\"TS_6_4\", \"TS_69_2\"],\n",
    "    [\"TS_86_3\", \"TS_6_6\"],\n",
    "]:\n",
    "    wandb.init(\n",
    "        project=\"czii2024\", name=f\"{notebook_name}_{train_exp_name[0]}\", config=param\n",
    "    )\n",
    "\n",
    "    # vaild_exp_name = [vaild_exp_name]\n",
    "    # train_exp_name = CFG.train_exp_names.copy()\n",
    "    # train_exp_name.remove(vaild_exp_name[0])\n",
    "\n",
    "    prime_train_exp_name = train_exp_name\n",
    "    valid_exp_name = [\"TS_73_6\", \"TS_99_9\", \"TS_6_4\", \"TS_69_2\", \"TS_86_3\", \"TS_6_6\"]\n",
    "\n",
    "    for v in train_exp_name:\n",
    "        valid_exp_name.remove(v)\n",
    "\n",
    "    prime_train_exp_name = \"_\".join(prime_train_exp_name)\n",
    "\n",
    "    print(\"train_exp_name\", train_exp_name)\n",
    "    print(\"valid_exp_name\", valid_exp_name)\n",
    "\n",
    "    train_exp_name = CFG.train_exp_names.copy()\n",
    "\n",
    "    for v in valid_exp_name:\n",
    "        train_exp_name.remove(v)\n",
    "\n",
    "    # valid_exp_name[0]の名前でディレクトリを作成\n",
    "    os.makedirs(f\"./{prime_train_exp_name}\", exist_ok=True)\n",
    "\n",
    "    train_dataset = EziiDataset(\n",
    "        exp_names=train_exp_name,\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "        particles_name=CFG.particles_name,\n",
    "        resolution=CFG.resolution,\n",
    "        zarr_type=CFG.train_zarr_types,\n",
    "        train=True,\n",
    "        augmentation=True,\n",
    "        slice=True,\n",
    "        pre_read=True,\n",
    "    )\n",
    "    valid_dataset = EziiDataset(\n",
    "        exp_names=valid_exp_name,\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "        particles_name=CFG.particles_name,\n",
    "        resolution=CFG.resolution,\n",
    "        zarr_type=CFG.valid_zarr_types,\n",
    "        augmentation=False,\n",
    "        train=True,\n",
    "        slice=True,\n",
    "        pre_read=True,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "\n",
    "    encoder = timm.create_model(\n",
    "        model_name=CFG.model_name,\n",
    "        pretrained=True,\n",
    "        in_chans=3,\n",
    "        num_classes=0,\n",
    "        global_pool=\"\",\n",
    "        features_only=True,\n",
    "    )\n",
    "    model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    criterion = DiceLoss()\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=10,\n",
    "        num_training_steps=CFG.epochs * len(train_loader),\n",
    "        # * batch_size,\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    seg_loss = SegmentationLoss(criterion)\n",
    "    padf = PadToSize(CFG.resolution)\n",
    "\n",
    "    best_model = None\n",
    "    best_constant = 0\n",
    "    best_score = -100\n",
    "    best_particle_score = {}\n",
    "\n",
    "    grand_train_loss = []\n",
    "    grand_valid_loss = []\n",
    "    grand_train_score = []\n",
    "    grand_valid_score = []\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        with tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\"\n",
    "        ) as tq:\n",
    "            for data in tq:\n",
    "                normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "                segmentation_map = data[\"segmentation_map\"]\n",
    "                zarr_embedding_idx = data[\"zarr_type_embedding_idx\"]\n",
    "\n",
    "                normalized_tomogram = padf(normalized_tomogram)\n",
    "                segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                # データ拡張\n",
    "                normalized_tomogram, segmentation_map = augment_data(\n",
    "                    normalized_tomogram, segmentation_map, p=CFG.augmentation_prob\n",
    "                )\n",
    "                normalized_tomogram = normalized_tomogram.cuda()\n",
    "                segmentation_map = segmentation_map.long().cuda()\n",
    "                zarr_embedding_idx = zarr_embedding_idx.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast():\n",
    "                    pred = model(\n",
    "                        preprocess_tensor(normalized_tomogram), zarr_embedding_idx\n",
    "                    )\n",
    "                    loss = seg_loss(pred, segmentation_map)\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                # 確率予測\n",
    "                prob_pred = torch.softmax(pred, dim=1)\n",
    "                tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "        del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        with tqdm(\n",
    "            valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\"\n",
    "        ) as tq:\n",
    "            with torch.no_grad():\n",
    "                for data in tq:\n",
    "                    normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "                    segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "                    zarr_embedding_idx = data[\"zarr_type_embedding_idx\"].cuda()\n",
    "\n",
    "                    normalized_tomogram = padf(normalized_tomogram)\n",
    "                    segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                    with autocast():\n",
    "                        pred = model(\n",
    "                            preprocess_tensor(normalized_tomogram), zarr_embedding_idx\n",
    "                        )\n",
    "                        loss = seg_loss(pred, segmentation_map)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "                    # 確率予測\n",
    "                    prob_pred = torch.softmax(pred, dim=1)\n",
    "                    tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "        del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # # ############### validation ################\n",
    "        train_nshuffle_original_tomogram = defaultdict(list)\n",
    "        train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "        train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        valid_original_tomogram = defaultdict(list)\n",
    "        valid_pred_tomogram = defaultdict(list)\n",
    "        valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        train_mean_scores = []\n",
    "        valid_mean_scores = []\n",
    "\n",
    "        # モデルの保存\n",
    "        make_dir_ = (\n",
    "            f\"../../../../../../../../mnt/d/kaggle-tmp-models/czii2024/{notebook_name}/\"\n",
    "        )\n",
    "        os.makedirs(make_dir_, exist_ok=True)\n",
    "        torch.save(model.state_dict(), make_dir_ + f\"model_{epoch}.pth\")\n",
    "\n",
    "        # ############### validation ################\n",
    "        train_nshuffle_original_tomogram = defaultdict(list)\n",
    "        train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "        train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        valid_original_tomogram = defaultdict(list)\n",
    "        valid_pred_tomogram = defaultdict(list)\n",
    "        valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "        train_mean_scores = []\n",
    "        valid_mean_scores = []\n",
    "\n",
    "        train_inferenced_array = {}\n",
    "        train_pred_array = []\n",
    "        train_gt_array = []\n",
    "        valid_inferenced_array = {}\n",
    "        valid_gt_array = []\n",
    "\n",
    "        # for exp_name in tqdm(CFG.train_exp_names):\n",
    "        for exp_name in valid_exp_name:  # 5つのデータで試す\n",
    "            # inferenced_array = inference(model, exp_name, train=False)\n",
    "            inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "                model, exp_name, train=False\n",
    "            )\n",
    "            valid_inferenced_array[exp_name] = inferenced_array\n",
    "            base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "            gt_df = create_gt_df(base_dir, [exp_name])\n",
    "            valid_gt_array.append(gt_df)\n",
    "\n",
    "        valid_gt_array = pd.concat(valid_gt_array)\n",
    "\n",
    "        b_constant = 0\n",
    "        b_score = -100\n",
    "        b_particle_score = {}\n",
    "\n",
    "        try:\n",
    "            best_thresholds, final_score = reduce_computation_sikii_search(\n",
    "                inferenced_array,\n",
    "                exp_name,\n",
    "                [\n",
    "                    0.05,\n",
    "                    0.1,\n",
    "                    0.2,\n",
    "                    0.3,\n",
    "                    # 0.4,\n",
    "                    # 0.5,\n",
    "                    # 0.6,\n",
    "                    # 0.7,\n",
    "                ],\n",
    "            )\n",
    "        except:\n",
    "            best_thresholds = [0.5] * 6\n",
    "            final_score = -50\n",
    "\n",
    "        b_score = final_score\n",
    "        b_particle_constant = {\n",
    "            \"apo-ferritin\": best_thresholds[0],\n",
    "            \"beta-amylase\": best_thresholds[1],\n",
    "            \"beta-galactosidase\": best_thresholds[2],\n",
    "            \"ribosome\": best_thresholds[3],\n",
    "            \"thyroglobulin\": best_thresholds[4],\n",
    "            \"virus-like-particle\": best_thresholds[5],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            valid_pred_array = []\n",
    "            for exp_name in valid_exp_name:\n",
    "                pred_df = inference2pos(\n",
    "                    pred_segmask=valid_inferenced_array[exp_name],\n",
    "                    exp_name=exp_name,\n",
    "                    sikii_dict=b_particle_constant,\n",
    "                )\n",
    "                valid_pred_array.append(pred_df)\n",
    "\n",
    "            valid_pred_array = pd.concat(valid_pred_array)\n",
    "\n",
    "            if len(valid_pred_array) != 0:\n",
    "                result_df, score_ = compute_lb(\n",
    "                    valid_pred_array,\n",
    "                    \"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "                    valid_exp_name,\n",
    "                )\n",
    "                particle_score = extract_particle_results(result_df)\n",
    "\n",
    "                b_score = score_\n",
    "                b_particle_score = particle_score\n",
    "        except:\n",
    "            b_score = -50\n",
    "            b_particle_score = {}\n",
    "\n",
    "        import gc\n",
    "        import torch.cuda as cuda\n",
    "\n",
    "        # del valid_pred_array, valid_gt_array\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "\n",
    "        # print(\"constant\", b_constant, \"score\", b_score)\n",
    "\n",
    "        # wandb-log\n",
    "        train_info = {\n",
    "            \"01_epoch\": epoch,\n",
    "            \"02_train_loss\": np.mean(train_loss),\n",
    "            \"03_valid_loss\": np.mean(valid_loss),\n",
    "            # \"train_score\": np.mean(train_mean_scores),\n",
    "            \"04_valid_best_score\": b_score,\n",
    "        }\n",
    "        train_info = {**train_info, **b_particle_score}\n",
    "        train_info = {**train_info, **b_particle_constant}\n",
    "        wandb.log(train_info)\n",
    "\n",
    "        # score-update\n",
    "        if b_score > best_score:\n",
    "            best_score = b_score\n",
    "            # best_score = np.mean(valid_mean_scores)\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, f\"./{prime_train_exp_name}/best_model.pth\")\n",
    "\n",
    "        print(\n",
    "            f\"train-epoch-loss:{np.mean(train_loss):.4f}\",\n",
    "            # f\"valid-epoch-loss:{np.mean(valid_loss):.4f}\",\n",
    "            # f\"train-beta4-score:{np.mean(train_mean_scores):.4f}\",\n",
    "            f\"valid-beta4-score:{b_score:.4f}\",\n",
    "        )\n",
    "\n",
    "        grand_train_loss.append(np.mean(train_loss))\n",
    "        # grand_valid_loss.append(np.mean(valid_loss))\n",
    "        # grand_train_score.append(np.mean(train_mean_scores))\n",
    "        grand_valid_score.append(b_score)\n",
    "\n",
    "    del model, optimizer, criterion, scheduler, scaler, seg_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lossとvalid_lossのプロット\n",
    "\n",
    "plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scoreとvalid_scoreのプロット\n",
    "plt.plot(grand_train_score, label=\"train_score\")\n",
    "plt.plot(grand_valid_score, label=\"valid_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
