{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatuya\u001b[0m (\u001b[33mlatent-walkers\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tatsuya/code/projects/kaggle/CryoET/experiments/exp067-recreate-baseline-renet34d-dstride-attention/wandb/run-20250127_073147-r7u05u4n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/latent-walkers/czii2024/runs/r7u05u4n' target=\"_blank\">exp067-recreate-baseline-renet34d-dstride-attention</a></strong> to <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/latent-walkers/czii2024' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/latent-walkers/czii2024/runs/r7u05u4n' target=\"_blank\">https://wandb.ai/latent-walkers/czii2024/runs/r7u05u4n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/latent-walkers/czii2024/runs/r7u05u4n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f87c5a2dc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from src.kaggle_notebook_metric import compute_lb, extract_particle_results\n",
    "from src.inference import inference, inference2pos, create_gt_df\n",
    "from metric import visualize_epoch_results\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_name = os.path.join(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "param = {\n",
    "    \"model\": CFG.model_name,\n",
    "    \"resolution\": CFG.resolution,\n",
    "    \"augmentation_prob\": CFG.augmentation_prob,\n",
    "    \"slice\": CFG.slice_,\n",
    "    \"epochs\": CFG.epochs,\n",
    "    \"lr\": CFG.lr,\n",
    "    \"batch_size\": CFG.batch_size,\n",
    "    \"weight_decay\": CFG.weight_decay,\n",
    "    \"num_workers\": CFG.num_workers,\n",
    "    \"augment_data_ratio\": CFG.augment_data_ratio,\n",
    "}\n",
    "wandb.init(project=\"czii2024\", name=notebook_name, config=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:35<00:00,  7.46it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "  0%|          | 0/264 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 630, 630])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    "    augmentation=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "# train_nshuffle_dataset = EziiDataset(\n",
    "#     exp_names=CFG.train_exp_names,\n",
    "#     base_dir=\"../../inputs/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.train_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    augmentation=False,\n",
    "    train=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# sikii値とexp_namesを入れるとスコアを出力する関数\n",
    "\n",
    "\n",
    "def compute_score(sikii_list, inferenced_array, exp_name):\n",
    "    apo_ferritin = sikii_list[0]\n",
    "    beta_amylase = sikii_list[1]\n",
    "    beta_galactosidase = sikii_list[2]\n",
    "    ribosome = sikii_list[3]\n",
    "    thyroglobulin = sikii_list[4]\n",
    "    virus_like_particle = sikii_list[5]\n",
    "\n",
    "    sikii_dict = {\n",
    "        \"apo-ferritin\": apo_ferritin,\n",
    "        \"beta-amylase\": beta_amylase,\n",
    "        \"beta-galactosidase\": beta_galactosidase,\n",
    "        \"ribosome\": ribosome,\n",
    "        \"thyroglobulin\": thyroglobulin,\n",
    "        \"virus-like-particle\": virus_like_particle,\n",
    "    }\n",
    "\n",
    "    all_pred = []\n",
    "\n",
    "    pred_df = inference2pos(\n",
    "        pred_segmask=inferenced_array, exp_name=exp_name, sikii_dict=sikii_dict\n",
    "    )\n",
    "\n",
    "    all_pred.append(pred_df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df = pd.concat(all_pred, axis=0).reset_index(drop=True)\n",
    "    pred_df = pred_df[pred_df[\"particle_type\"] != \"beta-amylase\"]\n",
    "    pred_df = pred_df.drop_duplicates(\n",
    "        subset=[\"experiment\", \"x\", \"y\", \"z\"], keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    pred_df = pred_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", [exp_name])\n",
    "\n",
    "    result_df, lb_score = compute_lb(\n",
    "        pred_df, \"../../inputs/train/overlay/ExperimentRuns/\", [exp_name]\n",
    "    )\n",
    "\n",
    "    return lb_score\n",
    "\n",
    "\n",
    "def reduce_computation_sikii_search(\n",
    "    inferenced_array: np.ndarray, exp_name: str, threshold_candidates: list[float]\n",
    ") -> tuple[list[float], float]:\n",
    "    \"\"\"\n",
    "    # How\n",
    "    6つのしきい値が互いに独立してスコアに貢献しているという前提で、\n",
    "    1次元ずつ最適なしきい値を探す手法を実装する.\n",
    "\n",
    "    1. 初期の best_thresholds (全要素 0.5 など適当な値) を用意\n",
    "    2. i=0 から i=5 まで順番に:\n",
    "       - threshold_candidates をすべて試し、他は固定したまま i 番目だけ変化させてスコアを計算\n",
    "       - 最良スコアが得られる候補値を確定し、best_thresholds[i] とする\n",
    "    3. 全部決まったら最終的なスコアを計算して返す\n",
    "\n",
    "    これにより、全組み合わせ (product) を回すよりも計算量が大幅に減少する.\n",
    "    \"\"\"\n",
    "    # Why not: 6値独立であるという前提が満たされていない場合、近似解になる可能性あり\n",
    "    best_thresholds = [0.5] * 6  # 適当な初期値でOK\n",
    "\n",
    "    for i in tqdm(range(6)):\n",
    "        best_local_score = -float(\"inf\")\n",
    "        best_local_value = None\n",
    "\n",
    "        for candidate in threshold_candidates:\n",
    "            current_thresholds = best_thresholds[:]  # 現在のベストを複製\n",
    "            current_thresholds[i] = candidate\n",
    "            score = compute_score(current_thresholds, inferenced_array, exp_name)\n",
    "            if score > best_local_score:\n",
    "                best_local_score = score\n",
    "                best_local_value = candidate\n",
    "\n",
    "        # i番目のしきい値を最適値に更新\n",
    "        best_thresholds[i] = best_local_value\n",
    "\n",
    "    final_score = compute_score(best_thresholds, inferenced_array, exp_name)\n",
    "    return best_thresholds, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "# model.load_state_dict(torch.load(\"./pretrained_model.pth\"))\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         \"../../../../../../../../mnt/d/kaggle-tmp-models/czii2024/exp059-recreate-baseline-renet34d-dstride-attention/model_4.pth\"\n",
    "#     )\n",
    "# )\n",
    "# model.load_state_dict(torch.load(\"./best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 16, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input-test\n",
    "\n",
    "x = torch.randn(2, 16, 1, 64, 64).cuda()\n",
    "model(x, torch.tensor([2, 0]).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"encoder\"と名のつくパラメータは学習しない\n",
    "# for layer, param in model.named_parameters():\n",
    "#     if \"encoder\" in layer:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# サンプルデータ\n",
    "num_classes = len(CFG.particles_name)  # クラス数\n",
    "colors = plt.cm.tab10(\n",
    "    np.arange(len(CFG.particles_name))\n",
    ")  # \"tab10\" カラーマップから色を取得\n",
    "\n",
    "# ListedColormap を作成\n",
    "class_colormap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "# カラーバー付きプロット\n",
    "def plot_with_colormap(data, title, original_tomogram):\n",
    "    masked_data = np.ma.masked_where(data <= 0, data)  # クラス0をマスク\n",
    "    plt.imshow(original_tomogram, cmap=\"gray\")\n",
    "    im = plt.imshow(masked_data, cmap=class_colormap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([6, 16, 320, 320])\n",
      "Augmented shape: torch.Size([6, 16, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# 回転\n",
    "# 3Dテンソルの各軸に対して指定した角度で回転する関数\n",
    "def rotate_3d(tomogram, segmentation_map, angle):\n",
    "    \"\"\"Rotates the 3D tensors tomogram and segmentation_map around the Z-axis.\"\"\"\n",
    "    rotated_tomogram = TF.rotate(tomogram, angle, expand=False)\n",
    "    rotated_segmentation_map = TF.rotate(segmentation_map, angle, expand=False)\n",
    "    return rotated_tomogram, rotated_segmentation_map\n",
    "\n",
    "\n",
    "# 平行移動\n",
    "# 指定された範囲でランダムに平行移動\n",
    "def translate_3d(tomogram, segmentation_map, max_shift):\n",
    "    \"\"\"Translates the 3D tensors by a random shift within max_shift.\"\"\"\n",
    "    shift_x = random.randint(-max_shift, max_shift)\n",
    "    shift_y = random.randint(-max_shift, max_shift)\n",
    "    translated_tomogram = TF.affine(\n",
    "        tomogram, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    translated_segmentation_map = TF.affine(\n",
    "        segmentation_map, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    return translated_tomogram, translated_segmentation_map\n",
    "\n",
    "\n",
    "# フリップ\n",
    "# 縦横（上下左右）ランダムフリップ\n",
    "def flip_3d(tomogram, segmentation_map):\n",
    "    \"\"\"Randomly flips the 3D tensors along height or width.\"\"\"\n",
    "    if random.random() > 0.5:  # Horizontal flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-1])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-1])\n",
    "    if random.random() > 0.5:  # Vertical flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-2])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-2])\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# クロッピング\n",
    "# 入力テンソルを中心またはランダムクロップで切り取る\n",
    "def crop_3d(tomogram, segmentation_map, crop_size):\n",
    "    \"\"\"Crops the 3D tensors to the specified crop_size.\"\"\"\n",
    "    _, depth, height, width = tomogram.size()\n",
    "    crop_d, crop_h, crop_w = crop_size\n",
    "\n",
    "    if crop_h > height or crop_w > width:\n",
    "        raise ValueError(\"Crop size cannot be larger than the original size.\")\n",
    "\n",
    "    start_h = random.randint(0, height - crop_h)  # Random starting position for height\n",
    "    start_w = random.randint(0, width - crop_w)  # Random starting position for width\n",
    "\n",
    "    cropped_tomogram = tomogram[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "    cropped_segmentation_map = segmentation_map[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "\n",
    "    return cropped_tomogram, cropped_segmentation_map\n",
    "\n",
    "\n",
    "# Mixup\n",
    "# 2つのサンプルを線形補間して混合\n",
    "def mixup(tomogram, segmentation_map, alpha=0.4):\n",
    "    \"\"\"Applies mixup augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size = tomogram.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_tomogram = lam * tomogram + (1 - lam) * tomogram[index, :]\n",
    "    mixed_segmentation_map = (\n",
    "        lam * segmentation_map + (1 - lam) * segmentation_map[index, :]\n",
    "    )\n",
    "\n",
    "    return mixed_tomogram, mixed_segmentation_map\n",
    "\n",
    "\n",
    "# Cutmix\n",
    "# ランダム領域を切り取って別のサンプルに貼り付け\n",
    "def cutmix(tomogram, segmentation_map, alpha=1.0):\n",
    "    \"\"\"Applies cutmix augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size, depth, height, width = tomogram.size()\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    cx = random.randint(0, width)\n",
    "    cy = random.randint(0, height)\n",
    "    cw = int(width * (1 - lam))\n",
    "    ch = int(height * (1 - lam))\n",
    "\n",
    "    x1 = max(cx - cw // 2, 0)\n",
    "    x2 = min(cx + cw // 2, width)\n",
    "    y1 = max(cy - ch // 2, 0)\n",
    "    y2 = min(cy + ch // 2, height)\n",
    "\n",
    "    tomogram[:, :, y1:y2, x1:x2] = tomogram[index, :, y1:y2, x1:x2]\n",
    "    segmentation_map[:, :, y1:y2, x1:x2] = segmentation_map[index, :, y1:y2, x1:x2]\n",
    "\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# データ拡張の組み合わせ適用\n",
    "def augment_data(\n",
    "    tomogram,\n",
    "    segmentation_map,\n",
    "    crop_size=(16, 256, 256),\n",
    "    max_shift=10,\n",
    "    rotation_angle=30,\n",
    "    p=0.5,\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "):\n",
    "    \"\"\"Applies a combination of rotation, translation, flipping, cropping, mixup, and cutmix to the inputs with probabilities.\"\"\"\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = rotate_3d(\n",
    "            tomogram,\n",
    "            segmentation_map,\n",
    "            angle=random.uniform(-rotation_angle, rotation_angle),\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = translate_3d(\n",
    "            tomogram, segmentation_map, max_shift=max_shift\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = flip_3d(tomogram, segmentation_map)\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = crop_3d(\n",
    "            tomogram, segmentation_map, crop_size=crop_size\n",
    "        )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = mixup(\n",
    "    #         tomogram, segmentation_map, alpha=mixup_alpha\n",
    "    #     )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = cutmix(\n",
    "    #         tomogram, segmentation_map, alpha=cutmix_alpha\n",
    "    #     )\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# バッチサイズ6, 深さ16, 高さ320, 幅320のランダムテンソル\n",
    "tomogram = torch.rand((6, 16, 320, 320))\n",
    "segmentation_map = torch.randint(0, 2, (6, 16, 320, 320))  # ラベルは0または1\n",
    "\n",
    "# データ拡張の適用\n",
    "aug_tomogram, aug_segmentation_map = augment_data(tomogram, segmentation_map, p=0.7)\n",
    "print(\"Original shape:\", tomogram.shape)\n",
    "print(\"Augmented shape:\", aug_tomogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoderのパラメータを固定\n",
    "\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    ")\n",
    "# criterion = nn.CrossEntropyLoss(\n",
    "#     #  weight=torch.tensor([2.0, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    "# )\n",
    "criterion = DiceLoss()\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=CFG.epochs * len(train_loader),\n",
    "    # * batch_size,\n",
    ")\n",
    "scaler = GradScaler()\n",
    "seg_loss = SegmentationLoss(criterion)\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 640, 640])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padf = PadToSize(CFG.resolution)\n",
    "padf(normalized_tomogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Training]: 100%|██████████| 264/264 [01:04<00:00,  4.09it/s, loss=0.9033]\n",
      "Epoch 1/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, loss=0.8437]\n",
      "100%|██████████| 6/6 [03:31<00:00, 35.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.9033 valid-beta4-score:0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 [Training]: 100%|██████████| 264/264 [01:06<00:00,  3.98it/s, loss=0.8318]\n",
      "Epoch 2/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, loss=0.8284]\n",
      "100%|██████████| 6/6 [03:27<00:00, 34.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8318 valid-beta4-score:0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 [Training]: 100%|██████████| 264/264 [01:10<00:00,  3.73it/s, loss=0.8165]\n",
      "Epoch 3/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, loss=0.8180]\n",
      "100%|██████████| 6/6 [03:26<00:00, 34.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8165 valid-beta4-score:0.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 [Training]: 100%|██████████| 264/264 [01:02<00:00,  4.21it/s, loss=0.8042]\n",
      "Epoch 4/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, loss=0.8125]\n",
      "100%|██████████| 6/6 [03:30<00:00, 35.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.8042 valid-beta4-score:0.3869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 [Training]: 100%|██████████| 264/264 [01:03<00:00,  4.17it/s, loss=0.7752]\n",
      "Epoch 5/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, loss=0.6898]\n",
      "100%|██████████| 6/6 [03:28<00:00, 34.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7752 valid-beta4-score:0.4653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80 [Training]: 100%|██████████| 264/264 [01:06<00:00,  3.96it/s, loss=0.7768]\n",
      "Epoch 6/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, loss=0.6328]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7768 valid-beta4-score:0.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 [Training]: 100%|██████████| 264/264 [01:06<00:00,  3.97it/s, loss=0.7719]\n",
      "Epoch 7/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, loss=0.8577]\n",
      "100%|██████████| 6/6 [03:36<00:00, 36.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7719 valid-beta4-score:0.3945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80 [Training]: 100%|██████████| 264/264 [01:03<00:00,  4.13it/s, loss=0.7493]\n",
      "Epoch 8/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, loss=0.6461]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7493 valid-beta4-score:0.3538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80 [Training]: 100%|██████████| 264/264 [01:04<00:00,  4.08it/s, loss=0.7355]\n",
      "Epoch 9/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, loss=0.6566]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7355 valid-beta4-score:0.4024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 [Training]: 100%|██████████| 264/264 [01:08<00:00,  3.84it/s, loss=0.7316]\n",
      "Epoch 10/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, loss=0.8578]\n",
      "100%|██████████| 6/6 [03:30<00:00, 35.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7316 valid-beta4-score:0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80 [Training]: 100%|██████████| 264/264 [01:03<00:00,  4.16it/s, loss=0.7388]\n",
      "Epoch 11/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.5745]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7388 valid-beta4-score:0.3870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 [Training]: 100%|██████████| 264/264 [01:11<00:00,  3.71it/s, loss=0.7201]\n",
      "Epoch 12/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, loss=0.5947]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7201 valid-beta4-score:0.5793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 [Training]: 100%|██████████| 264/264 [01:08<00:00,  3.84it/s, loss=0.7110]\n",
      "Epoch 13/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.6050]\n",
      "100%|██████████| 6/6 [03:45<00:00, 37.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7110 valid-beta4-score:0.4239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/80 [Training]: 100%|██████████| 264/264 [01:04<00:00,  4.07it/s, loss=0.7048]\n",
      "Epoch 14/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, loss=0.7755]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7048 valid-beta4-score:0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/80 [Training]: 100%|██████████| 264/264 [01:08<00:00,  3.86it/s, loss=0.6919]\n",
      "Epoch 15/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.5681]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6919 valid-beta4-score:0.5718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/80 [Training]: 100%|██████████| 264/264 [01:09<00:00,  3.82it/s, loss=0.7222]\n",
      "Epoch 16/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, loss=0.8574]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7222 valid-beta4-score:0.5487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/80 [Training]: 100%|██████████| 264/264 [01:04<00:00,  4.10it/s, loss=0.7065]\n",
      "Epoch 17/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, loss=0.4970]\n",
      "100%|██████████| 6/6 [03:34<00:00, 35.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7065 valid-beta4-score:0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/80 [Training]: 100%|██████████| 264/264 [01:06<00:00,  3.95it/s, loss=0.7019]\n",
      "Epoch 18/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, loss=0.8576]\n",
      "100%|██████████| 6/6 [03:33<00:00, 35.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7019 valid-beta4-score:0.4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/80 [Training]: 100%|██████████| 264/264 [01:08<00:00,  3.86it/s, loss=0.7004]\n",
      "Epoch 19/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, loss=0.5076]\n",
      "100%|██████████| 6/6 [03:43<00:00, 37.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.7004 valid-beta4-score:0.4768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/80 [Training]: 100%|██████████| 264/264 [01:09<00:00,  3.82it/s, loss=0.6744]\n",
      "Epoch 20/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.7591]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6744 valid-beta4-score:0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/80 [Training]: 100%|██████████| 264/264 [01:09<00:00,  3.77it/s, loss=0.6892]\n",
      "Epoch 21/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, loss=0.4750]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6892 valid-beta4-score:0.6052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80 [Training]: 100%|██████████| 264/264 [01:05<00:00,  4.03it/s, loss=0.6875]\n",
      "Epoch 22/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, loss=0.5934]\n",
      "100%|██████████| 6/6 [03:40<00:00, 36.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6875 valid-beta4-score:0.5865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/80 [Training]: 100%|██████████| 264/264 [01:07<00:00,  3.91it/s, loss=0.6825]\n",
      "Epoch 23/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.7608]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6825 valid-beta4-score:0.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80 [Training]: 100%|██████████| 264/264 [01:06<00:00,  3.99it/s, loss=0.6875]\n",
      "Epoch 24/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, loss=0.7336]\n",
      "100%|██████████| 6/6 [03:38<00:00, 36.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6875 valid-beta4-score:0.5839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80 [Training]: 100%|██████████| 264/264 [01:09<00:00,  3.81it/s, loss=0.6754]\n",
      "Epoch 25/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, loss=0.8576]\n",
      "100%|██████████| 6/6 [03:36<00:00, 36.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6754 valid-beta4-score:0.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80 [Training]: 100%|██████████| 264/264 [01:05<00:00,  4.06it/s, loss=0.6696]\n",
      "Epoch 26/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, loss=0.7376]\n",
      "100%|██████████| 6/6 [03:35<00:00, 35.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6696 valid-beta4-score:0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/80 [Training]: 100%|██████████| 264/264 [00:59<00:00,  4.42it/s, loss=0.6893]\n",
      "Epoch 27/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, loss=0.5135]\n",
      "100%|██████████| 6/6 [03:36<00:00, 36.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6893 valid-beta4-score:0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/80 [Training]: 100%|██████████| 264/264 [01:07<00:00,  3.93it/s, loss=0.6774]\n",
      "Epoch 28/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, loss=0.5162]\n",
      "100%|██████████| 6/6 [03:37<00:00, 36.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.6774 valid-beta4-score:0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/80 [Training]: 100%|██████████| 264/264 [01:07<00:00,  3.92it/s, loss=0.5783]\n",
      "Epoch 29/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, loss=0.7028]\n",
      "100%|██████████| 6/6 [03:20<00:00, 33.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.5783 valid-beta4-score:0.2754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/80 [Training]: 100%|██████████| 264/264 [01:07<00:00,  3.93it/s, loss=0.3274]\n",
      "Epoch 30/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.0014]\n",
      "100%|██████████| 6/6 [03:06<00:00, 31.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.3274 valid-beta4-score:0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/80 [Training]: 100%|██████████| 264/264 [01:07<00:00,  3.92it/s, loss=0.3528]\n",
      "Epoch 31/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, loss=0.6481]\n",
      "100%|██████████| 6/6 [03:07<00:00, 31.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-epoch-loss:0.3528 valid-beta4-score:0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80 [Training]: 100%|██████████| 264/264 [01:05<00:00,  4.06it/s, loss=0.3119]\n",
      "Epoch 32/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, loss=0.7254]\n",
      "  0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'particle_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m b_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    130\u001b[0m b_particle_score \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 132\u001b[0m best_thresholds, final_score \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_computation_sikii_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43minferenced_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.55\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.65\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m b_score \u001b[38;5;241m=\u001b[39m final_score\n\u001b[1;32m    155\u001b[0m b_particle_constant \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapo-ferritin\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_thresholds[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta-amylase\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_thresholds[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvirus-like-particle\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_thresholds[\u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m    162\u001b[0m }\n",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m, in \u001b[0;36mreduce_computation_sikii_search\u001b[0;34m(inferenced_array, exp_name, threshold_candidates)\u001b[0m\n\u001b[1;32m     74\u001b[0m current_thresholds \u001b[38;5;241m=\u001b[39m best_thresholds[:]  \u001b[38;5;66;03m# 現在のベストを複製\u001b[39;00m\n\u001b[1;32m     75\u001b[0m current_thresholds[i] \u001b[38;5;241m=\u001b[39m candidate\n\u001b[0;32m---> 76\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_thresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferenced_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_local_score:\n\u001b[1;32m     78\u001b[0m     best_local_score \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mcompute_score\u001b[0;34m(sikii_list, inferenced_array, exp_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     34\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pred_df[\u001b[43mpred_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparticle_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta-amylase\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pred_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(\n\u001b[1;32m     37\u001b[0m     subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pred_df\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/pandas/core/indexes/range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'particle_type'"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_constant = 0\n",
    "best_score = -100\n",
    "best_particle_score = {}\n",
    "\n",
    "grand_train_loss = []\n",
    "grand_valid_loss = []\n",
    "grand_train_score = []\n",
    "grand_valid_score = []\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\") as tq:\n",
    "        for data in tq:\n",
    "            normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "            segmentation_map = data[\"segmentation_map\"]\n",
    "            zarr_embedding_idx = data[\"zarr_type_embedding_idx\"]\n",
    "\n",
    "            normalized_tomogram = padf(normalized_tomogram)\n",
    "            segmentation_map = padf(segmentation_map)\n",
    "\n",
    "            # データ拡張\n",
    "            normalized_tomogram, segmentation_map = augment_data(\n",
    "                normalized_tomogram, segmentation_map, p=CFG.augmentation_prob\n",
    "            )\n",
    "            normalized_tomogram = normalized_tomogram.cuda()\n",
    "            segmentation_map = segmentation_map.long().cuda()\n",
    "            zarr_embedding_idx = zarr_embedding_idx.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(preprocess_tensor(normalized_tomogram), zarr_embedding_idx)\n",
    "                loss = seg_loss(pred, segmentation_map)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # 確率予測\n",
    "            prob_pred = torch.softmax(pred, dim=1)\n",
    "            tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "    del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\") as tq:\n",
    "        with torch.no_grad():\n",
    "            for data in tq:\n",
    "                normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "                segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "                zarr_embedding_idx = data[\"zarr_type_embedding_idx\"].cuda()\n",
    "\n",
    "                normalized_tomogram = padf(normalized_tomogram)\n",
    "                segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                with autocast():\n",
    "                    pred = model(\n",
    "                        preprocess_tensor(normalized_tomogram), zarr_embedding_idx\n",
    "                    )\n",
    "                    loss = seg_loss(pred, segmentation_map)\n",
    "                valid_loss.append(loss.item())\n",
    "\n",
    "                # 確率予測\n",
    "                prob_pred = torch.softmax(pred, dim=1)\n",
    "                tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "    del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    # モデルの保存\n",
    "    make_dir_ = (\n",
    "        f\"../../../../../../../../mnt/d/kaggle-tmp-models/czii2024/{notebook_name}/\"\n",
    "    )\n",
    "    os.makedirs(make_dir_, exist_ok=True)\n",
    "    torch.save(model.state_dict(), make_dir_ + f\"model_{epoch}.pth\")\n",
    "\n",
    "    # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    train_inferenced_array = {}\n",
    "    train_pred_array = []\n",
    "    train_gt_array = []\n",
    "    valid_inferenced_array = {}\n",
    "    valid_gt_array = []\n",
    "\n",
    "    # for exp_name in tqdm(CFG.train_exp_names):\n",
    "    for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "        # inferenced_array = inference(model, exp_name, train=False)\n",
    "        inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "            model, exp_name, train=False\n",
    "        )\n",
    "        valid_inferenced_array[exp_name] = inferenced_array\n",
    "        base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "        gt_df = create_gt_df(base_dir, [exp_name])\n",
    "        valid_gt_array.append(gt_df)\n",
    "\n",
    "    valid_gt_array = pd.concat(valid_gt_array)\n",
    "\n",
    "    b_constant = 0\n",
    "    b_score = -100\n",
    "    b_particle_score = {}\n",
    "\n",
    "    best_thresholds, final_score = reduce_computation_sikii_search(\n",
    "        inferenced_array,\n",
    "        exp_name,\n",
    "        [\n",
    "            0.05,\n",
    "            0.1,\n",
    "            0.15,\n",
    "            0.2,\n",
    "            0.25,\n",
    "            0.3,\n",
    "            0.35,\n",
    "            0.4,\n",
    "            0.45,\n",
    "            0.5,\n",
    "            0.55,\n",
    "            0.6,\n",
    "            0.65,\n",
    "            0.7,\n",
    "            0.75,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    b_score = final_score\n",
    "    b_particle_constant = {\n",
    "        \"apo-ferritin\": best_thresholds[0],\n",
    "        \"beta-amylase\": best_thresholds[1],\n",
    "        \"beta-galactosidase\": best_thresholds[2],\n",
    "        \"ribosome\": best_thresholds[3],\n",
    "        \"thyroglobulin\": best_thresholds[4],\n",
    "        \"virus-like-particle\": best_thresholds[5],\n",
    "    }\n",
    "\n",
    "    valid_pred_array = []\n",
    "    for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "        pred_df = inference2pos(\n",
    "            pred_segmask=valid_inferenced_array[exp_name],\n",
    "            exp_name=exp_name,\n",
    "            sikii_dict=b_particle_constant,\n",
    "        )\n",
    "        valid_pred_array.append(pred_df)\n",
    "\n",
    "    valid_pred_array = pd.concat(valid_pred_array)\n",
    "\n",
    "    if len(valid_pred_array) != 0:\n",
    "        result_df, score_ = compute_lb(\n",
    "            valid_pred_array,\n",
    "            \"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "            CFG.valid_exp_names,\n",
    "        )\n",
    "        particle_score = extract_particle_results(result_df)\n",
    "\n",
    "        b_score = score_\n",
    "        b_particle_score = particle_score\n",
    "\n",
    "    import gc\n",
    "    import torch.cuda as cuda\n",
    "\n",
    "    # del valid_pred_array, valid_gt_array\n",
    "    gc.collect()\n",
    "    cuda.empty_cache()\n",
    "\n",
    "    # print(\"constant\", b_constant, \"score\", b_score)\n",
    "\n",
    "    # wandb-log\n",
    "    train_info = {\n",
    "        \"01_epoch\": epoch,\n",
    "        \"02_train_loss\": np.mean(train_loss),\n",
    "        \"03_valid_loss\": np.mean(valid_loss),\n",
    "        # \"train_score\": np.mean(train_mean_scores),\n",
    "        \"04_valid_best_score\": b_score,\n",
    "    }\n",
    "    train_info = {**train_info, **b_particle_score}\n",
    "    train_info = {**train_info, **b_particle_constant}\n",
    "    wandb.log(train_info)\n",
    "\n",
    "    # score-update\n",
    "    if b_score > best_score:\n",
    "        best_score = b_score\n",
    "        # best_score = np.mean(valid_mean_scores)\n",
    "        best_model = model.state_dict()\n",
    "        torch.save(best_model, f\"./best_model.pth\")\n",
    "\n",
    "    print(\n",
    "        f\"train-epoch-loss:{np.mean(train_loss):.4f}\",\n",
    "        # f\"valid-epoch-loss:{np.mean(valid_loss):.4f}\",\n",
    "        # f\"train-beta4-score:{np.mean(train_mean_scores):.4f}\",\n",
    "        f\"valid-beta4-score:{b_score:.4f}\",\n",
    "    )\n",
    "\n",
    "    grand_train_loss.append(np.mean(train_loss))\n",
    "    # grand_valid_loss.append(np.mean(valid_loss))\n",
    "    # grand_train_score.append(np.mean(train_mean_scores))\n",
    "    grand_valid_score.append(b_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lossとvalid_lossのプロット\n",
    "\n",
    "plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scoreとvalid_scoreのプロット\n",
    "plt.plot(grand_train_score, label=\"train_score\")\n",
    "plt.plot(grand_valid_score, label=\"valid_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
