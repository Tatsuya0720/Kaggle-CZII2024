{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# import torch\n",
    "\n",
    "# # ViT Tiny Patch16 224 モデルの作成\n",
    "# model = timm.create_model(\n",
    "#     \"vit_tiny_patch16_224\",  # モデル名\n",
    "#     pretrained=False,        # 必要なら True\n",
    "#     in_chans=3,              # 入力チャネル数\n",
    "#     features_only=True,      # 各段階の特徴マップを取得\n",
    "# )\n",
    "\n",
    "# # ダミー入力 (例: B=1, C=3, H=224, W=224)\n",
    "# x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# # 各段の出力を取得\n",
    "# features = model(x)\n",
    "\n",
    "# # 出力の形状を確認\n",
    "# for i, f in enumerate(features):\n",
    "#     print(f\"Stage {i}: {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from src.inference import inference, inference2pos, create_gt_df\n",
    "from metric import visualize_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:11<00:00, 103.86it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "  0%|          | 0/300 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    "    augmentation=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "# train_nshuffle_dataset = EziiDataset(\n",
    "#     exp_names=CFG.train_exp_names,\n",
    "#     base_dir=\"/home/tumeda/workspace/kaggle/CZII_2/20. kaggle/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.train_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    augmentation=False,\n",
    "    train=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    # ここでチャンネル次元を挿入\n",
    "    normalized_tomogram = normalized_tomogram.unsqueeze(2)  # => (4,16,1,64,64)\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"segmentation_map\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "features_only not implemented for Vision Transformer models.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet3D(encoder\u001b[38;5;241m=\u001b[39mencoder)  \u001b[38;5;66;03m# .to(\"cuda\") # 確認用なのでGPUに載せない\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load(\"./pretrained_model.pth\"))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load(\"./best_model.pth\"))\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/timm/models/_factory.py:114\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    122\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/timm/models/vision_transformer.py:1464\u001b[0m, in \u001b[0;36mvit_tiny_patch16_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT-Tiny (Vit-Ti/16)\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m192\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m-> 1464\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_tiny_patch16_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/timm/models/vision_transformer.py:1441\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_vision_transformer\u001b[39m(variant, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_only\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_only not implemented for Vision Transformer models.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflexi\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant:\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;66;03m# FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;66;03m# interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m         _filter_fn \u001b[38;5;241m=\u001b[39m partial(checkpoint_filter_fn, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: features_only not implemented for Vision Transformer models."
     ]
    }
   ],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder)  # .to(\"cuda\") # 確認用なのでGPUに載せない\n",
    "# model.load_state_dict(torch.load(\"./pretrained_model.pth\"))\n",
    "# model.load_state_dict(torch.load(\"./best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# input-test\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 例: バッチサイズ=2, 深さ=16, 1ch, H=W=64\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# x = torch.randn(2, 16, 1, 64, 64).cuda()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# out = model(x)   # ここで forward が走る\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(out.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:45\u001b[0m, in \u001b[0;36mUnet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(b\u001b[38;5;241m*\u001b[39md, c, h, w)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ========== 2D ViTエンコーダ ==========\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# feats=[stage0, stage1, stage2]\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[43mencode2d_vit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimm_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 3段に対応\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ========== 3D デコーダ (3段) ==========\u001b[39;00m\n\u001b[1;32m     53\u001b[0m last, decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     54\u001b[0m     feature\u001b[38;5;241m=\u001b[39mencode[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],               \u001b[38;5;66;03m# 最深部 stage2\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     skip\u001b[38;5;241m=\u001b[39mencode[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m],  \u001b[38;5;66;03m# [stage1, stage0, None]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     depth_scaling\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],          \u001b[38;5;66;03m# 3段\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     spatial_scaling\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],        \u001b[38;5;66;03m# シンプルに空間は2倍ずつ\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:87\u001b[0m, in \u001b[0;36mencode2d_vit\u001b[0;34m(timm_encoder, input2d, batch_size, depth_scaler)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feats):\n\u001b[1;32m     86\u001b[0m     ds \u001b[38;5;241m=\u001b[39m depth_scaler[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(depth_scaler) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 87\u001b[0m     l_pooled_f, _ \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     encoded\u001b[38;5;241m.\u001b[39mappend(l_pooled_f)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:100\u001b[0m, in \u001b[0;36maggregate_depth\u001b[0;34m(x, batch_size, depth_scaling)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate_depth\u001b[39m(x, batch_size, depth_scaling):\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    x: (batch*depth, channel, H, W)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    => (b,d,c,H,W) => 3Dpool => ...\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     bd, c, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    101\u001b[0m     d \u001b[38;5;241m=\u001b[39m bd \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m    102\u001b[0m     b \u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "# input-test\n",
    "\n",
    "x = torch.randn(2, 16, 1, 64, 64).cuda()\n",
    "model(x).shape\n",
    "# 例: バッチサイズ=2, 深さ=16, 1ch, H=W=64\n",
    "# x = torch.randn(2, 16, 1, 64, 64).cuda()\n",
    "# out = model(x)   # ここで forward が走る\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"encoder\"と名のつくパラメータは学習しない\n",
    "# for layer, param in model.named_parameters():\n",
    "#     if \"encoder\" in layer:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# サンプルデータ\n",
    "num_classes = len(CFG.particles_name)  # クラス数\n",
    "colors = plt.cm.tab10(\n",
    "    np.arange(len(CFG.particles_name))\n",
    ")  # \"tab10\" カラーマップから色を取得\n",
    "\n",
    "# ListedColormap を作成\n",
    "class_colormap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "# カラーバー付きプロット\n",
    "def plot_with_colormap(data, title, original_tomogram):\n",
    "    masked_data = np.ma.masked_where(data <= 0, data)  # クラス0をマスク\n",
    "    plt.imshow(original_tomogram, cmap=\"gray\")\n",
    "    im = plt.imshow(masked_data, cmap=class_colormap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([6, 16, 320, 320])\n",
      "Augmented shape: torch.Size([6, 16, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# 回転\n",
    "# 3Dテンソルの各軸に対して指定した角度で回転する関数\n",
    "def rotate_3d(tomogram, segmentation_map, angle):\n",
    "    \"\"\"Rotates the 3D tensors tomogram and segmentation_map around the Z-axis.\"\"\"\n",
    "    rotated_tomogram = TF.rotate(tomogram, angle, expand=False)\n",
    "    rotated_segmentation_map = TF.rotate(segmentation_map, angle, expand=False)\n",
    "    return rotated_tomogram, rotated_segmentation_map\n",
    "\n",
    "\n",
    "# 平行移動\n",
    "# 指定された範囲でランダムに平行移動\n",
    "def translate_3d(tomogram, segmentation_map, max_shift):\n",
    "    \"\"\"Translates the 3D tensors by a random shift within max_shift.\"\"\"\n",
    "    shift_x = random.randint(-max_shift, max_shift)\n",
    "    shift_y = random.randint(-max_shift, max_shift)\n",
    "    translated_tomogram = TF.affine(\n",
    "        tomogram, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    translated_segmentation_map = TF.affine(\n",
    "        segmentation_map, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    return translated_tomogram, translated_segmentation_map\n",
    "\n",
    "\n",
    "# フリップ\n",
    "# 縦横（上下左右）ランダムフリップ\n",
    "def flip_3d(tomogram, segmentation_map):\n",
    "    \"\"\"Randomly flips the 3D tensors along height or width.\"\"\"\n",
    "    if random.random() > 0.5:  # Horizontal flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-1])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-1])\n",
    "    if random.random() > 0.5:  # Vertical flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-2])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-2])\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# クロッピング\n",
    "# 入力テンソルを中心またはランダムクロップで切り取る\n",
    "def crop_3d(tomogram, segmentation_map, crop_size):\n",
    "    \"\"\"Crops the 3D tensors to the specified crop_size.\"\"\"\n",
    "    _, depth, height, width = tomogram.size()\n",
    "    crop_d, crop_h, crop_w = crop_size\n",
    "\n",
    "    if crop_h > height or crop_w > width:\n",
    "        raise ValueError(\"Crop size cannot be larger than the original size.\")\n",
    "\n",
    "    start_h = random.randint(0, height - crop_h)  # Random starting position for height\n",
    "    start_w = random.randint(0, width - crop_w)  # Random starting position for width\n",
    "\n",
    "    cropped_tomogram = tomogram[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "    cropped_segmentation_map = segmentation_map[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "\n",
    "    return cropped_tomogram, cropped_segmentation_map\n",
    "\n",
    "\n",
    "# Mixup\n",
    "# 2つのサンプルを線形補間して混合\n",
    "def mixup(tomogram, segmentation_map, alpha=0.4):\n",
    "    \"\"\"Applies mixup augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size = tomogram.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_tomogram = lam * tomogram + (1 - lam) * tomogram[index, :]\n",
    "    mixed_segmentation_map = (\n",
    "        lam * segmentation_map + (1 - lam) * segmentation_map[index, :]\n",
    "    )\n",
    "\n",
    "    return mixed_tomogram, mixed_segmentation_map\n",
    "\n",
    "\n",
    "# Cutmix\n",
    "# ランダム領域を切り取って別のサンプルに貼り付け\n",
    "def cutmix(tomogram, segmentation_map, alpha=1.0):\n",
    "    \"\"\"Applies cutmix augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size, depth, height, width = tomogram.size()\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    cx = random.randint(0, width)\n",
    "    cy = random.randint(0, height)\n",
    "    cw = int(width * (1 - lam))\n",
    "    ch = int(height * (1 - lam))\n",
    "\n",
    "    x1 = max(cx - cw // 2, 0)\n",
    "    x2 = min(cx + cw // 2, width)\n",
    "    y1 = max(cy - ch // 2, 0)\n",
    "    y2 = min(cy + ch // 2, height)\n",
    "\n",
    "    tomogram[:, :, y1:y2, x1:x2] = tomogram[index, :, y1:y2, x1:x2]\n",
    "    segmentation_map[:, :, y1:y2, x1:x2] = segmentation_map[index, :, y1:y2, x1:x2]\n",
    "\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# データ拡張の組み合わせ適用\n",
    "def augment_data(\n",
    "    tomogram,\n",
    "    segmentation_map,\n",
    "    crop_size=(16, 12, 12),\n",
    "    max_shift=10,\n",
    "    rotation_angle=30,\n",
    "    p=0.5,\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "):\n",
    "    \"\"\"Applies a combination of rotation, translation, flipping, cropping, mixup, and cutmix to the inputs with probabilities.\"\"\"\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = rotate_3d(\n",
    "            tomogram,\n",
    "            segmentation_map,\n",
    "            angle=random.uniform(-rotation_angle, rotation_angle),\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = translate_3d(\n",
    "            tomogram, segmentation_map, max_shift=max_shift\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = flip_3d(tomogram, segmentation_map)\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = crop_3d(\n",
    "            tomogram, segmentation_map, crop_size=crop_size\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = mixup(\n",
    "            tomogram, segmentation_map, alpha=mixup_alpha\n",
    "        )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = cutmix(\n",
    "    #         tomogram, segmentation_map, alpha=cutmix_alpha\n",
    "    #     )\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# バッチサイズ6, 深さ16, 高さ320, 幅320のランダムテンソル\n",
    "tomogram = torch.rand((6, 16, 320, 320))\n",
    "segmentation_map = torch.randint(0, 2, (6, 16, 320, 320))  # ラベルは0または1\n",
    "\n",
    "# データ拡張の適用\n",
    "aug_tomogram, aug_segmentation_map = augment_data(tomogram, segmentation_map, p=0.7)\n",
    "print(\"Original shape:\", tomogram.shape)\n",
    "print(\"Augmented shape:\", aug_tomogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    #  weight=torch.tensor([2.0, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    ")\n",
    "# criterion = DiceLoss()\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=CFG.epochs * len(train_loader),\n",
    "    # * batch_size,\n",
    ")\n",
    "scaler = GradScaler()\n",
    "seg_loss = SegmentationLoss(criterion)\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padf = PadToSize(CFG.resolution)\n",
    "padf(normalized_tomogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Training]:   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 32\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_tomogram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m seg_loss(pred, segmentation_map)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:45\u001b[0m, in \u001b[0;36mUnet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(b\u001b[38;5;241m*\u001b[39md, c, h, w)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ========== 2D ViTエンコーダ ==========\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# feats=[stage0, stage1, stage2]\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[43mencode2d_vit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimm_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 3段に対応\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ========== 3D デコーダ (3段) ==========\u001b[39;00m\n\u001b[1;32m     53\u001b[0m last, decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     54\u001b[0m     feature\u001b[38;5;241m=\u001b[39mencode[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],               \u001b[38;5;66;03m# 最深部 stage2\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     skip\u001b[38;5;241m=\u001b[39mencode[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m],  \u001b[38;5;66;03m# [stage1, stage0, None]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     depth_scaling\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],          \u001b[38;5;66;03m# 3段\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     spatial_scaling\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],        \u001b[38;5;66;03m# シンプルに空間は2倍ずつ\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:87\u001b[0m, in \u001b[0;36mencode2d_vit\u001b[0;34m(timm_encoder, input2d, batch_size, depth_scaler)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feats):\n\u001b[1;32m     86\u001b[0m     ds \u001b[38;5;241m=\u001b[39m depth_scaler[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(depth_scaler) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 87\u001b[0m     l_pooled_f, _ \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     encoded\u001b[38;5;241m.\u001b[39mappend(l_pooled_f)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp071-3DUnet-ViT-004/src/network.py:100\u001b[0m, in \u001b[0;36maggregate_depth\u001b[0;34m(x, batch_size, depth_scaling)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate_depth\u001b[39m(x, batch_size, depth_scaling):\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    x: (batch*depth, channel, H, W)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    => (b,d,c,H,W) => 3Dpool => ...\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     bd, c, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    101\u001b[0m     d \u001b[38;5;241m=\u001b[39m bd \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m    102\u001b[0m     b \u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_constant = 0\n",
    "best_score = -100\n",
    "\n",
    "grand_train_loss = []\n",
    "grand_valid_loss = []\n",
    "grand_train_score = []\n",
    "grand_valid_score = []\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\") as tq:\n",
    "        for data in tq:\n",
    "            normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "            segmentation_map = data[\"segmentation_map\"]\n",
    "\n",
    "            normalized_tomogram = padf(normalized_tomogram)\n",
    "            segmentation_map = padf(segmentation_map)\n",
    "\n",
    "            # データ拡張\n",
    "            # normalized_tomogram, segmentation_map = augment_data(\n",
    "            #     normalized_tomogram, segmentation_map, p=CFG.augmentation_prob\n",
    "            # )\n",
    "            normalized_tomogram = normalized_tomogram.cuda()\n",
    "            segmentation_map = segmentation_map.long().cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(preprocess_tensor(normalized_tomogram))\n",
    "                loss = seg_loss(pred, segmentation_map)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # 確率予測\n",
    "            prob_pred = torch.softmax(pred, dim=1)\n",
    "            tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "    with tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\") as tq:\n",
    "        with torch.no_grad():\n",
    "            for data in tq:\n",
    "                normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "                segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "\n",
    "                normalized_tomogram = padf(normalized_tomogram)\n",
    "                segmentation_map = padf(segmentation_map)\n",
    "\n",
    "                with autocast():\n",
    "                    pred = model(preprocess_tensor(normalized_tomogram))\n",
    "                    loss = seg_loss(pred, segmentation_map)\n",
    "                valid_loss.append(loss.item())\n",
    "\n",
    "                # 確率予測\n",
    "                prob_pred = torch.softmax(pred, dim=1)\n",
    "                tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "    # # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    # モデルの保存\n",
    "    torch.save(model.state_dict(), \"./best_model.pth\")\n",
    "\n",
    "    # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    train_inferenced_array = {}\n",
    "    train_pred_array = []\n",
    "    train_gt_array = []\n",
    "    valid_inferenced_array = {}\n",
    "    valid_gt_array = []\n",
    "\n",
    "    # for exp_name in tqdm(CFG.train_exp_names):\n",
    "    for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "        # inferenced_array = inference(model, exp_name, train=False)\n",
    "        inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "            model, exp_name, train=False\n",
    "        )\n",
    "        valid_inferenced_array[exp_name] = inferenced_array\n",
    "        base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "        gt_df = create_gt_df(base_dir, [exp_name])\n",
    "        valid_gt_array.append(gt_df)\n",
    "\n",
    "    valid_gt_array = pd.concat(valid_gt_array)\n",
    "\n",
    "    b_constant = 0\n",
    "    b_score = -100\n",
    "    for constant in tqdm(np.linspace(0.1, 0.9, 15)):\n",
    "        valid_pred_array = []\n",
    "        sikii = {\n",
    "            \"apo-ferritin\": constant,\n",
    "            \"beta-amylase\": constant,\n",
    "            \"beta-galactosidase\": constant,\n",
    "            \"ribosome\": constant,\n",
    "            \"thyroglobulin\": constant,\n",
    "            \"virus-like-particle\": constant,\n",
    "        }\n",
    "        for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "            pred_df = inference2pos(\n",
    "                pred_segmask=valid_inferenced_array[exp_name],\n",
    "                exp_name=exp_name,\n",
    "                sikii_dict=sikii,\n",
    "            )\n",
    "            valid_pred_array.append(pred_df)\n",
    "\n",
    "        valid_pred_array = pd.concat(valid_pred_array)\n",
    "\n",
    "        if len(valid_pred_array) != 0:\n",
    "            score_ = score(\n",
    "                valid_pred_array,\n",
    "                valid_gt_array,\n",
    "                row_id_column_name=\"index\",\n",
    "                distance_multiplier=1.0,\n",
    "                beta=4,\n",
    "            )\n",
    "            if score_ > b_score:\n",
    "                b_score = score_\n",
    "                b_constant = constant\n",
    "\n",
    "        import gc\n",
    "        import torch.cuda as cuda\n",
    "\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "\n",
    "    print(\"constant\", b_constant, \"score\", b_score)\n",
    "\n",
    "    if b_score > best_score:\n",
    "        best_constant = b_constant\n",
    "        best_score = b_score\n",
    "        # best_score = np.mean(valid_mean_scores)\n",
    "        best_model = model.state_dict()\n",
    "        torch.save(best_model, f\"./best_model.pth\")\n",
    "\n",
    "    print(\n",
    "        f\"train-epoch-loss:{np.mean(train_loss):.4f}\",\n",
    "        f\"valid-epoch-loss:{np.mean(valid_loss):.4f}\",\n",
    "        # f\"train-beta4-score:{np.mean(train_mean_scores):.4f}\",\n",
    "        f\"valid-beta4-score:{b_score:.4f}\",\n",
    "    )\n",
    "\n",
    "    grand_train_loss.append(np.mean(train_loss))\n",
    "    grand_valid_loss.append(np.mean(valid_loss))\n",
    "    # grand_train_score.append(np.mean(train_mean_scores))\n",
    "    grand_valid_score.append(b_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.25it/s, loss=0.2251]\n",
      "Epoch 1/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s, loss=0.1691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.1691\n",
      "Epoch 1/80 Summary:\n",
      "Train Loss: 0.2251, Valid Loss: 0.1691, Best Validation Loss: 0.1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.37it/s, loss=0.0615]\n",
      "Epoch 2/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s, loss=0.0508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0508\n",
      "Epoch 2/80 Summary:\n",
      "Train Loss: 0.0615, Valid Loss: 0.0508, Best Validation Loss: 0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.40it/s, loss=0.0440]\n",
      "Epoch 3/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, loss=0.0742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 Summary:\n",
      "Train Loss: 0.0440, Valid Loss: 0.0742, Best Validation Loss: 0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.41it/s, loss=0.0352]\n",
      "Epoch 4/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s, loss=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0388\n",
      "Epoch 4/80 Summary:\n",
      "Train Loss: 0.0352, Valid Loss: 0.0388, Best Validation Loss: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.39it/s, loss=0.0297]\n",
      "Epoch 5/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0122\n",
      "Epoch 5/80 Summary:\n",
      "Train Loss: 0.0297, Valid Loss: 0.0122, Best Validation Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80 [Training]: 100%|██████████| 300/300 [00:27<00:00, 10.72it/s, loss=0.0247]\n",
      "Epoch 6/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s, loss=0.0103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0103\n",
      "Epoch 6/80 Summary:\n",
      "Train Loss: 0.0247, Valid Loss: 0.0103, Best Validation Loss: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.53it/s, loss=0.0222]\n",
      "Epoch 7/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 Summary:\n",
      "Train Loss: 0.0222, Valid Loss: 0.0138, Best Validation Loss: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.55it/s, loss=0.0175]\n",
      "Epoch 8/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s, loss=0.0042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0042\n",
      "Epoch 8/80 Summary:\n",
      "Train Loss: 0.0175, Valid Loss: 0.0042, Best Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.67it/s, loss=0.0174]\n",
      "Epoch 9/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, loss=0.0027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0027\n",
      "Epoch 9/80 Summary:\n",
      "Train Loss: 0.0174, Valid Loss: 0.0027, Best Validation Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.62it/s, loss=0.0170]\n",
      "Epoch 10/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s, loss=0.0068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 Summary:\n",
      "Train Loss: 0.0170, Valid Loss: 0.0068, Best Validation Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.58it/s, loss=0.0143]\n",
      "Epoch 11/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.28it/s, loss=0.0012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0012\n",
      "Epoch 11/80 Summary:\n",
      "Train Loss: 0.0143, Valid Loss: 0.0012, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.41it/s, loss=0.0146]\n",
      "Epoch 12/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 Summary:\n",
      "Train Loss: 0.0146, Valid Loss: 0.0022, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.49it/s, loss=0.0129]\n",
      "Epoch 13/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 Summary:\n",
      "Train Loss: 0.0129, Valid Loss: 0.0043, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.47it/s, loss=0.0146]\n",
      "Epoch 14/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/80 Summary:\n",
      "Train Loss: 0.0146, Valid Loss: 0.0013, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.66it/s, loss=0.0153]\n",
      "Epoch 15/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.59it/s, loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/80 Summary:\n",
      "Train Loss: 0.0153, Valid Loss: 0.0022, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.52it/s, loss=0.0124]\n",
      "Epoch 16/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, loss=0.0016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/80 Summary:\n",
      "Train Loss: 0.0124, Valid Loss: 0.0016, Best Validation Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.55it/s, loss=0.0118]\n",
      "Epoch 17/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0003\n",
      "Epoch 17/80 Summary:\n",
      "Train Loss: 0.0118, Valid Loss: 0.0003, Best Validation Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.69it/s, loss=0.0106]\n",
      "Epoch 18/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.47it/s, loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/80 Summary:\n",
      "Train Loss: 0.0106, Valid Loss: 0.0010, Best Validation Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.67it/s, loss=0.0099]\n",
      "Epoch 19/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0002\n",
      "Epoch 19/80 Summary:\n",
      "Train Loss: 0.0099, Valid Loss: 0.0002, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.64it/s, loss=0.0103]\n",
      "Epoch 20/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s, loss=0.0012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/80 Summary:\n",
      "Train Loss: 0.0103, Valid Loss: 0.0012, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.46it/s, loss=0.0098]\n",
      "Epoch 21/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/80 Summary:\n",
      "Train Loss: 0.0098, Valid Loss: 0.0003, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.69it/s, loss=0.0104]\n",
      "Epoch 22/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/80 Summary:\n",
      "Train Loss: 0.0104, Valid Loss: 0.0011, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.65it/s, loss=0.0094] \n",
      "Epoch 23/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80 Summary:\n",
      "Train Loss: 0.0094, Valid Loss: 0.0015, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.65it/s, loss=0.0096]\n",
      "Epoch 24/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.59it/s, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/80 Summary:\n",
      "Train Loss: 0.0096, Valid Loss: 0.0011, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.42it/s, loss=0.0092]\n",
      "Epoch 25/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/80 Summary:\n",
      "Train Loss: 0.0092, Valid Loss: 0.0007, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.40it/s, loss=0.0073]\n",
      "Epoch 26/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/80 Summary:\n",
      "Train Loss: 0.0073, Valid Loss: 0.0003, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.44it/s, loss=0.0088]\n",
      "Epoch 27/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s, loss=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/80 Summary:\n",
      "Train Loss: 0.0088, Valid Loss: 0.0005, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.37it/s, loss=0.0065]\n",
      "Epoch 28/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/80 Summary:\n",
      "Train Loss: 0.0065, Valid Loss: 0.0003, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.58it/s, loss=0.0065]\n",
      "Epoch 29/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/80 Summary:\n",
      "Train Loss: 0.0065, Valid Loss: 0.0004, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.45it/s, loss=0.0085]\n",
      "Epoch 30/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/80 Summary:\n",
      "Train Loss: 0.0085, Valid Loss: 0.0002, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/80 [Training]: 100%|██████████| 300/300 [00:27<00:00, 10.74it/s, loss=0.0052]\n",
      "Epoch 31/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/80 Summary:\n",
      "Train Loss: 0.0052, Valid Loss: 0.0004, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.64it/s, loss=0.0048]\n",
      "Epoch 32/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/80 Summary:\n",
      "Train Loss: 0.0048, Valid Loss: 0.0002, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.64it/s, loss=0.0069]\n",
      "Epoch 33/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/80 Summary:\n",
      "Train Loss: 0.0069, Valid Loss: 0.0002, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.50it/s, loss=0.0044]\n",
      "Epoch 34/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/80 Summary:\n",
      "Train Loss: 0.0044, Valid Loss: 0.0002, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.55it/s, loss=0.0033]\n",
      "Epoch 35/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/80 Summary:\n",
      "Train Loss: 0.0033, Valid Loss: 0.0003, Best Validation Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.61it/s, loss=0.0040]\n",
      "Epoch 36/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0001\n",
      "Epoch 36/80 Summary:\n",
      "Train Loss: 0.0040, Valid Loss: 0.0001, Best Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.61it/s, loss=0.0058]\n",
      "Epoch 37/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/80 Summary:\n",
      "Train Loss: 0.0058, Valid Loss: 0.0001, Best Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.63it/s, loss=0.0031]\n",
      "Epoch 38/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/80 Summary:\n",
      "Train Loss: 0.0031, Valid Loss: 0.0001, Best Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.37it/s, loss=0.0025]\n",
      "Epoch 39/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s, loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/80 Summary:\n",
      "Train Loss: 0.0025, Valid Loss: 0.0022, Best Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.47it/s, loss=0.0037]\n",
      "Epoch 40/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/80 Summary:\n",
      "Train Loss: 0.0037, Valid Loss: 0.0001, Best Validation Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.45it/s, loss=0.0022]\n",
      "Epoch 41/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 41/80 Summary:\n",
      "Train Loss: 0.0022, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.57it/s, loss=0.0024]\n",
      "Epoch 42/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/80 Summary:\n",
      "Train Loss: 0.0024, Valid Loss: 0.0001, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.51it/s, loss=0.0056]\n",
      "Epoch 43/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 43/80 Summary:\n",
      "Train Loss: 0.0056, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.52it/s, loss=0.0023]\n",
      "Epoch 44/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/80 Summary:\n",
      "Train Loss: 0.0023, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.64it/s, loss=0.0017]\n",
      "Epoch 45/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80 Summary:\n",
      "Train Loss: 0.0017, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.61it/s, loss=0.0016]\n",
      "Epoch 46/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/80 Summary:\n",
      "Train Loss: 0.0016, Valid Loss: 0.0001, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/80 [Training]: 100%|██████████| 300/300 [00:27<00:00, 10.73it/s, loss=0.0020]\n",
      "Epoch 47/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/80 Summary:\n",
      "Train Loss: 0.0020, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.46it/s, loss=0.0014]\n",
      "Epoch 48/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/80 Summary:\n",
      "Train Loss: 0.0014, Valid Loss: 0.0001, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.57it/s, loss=0.0023]\n",
      "Epoch 49/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/80 Summary:\n",
      "Train Loss: 0.0023, Valid Loss: 0.0001, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.62it/s, loss=0.0009]\n",
      "Epoch 50/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 50/80 Summary:\n",
      "Train Loss: 0.0009, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.64it/s, loss=0.0011]\n",
      "Epoch 51/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.27it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/80 Summary:\n",
      "Train Loss: 0.0011, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.67it/s, loss=0.0012]\n",
      "Epoch 52/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 52/80 Summary:\n",
      "Train Loss: 0.0012, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.43it/s, loss=0.0009]\n",
      "Epoch 53/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/80 Summary:\n",
      "Train Loss: 0.0009, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.44it/s, loss=0.0007]\n",
      "Epoch 54/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.16it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80 Summary:\n",
      "Train Loss: 0.0007, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.47it/s, loss=0.0011]\n",
      "Epoch 55/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/80 Summary:\n",
      "Train Loss: 0.0011, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.65it/s, loss=0.0009]\n",
      "Epoch 56/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/80 Summary:\n",
      "Train Loss: 0.0009, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.48it/s, loss=0.0007]\n",
      "Epoch 57/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/80 Summary:\n",
      "Train Loss: 0.0007, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.54it/s, loss=0.0009]\n",
      "Epoch 58/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/80 Summary:\n",
      "Train Loss: 0.0009, Valid Loss: 0.0001, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.66it/s, loss=0.0021]\n",
      "Epoch 59/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/80 Summary:\n",
      "Train Loss: 0.0021, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.62it/s, loss=0.0004]\n",
      "Epoch 60/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/80 Summary:\n",
      "Train Loss: 0.0004, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.59it/s, loss=0.0003]\n",
      "Epoch 61/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 61/80 Summary:\n",
      "Train Loss: 0.0003, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.42it/s, loss=0.0004]\n",
      "Epoch 62/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/80 Summary:\n",
      "Train Loss: 0.0004, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.57it/s, loss=0.0004]\n",
      "Epoch 63/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 63/80 Summary:\n",
      "Train Loss: 0.0004, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.70it/s, loss=0.0002]\n",
      "Epoch 64/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 64/80 Summary:\n",
      "Train Loss: 0.0002, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.60it/s, loss=0.0002]\n",
      "Epoch 65/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/80 Summary:\n",
      "Train Loss: 0.0002, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.39it/s, loss=0.0002]\n",
      "Epoch 66/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/80 Summary:\n",
      "Train Loss: 0.0002, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.41it/s, loss=0.0002]\n",
      "Epoch 67/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with validation loss: 0.0000\n",
      "Epoch 67/80 Summary:\n",
      "Train Loss: 0.0002, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.40it/s, loss=0.0002]\n",
      "Epoch 68/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/80 Summary:\n",
      "Train Loss: 0.0002, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.42it/s, loss=0.0001]\n",
      "Epoch 69/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.47it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.58it/s, loss=0.0001]\n",
      "Epoch 70/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.30it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.46it/s, loss=0.0001]\n",
      "Epoch 71/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.69it/s, loss=0.0001]\n",
      "Epoch 72/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.60it/s, loss=0.0001]\n",
      "Epoch 73/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.56it/s, loss=0.0001]\n",
      "Epoch 74/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.52it/s, loss=0.0001]\n",
      "Epoch 75/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.56it/s, loss=0.0001]\n",
      "Epoch 76/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.63it/s, loss=0.0001]\n",
      "Epoch 77/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/80 [Training]: 100%|██████████| 300/300 [00:25<00:00, 11.61it/s, loss=0.0001]\n",
      "Epoch 78/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.65it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/80 [Training]: 100%|██████████| 300/300 [00:26<00:00, 11.51it/s, loss=0.0001]\n",
      "Epoch 79/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/80 [Training]: 100%|██████████| 300/300 [00:28<00:00, 10.48it/s, loss=0.0001]\n",
      "Epoch 80/80 [Validation]: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/80 Summary:\n",
      "Train Loss: 0.0001, Valid Loss: 0.0000, Best Validation Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZGpJREFUeJzt3Xd4FNX+BvB3tqYX0gNpkNBD6BiqSpAmCqgXuCgEr3JRUfghXlGkqyigoqhgx0JREBAV0RABpffeJRAgpEBIJ9lk9/z+mOwmSxJIYyfl/TzPPsnOnJ09k43k9ZzvnJGEEAJERERE9YhK6Q4QERER2RoDEBEREdU7DEBERERU7zAAERERUb3DAERERET1DgMQERER1TsMQERERFTvMAARERFRvcMARERERPUOAxBRDRMdHY3g4OBKvXbmzJmQJKl6O1TDXLhwAZIkYenSpTZ/b0mSMHPmTMvzpUuXQpIkXLhw4Y6vDQ4ORnR0dLX2pyq/K0T1HQMQUTlJklSux5YtW5Tuar33wgsvQJIknDt3rsw2U6dOhSRJOHLkiA17VnEJCQmYOXMmDh06pHRXLMwhdMGCBUp3hajSNEp3gKi2+Pbbb62ef/PNN4iJiSmxvUWLFlV6n88++wwmk6lSr33ttdcwZcqUKr1/XTBy5EgsWrQIy5cvx/Tp00tts2LFCoSHh6NNmzaVfp8nnngCw4cPh16vr/Qx7iQhIQGzZs1CcHAw2rZta7WvKr8rRPUdAxBROT3++ONWz3ft2oWYmJgS22+Vk5MDBweHcr+PVqutVP8AQKPRQKPhf9ZdunRBaGgoVqxYUWoA2rlzJ+Li4vDWW29V6X3UajXUanWVjlEVVfldIarvOAVGVI3uvfdetG7dGvv370fPnj3h4OCAV199FQDw008/YeDAgfD394der0eTJk0wZ84cGI1Gq2PcWtdRfLrh008/RZMmTaDX69GpUyfs3bvX6rWl1QBJkoTx48dj3bp1aN26NfR6PVq1aoWNGzeW6P+WLVvQsWNH2NnZoUmTJvjkk0/KXVf0999/47HHHkNgYCD0ej0CAgLwf//3f7h582aJ83NycsKVK1cwePBgODk5wcvLC5MnTy7xs0hLS0N0dDRcXV3h5uaG0aNHIy0t7Y59AeRRoFOnTuHAgQMl9i1fvhySJGHEiBEwGAyYPn06OnToAFdXVzg6OqJHjx7YvHnzHd+jtBogIQRef/11NGrUCA4ODrjvvvtw/PjxEq9NTU3F5MmTER4eDicnJ7i4uKB///44fPiwpc2WLVvQqVMnAMCYMWMs06zm+qfSaoCys7Px4osvIiAgAHq9Hs2aNcOCBQsghLBqV5Hfi8pKTk7Gf/7zH/j4+MDOzg4RERH4+uuvS7RbuXIlOnToAGdnZ7i4uCA8PBzvv/++ZX9+fj5mzZqFsLAw2NnZwcPDA927d0dMTEy19ZXqH/6vIlE1u379Ovr374/hw4fj8ccfh4+PDwD5j6WTkxMmTZoEJycn/Pnnn5g+fToyMjIwf/78Ox53+fLlyMzMxH//+19IkoR58+Zh6NChOH/+/B1HArZt24Y1a9bg2WefhbOzMz744AM88sgjiI+Ph4eHBwDg4MGD6NevH/z8/DBr1iwYjUbMnj0bXl5e5TrvVatWIScnB8888ww8PDywZ88eLFq0CJcvX8aqVaus2hqNRvTt2xddunTBggULsGnTJrzzzjto0qQJnnnmGQBykHj44Yexbds2jBs3Di1atMDatWsxevTocvVn5MiRmDVrFpYvX4727dtbvfcPP/yAHj16IDAwENeuXcPnn3+OESNG4Omnn0ZmZia++OIL9O3bF3v27Ckx7XQn06dPx+uvv44BAwZgwIABOHDgAB544AEYDAardufPn8e6devw2GOPISQkBElJSfjkk0/Qq1cvnDhxAv7+/mjRogVmz56N6dOnY+zYsejRowcAoGvXrqW+txACDz30EDZv3oz//Oc/aNu2LX7//Xe89NJLuHLlCt577z2r9uX5vaismzdv4t5778W5c+cwfvx4hISEYNWqVYiOjkZaWhomTJgAAIiJicGIESPQu3dvvP322wCAkydPYvv27ZY2M2fOxNy5c/HUU0+hc+fOyMjIwL59+3DgwAH06dOnSv2kekwQUaU899xz4tb/hHr16iUAiCVLlpRon5OTU2Lbf//7X+Hg4CByc3Mt20aPHi2CgoIsz+Pi4gQA4eHhIVJTUy3bf/rpJwFA/Pzzz5ZtM2bMKNEnAEKn04lz585Zth0+fFgAEIsWLbJsGzRokHBwcBBXrlyxbDt79qzQaDQljlma0s5v7ty5QpIkcfHiRavzAyBmz55t1bZdu3aiQ4cOlufr1q0TAMS8efMs2woKCkSPHj0EAPHVV1/dsU+dOnUSjRo1Ekaj0bJt48aNAoD45JNPLMfMy8uzet2NGzeEj4+PePLJJ622AxAzZsywPP/qq68EABEXFyeEECI5OVnodDoxcOBAYTKZLO1effVVAUCMHj3asi03N9eqX0LIn7Ver7f62ezdu7fM8731d8X8M3v99det2j366KNCkiSr34Hy/l6Uxvw7OX/+/DLbLFy4UAAQ3333nWWbwWAQkZGRwsnJSWRkZAghhJgwYYJwcXERBQUFZR4rIiJCDBw48LZ9IqooToERVTO9Xo8xY8aU2G5vb2/5PjMzE9euXUOPHj2Qk5ODU6dO3fG4w4YNg7u7u+W5eTTg/Pnzd3xtVFQUmjRpYnnepk0buLi4WF5rNBqxadMmDB48GP7+/pZ2oaGh6N+//x2PD1ifX3Z2Nq5du4auXbtCCIGDBw+WaD9u3Dir5z169LA6lw0bNkCj0VhGhAC55ub5558vV38AuW7r8uXL+Ouvvyzbli9fDp1Oh8cee8xyTJ1OBwAwmUxITU1FQUEBOnbsWOr02e1s2rQJBoMBzz//vNW04cSJE0u01ev1UKnkf4KNRiOuX78OJycnNGvWrMLva7Zhwwao1Wq88MILVttffPFFCCHw22+/WW2/0+9FVWzYsAG+vr4YMWKEZZtWq8ULL7yArKwsbN26FQDg5uaG7Ozs205nubm54fjx4zh79myV+0VkxgBEVM0aNmxo+YNa3PHjxzFkyBC4urrCxcUFXl5elgLq9PT0Ox43MDDQ6rk5DN24caPCrzW/3vza5ORk3Lx5E6GhoSXalbatNPHx8YiOjkaDBg0sdT29evUCUPL87OzsSkytFe8PAFy8eBF+fn5wcnKyatesWbNy9QcAhg8fDrVajeXLlwMAcnNzsXbtWvTv398qTH799ddo06aNpb7Ey8sLv/76a7k+l+IuXrwIAAgLC7Pa7uXlZfV+gBy23nvvPYSFhUGv18PT0xNeXl44cuRIhd+3+Pv7+/vD2dnZarv5ykRz/8zu9HtRFRcvXkRYWJgl5JXVl2effRZNmzZF//790ahRIzz55JMl6pBmz56NtLQ0NG3aFOHh4XjppZdq/PIFVPMxABFVs+IjIWZpaWno1asXDh8+jNmzZ+Pnn39GTEyMpeahPJcyl3W1kbiluLW6X1seRqMRffr0wa+//oqXX34Z69atQ0xMjKVY99bzs9WVU97e3ujTpw9+/PFH5Ofn4+eff0ZmZiZGjhxpafPdd98hOjoaTZo0wRdffIGNGzciJiYG999//129xPzNN9/EpEmT0LNnT3z33Xf4/fffERMTg1atWtns0va7/XtRHt7e3jh06BDWr19vqV/q37+/Va1Xz5498c8//+DLL79E69at8fnnn6N9+/b4/PPPbdZPqntYBE1kA1u2bMH169exZs0a9OzZ07I9Li5OwV4V8fb2hp2dXakLB95uMUGzo0eP4syZM/j6668xatQoy/aqXKUTFBSE2NhYZGVlWY0CnT59ukLHGTlyJDZu3IjffvsNy5cvh4uLCwYNGmTZv3r1ajRu3Bhr1qyxmraaMWNGpfoMAGfPnkXjxo0t21NSUkqMqqxevRr33XcfvvjiC6vtaWlp8PT0tDyvyMreQUFB2LRpEzIzM61GgcxTrOb+2UJQUBCOHDkCk8lkNQpUWl90Oh0GDRqEQYMGwWQy4dlnn8Unn3yCadOmWUYgGzRogDFjxmDMmDHIyspCz549MXPmTDz11FM2OyeqWzgCRGQD5v/TLv5/1gaDAR9//LFSXbKiVqsRFRWFdevWISEhwbL93LlzJepGyno9YH1+QgirS5krasCAASgoKMDixYst24xGIxYtWlSh4wwePBgODg74+OOP8dtvv2Ho0KGws7O7bd93796NnTt3VrjPUVFR0Gq1WLRokdXxFi5cWKKtWq0uMdKyatUqXLlyxWqbo6MjAJTr8v8BAwbAaDTiww8/tNr+3nvvQZKkctdzVYcBAwYgMTER33//vWVbQUEBFi1aBCcnJ8v06PXr161ep1KpLItT5uXlldrGyckJoaGhlv1ElcERICIb6Nq1K9zd3TF69GjLbRq+/fZbm0413MnMmTPxxx9/oFu3bnjmmWcsf0hbt259x9swNG/eHE2aNMHkyZNx5coVuLi44Mcff6xSLcmgQYPQrVs3TJkyBRcuXEDLli2xZs2aCtfHODk5YfDgwZY6oOLTXwDw4IMPYs2aNRgyZAgGDhyIuLg4LFmyBC1btkRWVlaF3su8ntHcuXPx4IMPYsCAATh48CB+++03q1Ed8/vOnj0bY8aMQdeuXXH06FEsW7bMauQIAJo0aQI3NzcsWbIEzs7OcHR0RJcuXRASElLi/QcNGoT77rsPU6dOxYULFxAREYE//vgDP/30EyZOnGhV8FwdYmNjkZubW2L74MGDMXbsWHzyySeIjo7G/v37ERwcjNWrV2P79u1YuHChZYTqqaeeQmpqKu6//340atQIFy9exKJFi9C2bVtLvVDLli1x7733okOHDmjQoAH27duH1atXY/z48dV6PlTPKHPxGVHtV9Zl8K1atSq1/fbt28U999wj7O3thb+/v/jf//4nfv/9dwFAbN682dKurMvgS7vkGLdcll3WZfDPPfdcidcGBQVZXZYthBCxsbGiXbt2QqfTiSZNmojPP/9cvPjii8LOzq6Mn0KREydOiKioKOHk5CQ8PT3F008/bbmsuvgl3KNHjxaOjo4lXl9a369fvy6eeOIJ4eLiIlxdXcUTTzwhDh48WO7L4M1+/fVXAUD4+fmVuPTcZDKJN998UwQFBQm9Xi/atWsnfvnllxKfgxB3vgxeCCGMRqOYNWuW8PPzE/b29uLee+8Vx44dK/Hzzs3NFS+++KKlXbdu3cTOnTtFr169RK9evaze96effhItW7a0LElgPvfS+piZmSn+7//+T/j7+wutVivCwsLE/PnzrS7LN59LeX8vbmX+nSzr8e233wohhEhKShJjxowRnp6eQqfTifDw8BKf2+rVq8UDDzwgvL29hU6nE4GBgeK///2vuHr1qqXN66+/Ljp37izc3NyEvb29aN68uXjjjTeEwWC4bT+JbkcSogb9LygR1TiDBw/mJchEVOewBoiILG69bcXZs2exYcMG3Hvvvcp0iIjoLuEIEBFZ+Pn5ITo6Go0bN8bFixexePFi5OXl4eDBgyXWtiEiqs1YBE1EFv369cOKFSuQmJgIvV6PyMhIvPnmmww/RFTncASIiIiI6h3WABEREVG9wwBERERE9Q5rgEphMpmQkJAAZ2fnCi1DT0RERMoRQiAzMxP+/v4lbsR7KwagUiQkJCAgIEDpbhAREVElXLp0CY0aNbptGwagUpiXaL906RJcXFwU7g0RERGVR0ZGBgICAqxuBlwWBqBSmKe9XFxcGICIiIhqmfKUr7AImoiIiOodBiAiIiKqdxiAiIiIqN5hDRAREdULRqMR+fn5SneDqkCr1UKtVlfLsRiAiIioThNCIDExEWlpaUp3haqBm5sbfH19q7xOHwMQERHVaebw4+3tDQcHBy5wW0sJIZCTk4Pk5GQAgJ+fX5WOxwBERER1ltFotIQfDw8PpbtDVWRvbw8ASE5Ohre3d5Wmw1gETUREdZa55sfBwUHhnlB1MX+WVa3nYgAiIqI6j9NedUd1fZYMQERERFTvMAARERHVccHBwVi4cGG1HGvLli2QJKnWX1XHImgiIqIa6N5770Xbtm2rJbjs3bsXjo6OVe9UHcIAZENZeQVIyzHAXquGh5Ne6e4QEVEtJoSA0WiERnPnP+VeXl426FHtwikwG1q6PQ7d396MBX+cVrorRERUg0VHR2Pr1q14//33IUkSJEnC0qVLIUkSfvvtN3To0AF6vR7btm3DP//8g4cffhg+Pj5wcnJCp06dsGnTJqvj3ToFJkkSPv/8cwwZMgQODg4ICwvD+vXrK93fH3/8Ea1atYJer0dwcDDeeecdq/0ff/wxwsLCYGdnBx8fHzz66KOWfatXr0Z4eDjs7e3h4eGBqKgoZGdnV7ov5cURIBvSquW8aSgQCveEiKj+EkLgZr7R5u9rr1WX+wqm999/H2fOnEHr1q0xe/ZsAMDx48cBAFOmTMGCBQvQuHFjuLu749KlSxgwYADeeOMN6PV6fPPNNxg0aBBOnz6NwMDAMt9j1qxZmDdvHubPn49FixZh5MiRuHjxIho0aFCh89q/fz/+9a9/YebMmRg2bBh27NiBZ599Fh4eHoiOjsa+ffvwwgsv4Ntvv0XXrl2RmpqKv//+GwBw9epVjBgxAvPmzcOQIUOQmZmJv//+G0Lc/b+TDEA2ZA5A+UaTwj0hIqq/buYb0XL67zZ/3xOz+8JBV74/u66urtDpdHBwcICvry8A4NSpUwCA2bNno0+fPpa2DRo0QEREhOX5nDlzsHbtWqxfvx7jx48v8z2io6MxYsQIAMCbb76JDz74AHv27EG/fv0qdF7vvvsuevfujWnTpgEAmjZtihMnTmD+/PmIjo5GfHw8HB0d8eCDD8LZ2RlBQUFo164dADkAFRQUYOjQoQgKCgIAhIeHV+j9K4tTYDak1TAAERFR1XTs2NHqeVZWFiZPnowWLVrAzc0NTk5OOHnyJOLj4297nDZt2li+d3R0hIuLi+U2ExVx8uRJdOvWzWpbt27dcPbsWRiNRvTp0wdBQUFo3LgxnnjiCSxbtgw5OTkAgIiICPTu3Rvh4eF47LHH8Nlnn+HGjRsV7kNlcATIhnRqeeiTAYiISDn2WjVOzO6ryPtWh1uv5po8eTJiYmKwYMEChIaGwt7eHo8++igMBsNtj6PVaq2eS5IEk6n6/z45OzvjwIED2LJlC/744w9Mnz4dM2fOxN69e+Hm5oaYmBjs2LEDf/zxBxYtWoSpU6di9+7dCAkJqfa+FMcRIBuy1AAZWQNERKQUSZLgoNPY/FHRFYx1Oh2MxjvXKm3fvh3R0dEYMmQIwsPD4evriwsXLlTyp1NxLVq0wPbt20v0qWnTppZ7dWk0GkRFRWHevHk4cuQILly4gD///BOA/Hl069YNs2bNwsGDB6HT6bB27dq73m+OANmQpQaogCNARER0e8HBwdi9ezcuXLgAJyenMkdnwsLCsGbNGgwaNAiSJGHatGl3ZSSnLC+++CI6deqEOXPmYNiwYdi5cyc+/PBDfPzxxwCAX375BefPn0fPnj3h7u6ODRs2wGQyoVmzZti9ezdiY2PxwAMPwNvbG7t370ZKSgpatGhx1/vNESAbYhE0ERGV1+TJk6FWq9GyZUt4eXmVWdPz7rvvwt3dHV27dsWgQYPQt29ftG/f3mb9bN++PX744QesXLkSrVu3xvTp0zF79mxER0cDANzc3LBmzRrcf//9aNGiBZYsWYIVK1agVatWcHFxwV9//YUBAwagadOmeO211/DOO++gf//+d73fkrDFtWa1TEZGBlxdXZGeng4XF5dqO+6fp5Lw5NJ9iGjkip/Gd6+24xIRUelyc3MRFxeHkJAQ2NnZKd0dqga3+0wr8vebI0A2xBogIiKimoEByIY4BUZERDXduHHj4OTkVOpj3LhxSnev2rAI2oYYgIiIqKabPXs2Jk+eXOq+6iwLURoDkA3peBUYERHVcN7e3vD29la6G3cdp8BsSKuR14BgDRAREZGyGIBsiFNgRERENQMDkA3pGICIiIhqBAYgG+IIEBERUc3AAGRDWsvNUAW4/iQREZFyGIBsSKsp+nHnsxCaiIjuouDgYCxcuNDyXJIkrFu3rsz2Fy5cgCRJOHTo0B2PvWXLFkiShLS0tCr3Uym8DN6GzDVAgDwNptMwfxIRkW1cvXoV7u7uSnejxmAAsiHtLQGIiIjIVnx9fZXuQo3CIQgbUqskqOQyIBgYgIiIqAyffvop/P39YTJZ/614+OGH8eSTT+Kff/7Bww8/DB8fHzg5OaFTp07YtGnTbY956xTYnj170K5dO9jZ2aFjx444ePBglfr8448/olWrVtDr9QgODsY777xjtf/jjz9GWFgY7Ozs4OPjg0cffdSyb/Xq1QgPD4e9vT08PDwQFRWF7OzsKvXnTjgCZGNatQp5BSbWABERKUUIID/H9u+rdQAkqVxNH3vsMTz//PPYvHkzevfuDQBITU3Fxo0bsWHDBmRlZWHAgAF44403oNfr8c0332DQoEE4ffo0AgMD73j8rKwsPPjgg+jTpw++++47xMXFYcKECZU+tf379+Nf//oXZs6ciWHDhmHHjh149tln4eHhgejoaOzbtw8vvPACvv32W3Tt2hWpqan4+++/AchTcyNGjMC8efMwZMgQZGZm4u+//77rFwsxANmYzhyAeDsMIiJl5OcAb/rb/n1fTQB0juVq6u7ujv79+2P58uWWALR69Wp4enrivvvug0qlQkREhKX9nDlzsHbtWqxfvx7jx4+/4/GXL18Ok8mEL774AnZ2dmjVqhUuX76MZ555plKn9u6776J3796YNm0aAKBp06Y4ceIE5s+fj+joaMTHx8PR0REPPvggnJ2dERQUhHbt2gGQA1BBQQGGDh2KoKAgAEB4eHil+lERnAKzMfOVYKwBIiKi2xk5ciR+/PFH5OXlAQCWLVuG4cOHQ6VSISsrC5MnT0aLFi3g5uYGJycnnDx5EvHx8eU69smTJ9GmTRvY2dlZtkVGRla6rydPnkS3bt2stnXr1g1nz56F0WhEnz59EBQUhMaNG+OJJ57AsmXLkJMjj8JFRESgd+/eCA8Px2OPPYbPPvsMN27cqHRfyosjQDZmXguINUBERArROsijMUq8bwUMGjQIQgj8+uuv6NSpE/7++2+89957AIDJkycjJiYGCxYsQGhoKOzt7fHoo4/CYDDcjZ5XmbOzMw4cOIAtW7bgjz/+wPTp0zFz5kzs3bsXbm5uiImJwY4dO/DHH39g0aJFmDp1Knbv3o2QkJC71ieOANlY0WrQrAEiIlKEJMlTUbZ+lLP+x8zOzg5Dhw7FsmXLsGLFCjRr1gzt27cHAGzfvh3R0dEYMmQIwsPD4evriwsXLpT72C1atMCRI0eQm5tr2bZr164K9e/W423fvt1q2/bt29G0aVOo1WoAgEajQVRUFObNm4cjR47gwoUL+PPPPwHIBdrdunXDrFmzcPDgQeh0Oqxdu7bS/SkPjgDZGO8HRkRE5TVy5Eg8+OCDOH78OB5//HHL9rCwMKxZswaDBg2CJEmYNm1aiSvGbuff//43pk6diqeffhqvvPIKLly4gAULFlS6ny+++CI6deqEOXPmYNiwYdi5cyc+/PBDfPzxxwCAX375BefPn0fPnj3h7u6ODRs2wGQyoVmzZti9ezdiY2PxwAMPwNvbG7t370ZKSgpatGhR6f6UB0eAbMwyAsQiaCIiuoP7778fDRo0wOnTp/Hvf//bsv3dd9+Fu7s7unbtikGDBqFv376W0aHycHJyws8//4yjR4+iXbt2mDp1Kt5+++1K97N9+/b44YcfsHLlSrRu3RrTp0/H7NmzER0dDQBwc3PDmjVrcP/996NFixZYsmQJVqxYgVatWsHFxQV//fUXBgwYgKZNm+K1117DO++8g/79+1e6P+UhCd6UqoSMjAy4uroiPT0dLi4u1XrsBxf9jWNXMrB0TCfc28y7Wo9NRETWcnNzERcXh5CQEKuCX6q9bveZVuTvN0eAbIw1QERERMpjALIxLWuAiIiohhs3bhycnJxKfYwbN07p7lULFkHbmPkyeAYgIiKqqWbPno3JkyeXuq+6S0OUwgBkY+YRIAOLoImIqIby9vaGt3fdrlOtEVNgH330EYKDg2FnZ4cuXbpgz549Zbb97LPP0KNHD7i7u8Pd3R1RUVEl2gshMH36dPj5+cHe3h5RUVE4e/bs3T6NcmENEBERkfIUD0Dff/89Jk2ahBkzZuDAgQOIiIhA3759kZycXGr7LVu2YMSIEdi8eTN27tyJgIAAPPDAA7hy5Yqlzbx58/DBBx9gyZIl2L17NxwdHdG3b1+rBZ+UwnWAiIhsryJr5FDNVl2fpeKXwXfp0gWdOnXChx9+CEA+sYCAADz//POYMmXKHV9vNBrh7u6ODz/8EKNGjYIQAv7+/njxxRct85fp6enw8fHB0qVLMXz48Dse825eBj9x5UGsO5SA1wa2wFM9GlfrsYmIyJrJZMLZs2ehVqvh5eUFnU4HqYIrMlPNIISAwWBASkoKjEYjwsLCoFJZj+NU5O+3ojVABoMB+/fvxyuvvGLZplKpEBUVhZ07d5brGDk5OcjPz0eDBg0AAHFxcUhMTERUVJSljaurK7p06YKdO3eWGoDy8vIsN5sD5B/g3WKpAeIIEBHRXadSqRASEoKrV68iIUGB+39RtXNwcEBgYGCJ8FNRigaga9euwWg0wsfHx2q7j48PTp06Va5jvPzyy/D397cEnsTERMsxbj2med+t5s6di1mzZlW0+5ViuRt8AWuAiIhsQafTITAwEAUFBTAajUp3h6pArVZDo9FUyyherb4K7K233sLKlSuxZcuWKq3w+corr2DSpEmW5xkZGQgICKiOLpbAGiAiItuTJAlarRZarVbprlANoWgA8vT0hFqtRlJSktX2pKQk+Pr63va1CxYswFtvvYVNmzahTZs2lu3m1yUlJcHPz8/qmG3bti31WHq9Hnq9vpJnUTFcB4iIiEh5il4FptPp0KFDB8TGxlq2mUwmxMbGIjIysszXzZs3D3PmzMHGjRvRsWNHq30hISHw9fW1OmZGRgZ2795922PaCmuAiIiIlKf4FNikSZMwevRodOzYEZ07d8bChQuRnZ2NMWPGAABGjRqFhg0bYu7cuQCAt99+G9OnT8fy5csRHBxsqesxL9EtSRImTpyI119/HWFhYQgJCcG0adPg7++PwYMHK3WaFrwVBhERkfIUD0DDhg1DSkoKpk+fjsTERLRt2xYbN260FDHHx8dbVXovXrwYBoMBjz76qNVxZsyYgZkzZwIA/ve//yE7Oxtjx45FWloaunfvjo0bN9aIOwHrWARNRESkOMXXAaqJ7uY6QJ/+9Q/e3HAKQ9s1xLvD2lbrsYmIiOqzivz9Vnwl6PqGNUBERETKYwCyMdYAERERKY8ByMZ0vBkqERGR4hiAbEyr4TpARERESmMAsjFLDVABAxAREZFSGIBsjDVAREREymMAsjHWABERESmPAcjGOAJERESkPAYgGzPfDJXrABERESmHAcjGtBqOABERESmNAcjGLDVAvBcYERGRYhiAbIw1QERERMpjALIx1gAREREpjwHIxjgCREREpDwGIBvTFRZBF3AdICIiIsUwANmYeQSowCRgMjEEERERKYEByMbMNUAAkG/iNBgREZESGIBszDwCBPB2GEREREphALIxqwDEO8ITEREpggHIxtQqCWqVPA3GK8GIiIiUwQCkAK4FREREpCwGIAUUrQXEGiAiIiIlMAApQMfFEImIiBTFAKQA8wiQgUXQREREimAAUoBWwyJoIiIiJTEAKYA1QERERMpiAFIAa4CIiIiUxQCkAEsNEAMQERGRIhiAFGBeB4grQRMRESmDAUgBrAEiIiJSFgOQAnQa1gAREREpiQFIAawBIiIiUhYDkAIsNUAMQERERIpgAFKApQaIRdBERESKYABSgI5F0ERERIpiAFIAa4CIiIiUxQCkAN4LjIiISFkMQArQ8lYYREREitIo3YF65fJ+IH4nmmU6AvBmDRAREZFCOAJkS3FbgD+motWNWACAgVeBERERKYIByJY0dgAALfIBcAqMiIhIKQxAtqTRAwC0wgCAAYiIiEgpDEC2pLEHAOhEHgCuA0RERKQUBiBbKhwB0pjkESCuA0RERKQMBiBbKqwB0pinwFgETUREpAgGIFvSFgYgE2uAiIiIlMQAZEvmESATa4CIiIiUxABkS4UBSF0YgFgDREREpAwGIFsqLIJWcwqMiIhIUQxAtlR4GbzKmAuAAYiIiEgpDEC2ZB4BMhbWABWwBoiIiEgJDEC2VFgDpDIZAAiOABERESmEAciWCkeAAECPfBZBExERKYQByJa09pZv9TBwBIiIiEghDEC2pNIAkvwj1yOf6wAREREphAHIliTJUgekl/J5KwwiIiKFMADZmjkAsQaIiIhIMQxAtlYYgOxYA0RERKQYBiBbK7wSTI98mARgNLEOiIiIyNYYgGytWA0QwNWgiYiIlMAAZGvaoikwgDdEJSIiUgIDkK0VK4IGwCvBiIiIFMAAZGuFNUAOKvMUGGuAiIiIbI0ByNYKR4AcVAUAWANERESkBMUD0EcffYTg4GDY2dmhS5cu2LNnT5ltjx8/jkceeQTBwcGQJAkLFy4s0WbmzJmQJMnq0bx587t4BhVkCUDyCBBrgIiIiGxP0QD0/fffY9KkSZgxYwYOHDiAiIgI9O3bF8nJyaW2z8nJQePGjfHWW2/B19e3zOO2atUKV69etTy2bdt2t06h4m4JQBwBIiIisj1FA9C7776Lp59+GmPGjEHLli2xZMkSODg44Msvvyy1fadOnTB//nwMHz4cer2+1DYAoNFo4Ovra3l4enrerVOouMIaIHupcAqsgDVAREREtqZYADIYDNi/fz+ioqKKOqNSISoqCjt37qzSsc+ePQt/f380btwYI0eORHx8/G3b5+XlISMjw+px1xTeEd4cgDgFRkREZHuKBaBr167BaDTCx8fHaruPjw8SExMrfdwuXbpg6dKl2LhxIxYvXoy4uDj06NEDmZmZZb5m7ty5cHV1tTwCAgIq/f53ZBkBktcB4hQYERGR7SleBF3d+vfvj8ceewxt2rRB3759sWHDBqSlpeGHH34o8zWvvPIK0tPTLY9Lly7dvQ6a7wXGlaCJiIgUo1HqjT09PaFWq5GUlGS1PSkp6bYFzhXl5uaGpk2b4ty5c2W20ev1t60pqlaFI0AMQERERMpRbARIp9OhQ4cOiI2NtWwzmUyIjY1FZGRktb1PVlYW/vnnH/j5+VXbMatEI9cAmQOQgUXQRERENqfYCBAATJo0CaNHj0bHjh3RuXNnLFy4ENnZ2RgzZgwAYNSoUWjYsCHmzp0LQC6cPnHihOX7K1eu4NChQ3ByckJoaCgAYPLkyRg0aBCCgoKQkJCAGTNmQK1WY8SIEcqc5K0sd4NnDRAREZFSFA1Aw4YNQ0pKCqZPn47ExES0bdsWGzdutBRGx8fHQ6UqGqRKSEhAu3btLM8XLFiABQsWoFevXtiyZQsA4PLlyxgxYgSuX78OLy8vdO/eHbt27YKXl5dNz61Mt94LjAGIiIjI5hQNQAAwfvx4jB8/vtR95lBjFhwcDCFuP2W0cuXK6ura3VE4AqRjACIiIlJMnbsKrMYrXAdIL+QpMANvhkpERGRzDEC2VjgCpDXXABVwBIiIiMjWGIBsrbAGSCdYBE1ERKQUBiBbK7wMXitYA0RERKQUBiBbM0+BiTwArAEiIiJSAgOQrRVOgWk5BUZERKQYBiBbKxwB0phYBE1ERKQUBiBbK7wMXiMMkGBCgYlTYERERLbGAGRrmqKbrupQAAOnwIiIiGyOAcjWCmuAAPl+YJwCIyIisj0GIFtTaQBJ/rHrkc8iaCIiIgUwANmaJFnWArKTDMjnZfBEREQ2xwCkhMI6ID3yWQNERESkAAYgJRTWAXEKjIiISBkMQErQygHIDgYGICIiIgUwACnBPAIk5SO/gDVAREREtsYApATWABERESmKAUgJrAEiIiJSFAOQEjSsASIiIlISA5ASitcAcR0gIiIim2MAUoKlBsgAA2+FQUREZHMMQEpgDRAREZGiGICUwHWAiIiIFMUApATWABERESmKAUgJXAeIiIhIUQxASjDfDb5wCkwIjgIRERHZEgOQEoqNAAkBGE0MQERERLbEAKSEYjVAAFgHREREZGMMQEooNgIEgHVARERENsYApARtUQ0QAF4KT0REZGMMQEooHAGys0yBMQARERHZEgOQEsw3QzUHoALWABEREdkSA5ASbhkBYg0QERGRbTEAKUHDGiAiIiIlMQApodjNUAEGICIiIltjAFJC4RSYjkXQREREimAAUoL5MnghT4EZWARNRERkU5UKQJcuXcLly5ctz/fs2YOJEyfi008/rbaO1WnmESDWABERESmiUgHo3//+NzZv3gwASExMRJ8+fbBnzx5MnToVs2fPrtYO1kmFNUBaFECCiQGIiIjIxioVgI4dO4bOnTsDAH744Qe0bt0aO3bswLJly7B06dLq7F/dVDgCBAA6FDAAERER2VilAlB+fj70evmP+KZNm/DQQw8BAJo3b46rV69WX+/qqsLL4AH5UngDb4ZKRERkU5UKQK1atcKSJUvw999/IyYmBv369QMAJCQkwMPDo1o7WCepNYCkBiBfCp9fwBEgIiIiW6pUAHr77bfxySef4N5778WIESMQEREBAFi/fr1laozuwLwWkGTgFBgREZGNaSrzonvvvRfXrl1DRkYG3N3dLdvHjh0LBweHautcnabRA/nZsEM+AxAREZGNVWoE6ObNm8jLy7OEn4sXL2LhwoU4ffo0vL29q7WDdVbhWkB61gARERHZXKUC0MMPP4xvvvkGAJCWloYuXbrgnXfeweDBg7F48eJq7WCdVXglmJ4jQERERDZXqQB04MAB9OjRAwCwevVq+Pj44OLFi/jmm2/wwQcfVGsH6yxLDRCLoImIiGytUgEoJycHzs7OAIA//vgDQ4cOhUqlwj333IOLFy9WawfrrMIAZAcWQRMREdlapQJQaGgo1q1bh0uXLuH333/HAw88AABITk6Gi4tLtXawzip2R3jWABEREdlWpQLQ9OnTMXnyZAQHB6Nz586IjIwEII8GtWvXrlo7WGexBoiIiEgxlboM/tFHH0X37t1x9epVyxpAANC7d28MGTKk2jpXpxWvAWIAIiIisqlKBSAA8PX1ha+vr+Wu8I0aNeIiiBWhLaoBMjAAERER2VSlpsBMJhNmz54NV1dXBAUFISgoCG5ubpgzZw5MJv4xLxdLDZABhgLWABEREdlSpUaApk6dii+++AJvvfUWunXrBgDYtm0bZs6cidzcXLzxxhvV2sk6iTVAREREiqlUAPr666/x+eefW+4CDwBt2rRBw4YN8eyzzzIAlYf5MnjeC4yIiMjmKjUFlpqaiubNm5fY3rx5c6Smpla5U/VCscvgGYCIiIhsq1IBKCIiAh9++GGJ7R9++CHatGlT5U7VC1wHiIiISDGVmgKbN28eBg4ciE2bNlnWANq5cycuXbqEDRs2VGsH66ziNUC8FQYREZFNVWoEqFevXjhz5gyGDBmCtLQ0pKWlYejQoTh+/Di+/fbb6u5j3VR4N3jWABEREdlepdcB8vf3L1HsfPjwYXzxxRf49NNPq9yxOo9XgRERESmmUiNAVA1YA0RERKQYBiClcASIiIhIMQxAStGwBoiIiEgpFaoBGjp06G33p6WlVbgDH330EebPn4/ExERERERg0aJFZd5T7Pjx45g+fTr279+Pixcv4r333sPEiROrdEzF8CowIiIixVRoBMjV1fW2j6CgIIwaNarcx/v+++8xadIkzJgxAwcOHEBERAT69u2L5OTkUtvn5OSgcePGeOutt+Dr61stx1RM8XuBsQaIiIjIpiQhhGJ/fbt06YJOnTpZFlU0mUwICAjA888/jylTptz2tcHBwZg4cWKJEaCqHNMsIyMDrq6uSE9Ph4uLS8VPrDwu7wc+vx+XhScGqhbj8IwH7s77EBER1RMV+futWA2QwWDA/v37ERUVVdQZlQpRUVHYuXNnjTnmXaMtGgFiDRAREZFtVXodoKq6du0ajEYjfHx8rLb7+Pjg1KlTNj1mXl4e8vLyLM8zMjIq9f4VwnuBERERKYZXgQGYO3euVS1TQEDA3X9Tq8vgBRSciSQiIqp3FAtAnp6eUKvVSEpKstqelJRUZoHz3TrmK6+8gvT0dMvj0qVLlXr/Cim8DF4vFUAFE/JZCE1ERGQzigUgnU6HDh06IDY21rLNZDIhNjbWcoNVWx1Tr9fDxcXF6nHXFY4AAYCO02BEREQ2pVgNEABMmjQJo0ePRseOHdG5c2csXLgQ2dnZGDNmDABg1KhRaNiwIebOnQtALnI+ceKE5fsrV67g0KFDcHJyQmhoaLmOWWMU1gABrAMiIiKyNUUD0LBhw5CSkoLp06cjMTERbdu2xcaNGy1FzPHx8VCpigapEhIS0K5dO8vzBQsWYMGCBejVqxe2bNlSrmPWGGoNhKSGJIywgwEGBiAiIiKbUXQdoJrKJusAAcCbDQFDFnrmvYcVL49AQzf7u/deREREdVytWAeIwNthEBERKYQBSEnFbodRYGIAIiIishUGICUVjgDZwQBDAWciiYiIbIUBSEmWtYB4FRgREZEtMQApyWo1aAYgIiIiW2EAUlKx+4HxMngiIiLbYQBSUuEd4e1g4K0wiIiIbIgBSEnmESCJl8ETERHZEgOQklgDREREpAgGICVpiqbAWANERERkOwxASipWBM0aICIiItthAFKSpQbIwCkwIiIiG2IAUhJrgIiIiBTBAKSk4jVAvAqMiIjIZhiAlKRlDRAREZESGICUVHwdIE6BERER2QwDkJJYA0RERKQIBiAlFd4NnusAERER2RYDkJLMI0BSPvILWANERERkKwxASrJaCJEjQERERLbCAKSkwhEg+W7wDEBERES2wgCkJK1cA6RDPmuAiIiIbIgBSEnFa4C4DhAREZHNMAApyVIDZEA+V4ImIiKyGQYgJVluhcEiaCIiIltiAFJSsREg1gARERHZDgOQkgoDkE4yoqAgX+HOEBER1R8MQEoqLIIGABgNyvWDiIionmEAUlLhCBAAqApyFewIERFR/cIApCS1BiZJI39fkKdsX4iIiOoRBiCFCbU8DSYZOQJERERkKwxACjMVBiC18ZYRICGAdc8C619QoFdERER1GwOQwkRhHZDq1gCUfgk4tAw48DWQk6pAz4iIiOouBiClFV4JpjLdEoBSThd9n5Fgww4RERHVfQxACitzBCjlVNH3mVdt2CMiIqK6jwFIaYUjQJpb1wEqHoAyrtiwQ0RERHUfA5DCJI09AEBz2ykwjgARERFVJwYgpWkLR4CEASaTkLcJcUsA4ggQERFRdWIAUphkviO8ZEC+qfCGqJmJQF5GUSMWQRMREVUrBiCFqXTyFJge+cg3Fo4AFa//AVgETUREVM0YgBQmaeURID3ykV9QOAJknv7yCJO/cgqMiIioWjEAKUylKRaAjOYAVDgC1OQ++WtuOmDIVqB3REREdRMDkNKK1QAZjLeMADXqBOic5O95JRgREVG1YQBSWvEpMHMN0LXCAOTVDHDxl7/nNBgREVG1YQBSmmUKzCBPgWVfA3KuA5DkGiBzAGIhNBERUbVhAFJa4UrQeuTDUGAqqv9xCwR0DoAzR4CIiIiqGwOQ0oqvA2QsFoC8mstfLVNgXAuIiIioujAAKU1zSw1QSrH6H6BYAOIUGBERUXVhAFLarZfBWwLQrSNAnAIjIiKqLgxASiusAbJcBl9WAGIRNBERUbVhAFKatuhWGKacG0BWorzds3AVaHMRdFYyUGBQoINERER1DwOQ0opdBZZz5YS8zaUhYOcif+/gAah1AERROCIiIqIqYQBSWmENkA75yLh0TN5mLoAGAJUKcPaTv2chNBERUbVgAFJasRog9fUz8jZz/Y8ZC6GJiIiqFQOQ0jRFNUC+eRflbcVHgACuBURERFTNGICUVjgCZC/lI1RVOMJT1ggQrwQjIiKqFhqlO1DvFdYA2SMXjtJNeZtnU+s2vB0GERFRteIIkNIKA5AK8p3g01TugEMD6zZcDZqIiKhaMQApTWtn9fS00R9Gk7BuwxogIiKiasUApDS13urpKaM/ziZnWrex1AAlACaTjTpGRERUdzEAKU2tAVRFpVhnRSMcjE+zbuPkA0gqwFQA5Fyzbf+IiIjqIAagmqDwUngA+Ef442D8Dev9ai3g6C1/z0JoIiKiKmMAqgk0RdNgZ02NcOhSWsk2rAMiIiKqNgxANUHhlWAmO3dcgwvOJmchIzffug0DEBERUbWpEQHoo48+QnBwMOzs7NClSxfs2bPntu1XrVqF5s2bw87ODuHh4diwYYPV/ujoaEiSZPXo16/f3TyFqikcAVJ5N0dAAwcIARy5lG7dhgGIiIio2igegL7//ntMmjQJM2bMwIEDBxAREYG+ffsiOTm51PY7duzAiBEj8J///AcHDx7E4MGDMXjwYBw7dsyqXb9+/XD16lXLY8WKFbY4ncrRFtYAeTVDuwB3AMChS7fUAXE1aCIiomqjeAB699138fTTT2PMmDFo2bIllixZAgcHB3z55Zeltn///ffRr18/vPTSS2jRogXmzJmD9u3b48MPP7Rqp9fr4evra3m4u7vb4nQqx1wD5NkMbQPcAKDklWBcDZqIiKjaKBqADAYD9u/fj6ioKMs2lUqFqKgo7Ny5s9TX7Ny506o9APTt27dE+y1btsDb2xvNmjXDM888g+vXr5fZj7y8PGRkZFg9bMq3jXyZe0hPtAt0AwAcvJQGIYotiMgpMCIiomqjaAC6du0ajEYjfHx8rLb7+PggMTGx1NckJibesX2/fv3wzTffIDY2Fm+//Ta2bt2K/v37w2g0lnrMuXPnwtXV1fIICAio4plV0MB3gf+dB3xbo6W/C3RqFVKzDYhPzSlqU/x2GEKUfhwiIiIqF8WnwO6G4cOH46GHHkJ4eDgGDx6MX375BXv37sWWLVtKbf/KK68gPT3d8rh06ZJtO6xSAfbyFJ1eo0ZLfxcAsL4c3hyA8rOB3FsKpImIiKhCFA1Anp6eUKvVSEpKstqelJQEX1/fUl/j6+tbofYA0LhxY3h6euLcuXOl7tfr9XBxcbF6KMkyDVa8DkhrbwlJnAYjIiKqGkUDkE6nQ4cOHRAbG2vZZjKZEBsbi8jIyFJfExkZadUeAGJiYspsDwCXL1/G9evX4efnVz0dv8vaBcpBp8SK0C4N5a+ZDEBERERVofgU2KRJk/DZZ5/h66+/xsmTJ/HMM88gOzsbY8aMAQCMGjUKr7zyiqX9hAkTsHHjRrzzzjs4deoUZs6ciX379mH8+PEAgKysLLz00kvYtWsXLly4gNjYWDz88MMIDQ1F3759FTnHimpXeCXYiasZyM0vVrfkXBjgOAJERERUJZo7N7m7hg0bhpSUFEyfPh2JiYlo27YtNm7caCl0jo+Ph0pVlNO6du2K5cuX47XXXsOrr76KsLAwrFu3Dq1btwYAqNVqHDlyBF9//TXS0tLg7++PBx54AHPmzIFery+1DzVNI3d7eDrpcC3LgOMJGegQVDj1VbwQmoiIiCpNEoKXFN0qIyMDrq6uSE9PV6we6Kmv92HTySS8NrAFnurRWN645W1gy5tA+9HAQx8o0i8iIqKaqiJ/vxWfAqPSmQuhra8E4xQYERFRdWAAqqHalbYiNG+HQUREVC0YgGqoNgFu0KolXEm7iSOX0+SNvB0GERFRtWAAqqGc9BoMaiMHnq+2X5A3mkeAbt4A8m8q0zEiIqI6gAGoBhvTLQQA8MuRBCRn5AJ2roDWUd7JOiAiIqJKYwCqwcIbuaJjkDvyjQLf7boISBILoYmIiKoBA1AN92R3eRRo2e54eVFEFkITERFVGQNQDfdASx80dLPH9WwD1h9OKLodBguhiYiIKo0BqIbTqFUYFRkEAPhyWxwEb4dBRERUZQxAtcDwToGw16pxKjETcQZXeSMDEBERUaUxANUCrg5aPNJBnvr69aJa3ph2UcEeERER1W4MQLVEdFe5GHrdJTt5w/XzAG/jRkREVCkMQLVEqLcTejX1QrzJGyaogPxsICtJ6W4RERHVSgxAtciYbsHIhwYJwlPecP0fZTtERERUSzEA1SI9w7zQ2MsR500+8oZUBiAiIqLKYACqRVQqCWO6hSBO+AIAshPPKNwjIiKi2okBqJYZ1jEAuc7BAIATxw5BsBCaiIiowhiAahmdRoUH7+sOAHDKuojle+IV7hEREVHtwwBUCzVsEg4ACJKS8PovJ3A+JUvhHhEREdUuDEC1kVsghKSGg5QHl/xr+L8fDqPAaFK6V0RERLUGA1BtpNZCcgsEALS0S8HhS2n4aDOvCCMiIiovBqDayqMJAGBCW/kj/ODPszh8KU3BDhEREdUeDEC1VQM5AEU4pGJgGz8YTQL/9/0h3DQYFe4YERFRzccAVFsVjgBJqf/gjcGt4e2sx/lr2Zj+0zGFO0ZERFTzMQDVVoUjQEg9DzcHHRYOawuVBKzafxnf7+Wl8URERLfDAFRbNZDvDo/U84DJhK6hnpjUpykAYNpPx3E8IV3BzhEREdVsDEC1lVsQoNIABblAZgIA4Nl7Q3F/c28YCkx45rsDSL+Zr3AniYiIaiYGoNpKrZFDEGC5K7xKJeHdf0Wgkbs94lNzMHnVYd4qg4iIqBQMQLWZh7kOqGgNIDcHHT4e2R46tQoxJ5LwyV/nFeocERFRzcUAVJsVK4Qurk0jN8x4qCUAYN7GU9h1/rqte0ZERFSjMQDVZuYRoOslR3n+3TkQQ9s1hEkAzy07gK1nUmzcOSIiopqLAag2s1wJVvI2GJIk4Y0h4Wjl74Lr2QaM/nIPXl59BBm5LIwmIiJiAKrNLFNgcYCp5M1Q7XVqrB7XFWO6BUOSgO/3XULf9/7C5tPJNu4oERFRzcIAVJu5BgAqLWDMAzIul9rEXqfGjEGt8P3YSAR7OOBqei7GfLUXL606zMvkiYio3mIAqs3UGsA9WP7++u3vBt85pAF+m9ATT3YLgVS4YvRjS3Ygk1NiRERUDzEA1XalXApfFnudGtMHtcQP/42El7MeZ5KyMHHlIRhNXCuIiIjqFwag2q54HVA5dQpugM9GdYReo0LsqWTM+/3UXeocERFRzcQAVNuZrwS7wxTYrdoGuGHeo20AAJ9sPY8f95deQ0RERFQXMQDVdhWYArvVw20bYvx9oQCAV9YcxYH4G9XZMyIiohqLAai2M0+B3bgAmIwVfvmkPk3xQEsfGIwmjP1mPxLSblZv/4iIiGogBqDazrURoNYBRgOQfqnCL1epJLw3rC2a+zrjWlYenv5mH/4+m4Ld56/jYPwNHE9Ix7nkTCSk3US+seRaQ0RERLWRRukOUBWp1IB7CHDttFwHZL4svgIc9Rp8ProjHv5wO44nZOCJL/aU2k6SAC8nPfxc7eDnag9fVzt0C/VEn5Y+VTwJIiIi22IAqgs8msgBKPU8gN6VOkQjdwd8Gd0J834/hetZBhgKTMgrMCHfaILBaEJWbgEKTALJmXlIzszD4cvpAIClOy5gUIQ/Xn+4NVwdtNV4UkRERHcPA1Bd0KCx/DW15E1RKyIiwA3Lnrqn1H0mk8D1bAMS03NxNf0mEjNycSYpEyv2XMLPhxOw70Iq3nksAl1DPavUByIiIltgAKoLzAGogpfCV4RKJcHLWQ8vZz3CG7latj/SvhH+7/tDuHA9B//+fDee7hGCyX2bQa9RV+n9hBBIyczD6aRMnEnKwpnETJxOykT6zXyM6RaMJ+4JgiRJVT0tIiKqpyQhBJcBvkVGRgZcXV2Rnp4OFxcXpbtzZ+e3AN88DHiEAs/vt/nbZ+cV4PVfT2LFnngAQHNfZ9zT2ANpOQak3cxH+s18pOfIt9wY3K4hHr8nCA0cdaUeKz0nH9/tvohvdl5AUkZeme85sI0f3hoaDmc7TrsREZGsIn+/GYBKUesCUNolYGFrQKUBpibJ9wi7nZxUIGYaEBgJtHu82rqx6UQSXv7xCK5nG27bzk6rwqMdGuE/3RsjxNMRAHAl7Sa+3BaHFXvikWOQL+dXSUCwhyOa+jijqa8zmvo44cqNm5j/+2kUmARCPB3x8cj2aOFXCz4jIiK66xiAqqjWBSCTCXjTDyjIBV44WDQlVpq0S8B3Q4FrZwCNPfDiKcDerdq6kpKZh+92XUSByQQ3ex1c7bVwddDCzV6LhPSb+GJbHI5dyQAgX1UW1cIHTnoNfj6cgILCe5I193XGf3s1Rv/WfrDTlpxKOxB/A+OXHUBCei70GhXmPNwa/+oUUG3nQEREtRMDUBXVugAEAB/dA6ScBEb+CIRFld4m+ZQcfjKuFG0bsADo/LRt+gi5tmfX+VR89vd5/Hkq2Wpf1yYe+G+vJugZ5nnH+p7UbAMm/XAIW06nAAAGhPuiZ5gXwnycEOrtDFd7To0REdU3DEBVVCsD0MqRwKlfAJeGQLeJQPsnAK190f5Le4BljwG5aYBnM6BZP2D7+4B3K+CZ7fJwjI2dS87EtzsvIq/AhH93CUSbRm4Ver3JJLB46z9454/TuPWG9j4ueoR5O6NtgBvua+6FtgHuUKtYNE1EVJcxAFVRrQxAcX8DPz4FZCXKz518gK7PAx3GABd3AD+MAgpuAo06Af/+QQ487zSXp82eigUadVS2/1VwIP4Gfj6cgHPJWTiXnIWr6bkl2rg5aNEzzAv3N/dGz6ZeZRZhExFR7cUAVEW1MgABQH4ucOg7YNvCotti2DcActMBYQRC+wD/+hrQyYXHWDsOOLxCLoR++CPFul3dMnLzcS5ZvnR++z/XsfV0MjJyCyz7JQkI9XJCm0ZuiAhwRZtGbmjh52y5dD8rrwCJ6bnyI0MOU/Lq1/IK2Pa6orqk1GyDJXidS85CSlYeIhq54t5mXmji5cRL9YmIbIgBqIpqbQAyKzAAR1YCf78L3IiTt7UZJoccdbHamPhdwJd9Aa2DXAxt51r68Wq5AqMJBy+lYfOpZPx5KhmnEjNLtNGqJfi72eN6lgFZeQWlHKWIu4MW3s52SMnKQ+ptrnhr6GaPnk090aupF5r7uiArr0BeEqDwkXEzH/Y6NXxc7ODrYgdfVzt4Ouk5VUdEVEkMQFVU6wOQmbEAOPkTYMgG2j4OqG65960QwMf3ACmngIHvAJ2eUqafNpacmYsjl9Jx5HIajlxJx5HL6SWCjLOdxhJKACAh7SaupudaLtEvrqGbPUK9nRDq7QR3By12x6Vid1wqDAUVv3msWiXBx1mPh9o2xH97Nob7HabqsvMKoFWroNPwvsZERAxAVVRnAlB57FoMbJwC+IQD4/5WpBhaaUIIXL5xE1fSbsLLWQ9fFzs46kuupSSEQEZuAa6m30RSRh48HHVo7OUIB13JtjcNRuyKu46tp1Pw19kUXE3LhYu9Rl4WoPDhYqdFjsGIxIxcJGXkIjkzD8Zi1dxOeg2e7BaM//RobHVVW4HRhL/OpuD7vZcQezIZ9lo1JvZpilGRQdCqKxaEziZlYmHsWeTkFeCtR9rAx8WuQq8nIqpJGICqqF4FoJxUuRjamAc8/SfQsIPSPaq3jCaB61l5OBCfhg9iz+LEVXm9JGc7DZ7u0RhRLXzwy5EE/HjgcqmrZDf1ccLMQa3KdT+2hLSbWLjpDFbvv2y5gq6hmz2+GtMJTX2cq/W8iIhshQGoiupVAAKANWOBI98D7UcBDy0q/+tuXAQ0esDZ9+71rZ4ymQT+OJGI92LO4nRSyZqlBo46DGnXEI92aIRDl9Iwb+Mp3Ci83cjAcD+8OrAFGrrZl3hdWo4Bi7f8g692XLBM0T3Q0gfnkrNw/lo2nO00+PSJjohs4lGh/sZfz8HF1Gx0D73zGk5ERHcLA1AV1bsAdHEH8FV/QOtYWAxdjnM+8zuwYgSgdwKe2QG4Nrr7/ayHTCaBX49excJNZxB3LRs9m3phWMcA9G7hY1X3k56Tj3djTuPbXRdhEvLtRkK9nUoc7+L1HGQWXhHXOaQBpvRvjvaB7riRbcDT3+zDvos3oFOrMP+xNni4bcM79u9aVh4+iD2L5bvjUWAS6BbqgbeGtkFAA4fq+yEQEZUTA1AV1bsAJATwUWf59hgPvgd0fPL27eN3Ad8MltcVAoAm9wOPr6mX9UO2IoRAgUncscbn5NUMzFh/HHviUsts09zXGS/3a457m3lZjdbk5hsx6YdD2HBUXkvq5X7NMa5X41JHdG4ajPhi23ks2XrectWcRiWhwCTgoFNjSv/meLxLEFRVvKLtavpNfL3jIkxC4IXeYXAqpTbLVv5JycL+CzfwcDt/y5IJRFSzMABVUb0LQACw8yPg91cBvwjgv3+V3S7pBPBVP3ltoaDuwJV98mKKDy4EOo6xWXepbEIIHLyUhvSb+SX2Oeo06BBU9qrYJpPAmxtO4vNt8vIJLf1cENDAHn6u9vB1lS/Xz8wrwEd/nrOskRTe0BWvDGgOf1d7/O/HI5bw1Tm4Ad5+tI3lhrcVcS45E0u2nsdPh64g3yj/ExXYwAHvDYtAh6AGFT5eVcWeTMLzKw4ix2BE5+AG+OSJDne8Qo+IbI8BqIrqZQDKSQXeaQYYDcCYjUBQZMk2Ny7K6wZlXgUCugBPrAP2fQn8MRXQFU6FuQeV/z2FAP75EyjIA5r15whSDfLV9jjM/uUEbvevQ0M3e/yvXzMMauNvGekxmQSW7b6Iub+dQo7BCL1Ghf90D0FUSx9ENHK74xpHB+JvYPGWfxBzIsmyrUtIA8tVeioJGH9/GF64PxSaCl7xVhlCCHy1/QJe//WE1e1WGns64qsxnRDkUfFwR0R3DwNQFdXLAATIt9I4ugqQVEDYA0DH/wChvQGVGsi+Joef6+cArxbAmA2AQwPAZAS+GgBc2gWE9ASe+KnkekOlid8FxMyQXwcAbYbL02861o7UFPHXc3AqMQOJGbmWlbGvpuciK68AgyL8MCoyGHba0qeCLqXm4JU1R7Ht3DXLNjcHLbqHygtDdg31RFqOAaeuZuJUYgZOJWbiVGImUjLlq9skSS7OHterCdoFuiMjNx8zfjqOtQflG/m2DXDDwmFtEXyH0SUhBI5cTsdvxxKRmZuPB9v4457GDcpVqF1gNGHWzyfw7a6LAIARnQPw+D1BGPvNflxJu4kGjjp8NqqDIiNSRFQ6BqAqqrcBKOMqsPa/QNzWom1ugfL9xE6uBxIOAq4BwH/+AFz8i9pc/wdY3E2uCbrT3eVTTgObZgGnf5Wfa+wAY758qw7vlsC/vgU8Q+/O+ZFNCSEXcP92NBF/n02xuh1JWbRqCUPaNcTYnk1KLeJefzgBU9ceRWZuARx0agxt3xBNvJwQ7OmIxp6OaOhmD5Uk4UD8Dfx2LBEbjyXiStpNq2M09nTE8M4BeKR9I3g46UvtR2ZuPsYvP4itZ1IgScAr/Zvj6R5yPVRyZi6e+nofjlxOh06jwnv/aouBbfwq90MiomrFAFRF9TYAmV07K09tHVom1/qYOXgAT/4OeIaVfM3uT4Df/iffVuOZHUCDkKJ9JhOQeATY9wVw8DtAmORRpnZPAPdOAVLPA6vGANnJgM4ZGPwx0PIh6+ObTMC103KAcg0AvJoCeq5XU1sUGE04fDkNW0+nYOuZFBy5kg5nvQYt/FzQ3NcZzQu/NvN1LnVhyeKupN3EpO8PYXcphd6+6nTcrz2BX3PDkQ45QDno1LivuTec9Rr8fDgB2YWreWvVEh5o5YuIRq7Iyzcht8CI3HwTcvON2B2XinPJWbDTqrBwWDv0a2291EOOoQAvrDiETSflqbrorsHoEtIAYT7OCPZwqPD0XI6hAAUmARc7bZlthBCIu5aNPXGpOJuchU7BDXB/c+86uQq4ocCEM0mZZS40SlSWWheAPvroI8yfPx+JiYmIiIjAokWL0Llz5zLbr1q1CtOmTcOFCxcQFhaGt99+GwMGDLDsF0JgxowZ+Oyzz5CWloZu3bph8eLFCAsr5Q93Kep9ADIz5ADH18phKPMqMOzbshdKNJmAbx4CLvwNBHUDhiwBzm8Fzm8Gzm8Bcq4XtW3+INB7OuDVrGhbZqIcguJ3yM+7Pi9Pw13aDcTvBi7vsQ5jAODSSD6GV3O5eLvpA4C9e7X+CCrNWCAHuoyrQGaCXFvl5Au4+Mlf6/lUX26+XB9U2TWDjCaB345dxfGEDMSlZONKSiruu7EKY1U/wUnKRbpwxJ++T8Kx+1j0bN7QMlWXnVeAnw8nYMWeeBy+nH7b9/B21uOL0Z0Q3qj0e+QZTQKv/3oCX22/YLVdq5bQ2NMJoT7yrVG0ahV0hbcr0apVEAJIyszF1cLbq1xNz7UUrJtXF2/s6YTGXo4I9nREQtpN7IlLxd4LqbiWZX3LFg9HHYa2b4h/dQxAWC1fwNJQYMK2cyn49UgiYk4kIiO3AK72Wvy7SyCiuwZzlXIql1oVgL7//nuMGjUKS5YsQZcuXbBw4UKsWrUKp0+fhre3d4n2O3bsQM+ePTF37lw8+OCDWL58Od5++20cOHAArVu3BgC8/fbbmDt3Lr7++muEhIRg2rRpOHr0KE6cOAE7uzv/R8QAVEk3LgAfdwXys0vu0zkBIb2AbhOAwC6lv96YD8TOAnaUsRij1kEOPBkJQFZSyf0qrVyz1GoI0GyA9XpGBQZ5BCnxmFzH5BYINOoohyfVLXUshhzgwjbgXIxcpG3Ml0NdcHcgpIf8WjOTSV4+4PJe+ZF4VO5fdrI80lUWO1fA2U9+uPjLj+LfO/kCjp4l+0bWTCbg2Gp5WjXjMgDAqHOB2iCvoo0GjYE+c4DmA0sU2R9PSMeaA1eQmm2AnVYFvUYNO60aeo0KznYaPBThD+9y/NH99chVbD6djLNJmTibnFXq/eKqi06jQtsANzT2dETsqWRLzRQAtAt0Q1QLHxQYBTJy85GZm4/M3AJk5ObjpsGIfKNAvtEEQ4EJBqMJBUYBJzsNGjjq4OGos3x1d9TBSa+Bk14Dx8KHk16DfKMJyZm5SEzPQ2JGLpILb9+ikiS42GvgYqeFi70WLnby9052Ra910mvgZKeBViUhK6/A8sjOMyL9Zj52/HMNMSeSLGtUmc/VvFinVi3hoYiGeKpHCFr48d9kKlutCkBdunRBp06d8OGHHwIATCYTAgIC8Pzzz2PKlCkl2g8bNgzZ2dn45ZdfLNvuuecetG3bFkuWLIEQAv7+/njxxRcxefJkAEB6ejp8fHywdOlSDB8+/I59YgCqgv1LgZ8nyFNc/u3lNYKa3Ac06mR9J/rbObEe2PgKACFfbRbQRQ5NPq2LjpGTKgePlFNA8im5bin5RNEx1HogNEoOGklH5TamkpeFQ+sI+LcDGraXA8f5LcCF7fKtQcriFggERgJZycCV/UBeRuntJLW8Srazr9yfrER5RKjgZuntS7xeBTh6A84+gJOPfC6GbCAvEzBkAXlZ8nNJAvQucuCzfHWWA1iBQT6XAoO8XIEwyfvsXAsfbvJXlVo+piG78LiZ8vcqjRxe9U7yV51T4eiVJB/L/ADkq/pUKrnfklo+pqSW9xsNgKlA/mo0yCNkWnv5uHpneepT7yxvE0a5uN5UUPQQJvlYkqrwuCrgZhrw13wg4YD8/q4BQO8ZQKvB8vTtn2/IQRSQl2zoMUk+X0kq7GPhw+LWfwqlorYwv0aSzxOi6GsxJgEkZeYVrox9EzcNBSgwGmE0GlFgNCLfaAKEgLuTHl5OdvBwsYe3iz28nO0ACCRcy0BCajoSUzORlJaJa2mZcLHTIMzXFc383dDY2xU6rRaQ1CiAhP3xmdh4Ihm7LqQhz6SCCbcfUROF+81fJQhIhecgFT4v6zUAoIKp2OuK2hcdt+R7WP9EBVQQUMMENUxQwQQNjBCQUAAVXB3t0DXUB92b+qBlowbYFZeKH/ZewpErRaN1rf1d4eqghVYlQaWSoFGpLFcXmoT8uZiEgNEkIIR5G2Aq/OgEJEgA7LRq2OvUsNdr4aBVw16ngU6jsuq9OTObj2MyCRghQQgBIQQkSQWNClCrVNAU9ketkoqy9i1/XlWFOyRJbqOSpFI/MblZURuVBLnXhZ0yv04qbFP6p17s97PY72ppn7Hl2Ja+qeSfk1T0ucqHKTqe5dtSjmd1XABSGVd/enl6oZG/f6n7KqvWBCCDwQAHBwesXr0agwcPtmwfPXo00tLS8NNPP5V4TWBgICZNmoSJEydats2YMQPr1q3D4cOHcf78eTRp0gQHDx5E27ZtLW169eqFtm3b4v333y9xzLy8POTlFf3By8jIQEBAAANQZaWcBpy8bT8dlXxKnrI7vkYOR7fSuwK+rQGPULnuKOGg/Ee/NK4BcoAK6yMXal/YJj8SDsh/kIvTOshhr1FHOUi5BQLO/qWP4AghB6bMRHmkKPNqsa9XgYwr8vfZKbcfQaIiOmegx/8B9zwrByizvExg20Jg54dy+COiGmWn/2hEjv2gWo9ZkQCkaHXZtWvXYDQa4ePjY7Xdx8cHp06dKvU1iYmJpbZPTEy07DdvK6vNrebOnYtZs2ZV6hyoFMVre2zJuzng/YpcWJ18Ajj1qxwifMPl0SO3QOtpEJOxcPpqnzySk5UEBHUFQvvI51C8bWhv+WtellyXdHkv4Oglj2x5twTU5fxPSZKKRl9u93MyFgA51+SglJUkP3IzAJ1j4YiJU+H3TvI55mbIwcr8NS9THjHR6OTRJ03hQ1LJ+3LT5Joq88NYUDTKo3eSQ4XOUQ57huzCEafCUSHzqFPxERLL/yqb5Gkp8yiOMMr9UGsLHzr5oVID+TcLR5uyCvucJW9TaeT9xb9ajTgZC78KoPG98uftVHK6HHpnoPc0oEM0sGUucHF70evMxzIZy15/yjLSU+w1EEXnXfyr/IJi/8df+L35Z1N8FKn4/uKjaJIkf1ZqXbGfVbFzN/88TQWF35uKvjcVFO4z3XI+xb8vPhpgfn7LOUhlvMbSvPi5Fx8RK37+xX8O1seQD1Hss5XURctmmG4d+Ss5lWgSAsbCEZ3SxqrM41gAIMyjJlZ9M4+AiGI/iqLvJQir0axbx1ZKGz0xbxdW31u/rrTRsKI+l3K8MoYlRCnPpDKPYu5N0ZjWnUY7JKue3v4V1udU8ud0u5+HmV5f+lWYtsLyegCvvPIKJk2aZHluHgGiWkqSAJ9W8uN2VGrAu4X8aP9E+Y6td5LDkDkQ3S1qTdH0GVWNW4B8ZSEprqpLnaoKH1Q3tFf4/RX9XfL09IRarUZSknVBa1JSEnx9S/+H39fX97btzV8rcky9Xg8XFxerBxEREdVdigYgnU6HDh06IDY21rLNZDIhNjYWkZGl3IoBQGRkpFV7AIiJibG0DwkJga+vr1WbjIwM7N69u8xjEhERUf2i+BTYpEmTMHr0aHTs2BGdO3fGwoULkZ2djTFj5Btrjho1Cg0bNsTcuXMBABMmTECvXr3wzjvvYODAgVi5ciX27duHTz/9FIBcwT5x4kS8/vrrCAsLs1wG7+/vb1VoTURERPWX4gFo2LBhSElJwfTp05GYmIi2bdti48aNliLm+Ph4qIrdW6pr165Yvnw5XnvtNbz66qsICwvDunXrLGsAAcD//vc/ZGdnY+zYsUhLS0P37t2xcePGcq0BRERERHWf4usA1URcB4iIiKj2qcjfbxbUExERUb3DAERERET1DgMQERER1TsMQERERFTvMAARERFRvcMARERERPUOAxARERHVOwxAREREVO8wABEREVG9o/itMGoi8+LYGRkZCveEiIiIysv8d7s8N7lgACpFZmYmACAgIEDhnhAREVFFZWZmwtXV9bZteC+wUphMJiQkJMDZ2RmSJFXrsTMyMhAQEIBLly7V2fuM8RzrBp5j3cBzrBt4juUjhEBmZib8/f2tbqReGo4AlUKlUqFRo0Z39T1cXFzq7C+xGc+xbuA51g08x7qB53hndxr5MWMRNBEREdU7DEBERERU7zAA2Zher8eMGTOg1+uV7spdw3OsG3iOdQPPsW7gOVY/FkETERFRvcMRICIiIqp3GICIiIio3mEAIiIionqHAYiIiIjqHQYgG/roo48QHBwMOzs7dOnSBXv27FG6S5X2119/YdCgQfD394ckSVi3bp3VfiEEpk+fDj8/P9jb2yMqKgpnz55VprOVNHfuXHTq1AnOzs7w9vbG4MGDcfr0aas2ubm5eO655+Dh4QEnJyc88sgjSEpKUqjHFbd48WK0adPGsvBYZGQkfvvtN8v+2n5+pXnrrbcgSRImTpxo2Vbbz3PmzJmQJMnq0bx5c8v+2n5+ZleuXMHjjz8ODw8P2NvbIzw8HPv27bPsr+3/7gQHB5f4HCVJwnPPPQegbnyORqMR06ZNQ0hICOzt7dGkSRPMmTPH6t5dNvscBdnEypUrhU6nE19++aU4fvy4ePrpp4Wbm5tISkpSumuVsmHDBjF16lSxZs0aAUCsXbvWav9bb70lXF1dxbp168Thw4fFQw89JEJCQsTNmzeV6XAl9O3bV3z11Vfi2LFj4tChQ2LAgAEiMDBQZGVlWdqMGzdOBAQEiNjYWLFv3z5xzz33iK5duyrY64pZv369+PXXX8WZM2fE6dOnxauvviq0Wq04duyYEKL2n9+t9uzZI4KDg0WbNm3EhAkTLNtr+3nOmDFDtGrVSly9etXySElJseyv7ecnhBCpqakiKChIREdHi927d4vz58+L33//XZw7d87Sprb/u5OcnGz1GcbExAgAYvPmzUKIuvE5vvHGG8LDw0P88ssvIi4uTqxatUo4OTmJ999/39LGVp8jA5CNdO7cWTz33HOW50ajUfj7+4u5c+cq2KvqcWsAMplMwtfXV8yfP9+yLS0tTej1erFixQoFelg9kpOTBQCxdetWIYR8TlqtVqxatcrS5uTJkwKA2Llzp1LdrDJ3d3fx+eef17nzy8zMFGFhYSImJkb06tXLEoDqwnnOmDFDRERElLqvLpyfEEK8/PLLonv37mXur4v/7kyYMEE0adJEmEymOvM5Dhw4UDz55JNW24YOHSpGjhwphLDt58gpMBswGAzYv38/oqKiLNtUKhWioqKwc+dOBXt2d8TFxSExMdHqfF1dXdGlS5dafb7p6ekAgAYNGgAA9u/fj/z8fKvzbN68OQIDA2vleRqNRqxcuRLZ2dmIjIysc+f33HPPYeDAgVbnA9Sdz/Hs2bPw9/dH48aNMXLkSMTHxwOoO+e3fv16dOzYEY899hi8vb3Rrl07fPbZZ5b9de3fHYPBgO+++w5PPvkkJEmqM59j165dERsbizNnzgAADh8+jG3btqF///4AbPs58maoNnDt2jUYjUb4+PhYbffx8cGpU6cU6tXdk5iYCAClnq95X21jMpkwceJEdOvWDa1btwYgn6dOp4Obm5tV29p2nkePHkVkZCRyc3Ph5OSEtWvXomXLljh06FCdOD8AWLlyJQ4cOIC9e/eW2FcXPscuXbpg6dKlaNasGa5evYpZs2ahR48eOHbsWJ04PwA4f/48Fi9ejEmTJuHVV1/F3r178cILL0Cn02H06NF17t+ddevWIS0tDdHR0QDqxu8pAEyZMgUZGRlo3rw51Go1jEYj3njjDYwcORKAbf9+MAARlcNzzz2HY8eOYdu2bUp3pdo1a9YMhw4dQnp6OlavXo3Ro0dj69atSner2ly6dAkTJkxATEwM7OzslO7OXWH+v2cAaNOmDbp06YKgoCD88MMPsLe3V7Bn1cdkMqFjx4548803AQDt2rXDsWPHsGTJEowePVrh3lW/L774Av3794e/v7/SXalWP/zwA5YtW4bly5ejVatWOHToECZOnAh/f3+bf46cArMBT09PqNXqEtX6SUlJ8PX1VahXd4/5nOrK+Y4fPx6//PILNm/ejEaNGlm2+/r6wmAwIC0tzap9bTtPnU6H0NBQdOjQAXPnzkVERATef//9OnN++/fvR3JyMtq3bw+NRgONRoOtW7figw8+gEajgY+PT504z+Lc3NzQtGlTnDt3rs58jn5+fmjZsqXVthYtWlim+urSvzsXL17Epk2b8NRTT1m21ZXP8aWXXsKUKVMwfPhwhIeH44knnsD//d//Ye7cuQBs+zkyANmATqdDhw4dEBsba9lmMpkQGxuLyMhIBXt2d4SEhMDX19fqfDMyMrB79+5adb5CCIwfPx5r167Fn3/+iZCQEKv9HTp0gFartTrP06dPIz4+vlad561MJhPy8vLqzPn17t0bR48exaFDhyyPjh07YuTIkZbv68J5FpeVlYV//vkHfn5+deZz7NatW4llKM6cOYOgoCAAdeffHQD46quv4O3tjYEDB1q21ZXPMScnByqVdfRQq9UwmUwAbPw5VmtJNZVp5cqVQq/Xi6VLl4oTJ06IsWPHCjc3N5GYmKh01yolMzNTHDx4UBw8eFAAEO+++644ePCguHjxohBCvozRzc1N/PTTT+LIkSPi4YcfrlWXowohxDPPPCNcXV3Fli1brC5NzcnJsbQZN26cCAwMFH/++afYt2+fiIyMFJGRkQr2umKmTJkitm7dKuLi4sSRI0fElClThCRJ4o8//hBC1P7zK0vxq8CEqP3n+eKLL4otW7aIuLg4sX37dhEVFSU8PT1FcnKyEKL2n58Q8hIGGo1GvPHGG+Ls2bNi2bJlwsHBQXz33XeWNnXh3x2j0SgCAwPFyy+/XGJfXfgcR48eLRo2bGi5DH7NmjXC09NT/O9//7O0sdXnyABkQ4sWLRKBgYFCp9OJzp07i127dindpUrbvHmzAFDiMXr0aCGEfCnjtGnThI+Pj9Dr9aJ3797i9OnTyna6gko7PwDiq6++srS5efOmePbZZ4W7u7twcHAQQ4YMEVevXlWu0xX05JNPiqCgIKHT6YSXl5fo3bu3JfwIUfvPryy3BqDafp7Dhg0Tfn5+QqfTiYYNG4phw4ZZrY9T28/P7OeffxatW7cWer1eNG/eXHz66adW++vCvzu///67AFBqv+vC55iRkSEmTJggAgMDhZ2dnWjcuLGYOnWqyMvLs7Sx1ecoCVFs+UUiIiKieoA1QERERFTvMAARERFRvcMARERERPUOAxARERHVOwxAREREVO8wABEREVG9wwBERERE9Q4DEBFROUiShHXr1indDSKqJgxARFTjRUdHQ5KkEo9+/fop3TUiqqU0SneAiKg8+vXrh6+++spqm16vV6g3RFTbcQSIiGoFvV4PX19fq4e7uzsAeXpq8eLF6N+/P+zt7dG4cWOsXr3a6vVHjx7F/fffD3t7e3h4eGDs2LHIysqyavPll1+iVatW0Ov18PPzw/jx4632X7t2DUOGDIGDgwPCwsKwfv36u3vSRHTXMAARUZ0wbdo0PPLIIzh8+DBGjhyJ4cOH4+TJkwCA7Oxs9O3bF+7u7ti7dy9WrVqFTZs2WQWcxYsX47nnnsPYsWNx9OhRrF+/HqGhoVbvMWvWLPzrX//CkSNHMGDAAIwcORKpqak2PU8iqibVfntVIqJqNnr0aKFWq4Wjo6PV44033hBCCAFAjBs3zuo1Xbp0Ec8884wQQohPP/1UuLu7i6ysLMv+X3/9VahUKpGYmCiEEMLf319MnTq1zD4AEK+99prleVZWlgAgfvvtt2o7TyKyHdYAEVGtcN9992Hx4sVW2xo0aGD5PjIy0mpfZGQkDh06BAA4efIkIiIi4OjoaNnfrVs3mEwmnD59GpIkISEhAb17975tH9q0aWP53tHRES4uLkhOTq7sKRGRghiAiKhWcHR0LDElVV3s7e3L1U6r1Vo9lyQJJpPpbnSJiO4y1gARUZ2wa9euEs9btGgBAGjRogUOHz6M7Oxsy/7t27dDpVKhWbNmcHZ2RnBwMGJjY23aZyJSDkeAiKhWyMvLQ2JiotU2jUYDT09PAMCqVavQsWNHdO/eHcuWLcOePXvwxRdfAABGjhyJGTNmYPTo0Zg5cyZSUlLw/PPP44knnoCPjw8AYObMmRg3bhy8vb3Rv39/ZGZmYvv27Xj++edte6JEZBMMQERUK2zcuBF+fn5W25o1a4ZTp04BkK/QWrlyJZ599ln4+flhxYoVaNmyJQDAwcEBv//+OyZMmIBOnTrBwcEBjzzyCN59913LsUaPHo3c3Fy89957mDx5Mjw9PfHoo4/a7gSJyKYkIYRQuhNERFUhSRLWrl2LwYMHK90VIqolWANERERE9Q4DEBEREdU7rAEiolqPM/lEVFEcASIiIqJ6hwGIiIiI6h0GICIiIqp3GICIiIio3mEAIiIionqHAYiIiIjqHQYgIiIiqncYgIiIiKjeYQAiIiKieuf/Afrmm3/4kGKRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scoreなし ver.\n",
    "# best_model = None\n",
    "# best_constant = 0\n",
    "# best_score = -100\n",
    "\n",
    "# grand_train_loss = []\n",
    "# grand_valid_loss = []\n",
    "\n",
    "# for epoch in range(CFG.epochs):\n",
    "#     model.train()\n",
    "#     train_loss = []\n",
    "#     valid_loss = []\n",
    "#     with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\") as tq:\n",
    "#         for data in tq:\n",
    "#             normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "#             segmentation_map = data[\"segmentation_map\"]\n",
    "\n",
    "#             normalized_tomogram = padf(normalized_tomogram)\n",
    "#             segmentation_map = padf(segmentation_map)\n",
    "\n",
    "#             normalized_tomogram = normalized_tomogram.cuda()\n",
    "#             segmentation_map = segmentation_map.long().cuda()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             with autocast():\n",
    "#                 pred = model(preprocess_tensor(normalized_tomogram))\n",
    "#                 loss = seg_loss(pred, segmentation_map)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             scheduler.step()\n",
    "#             train_loss.append(loss.item())\n",
    "\n",
    "#             tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "#     with tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\") as tq:\n",
    "#         for data in tq:\n",
    "#             normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "#             segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "\n",
    "#             normalized_tomogram = padf(normalized_tomogram)\n",
    "#             segmentation_map = padf(segmentation_map)\n",
    "\n",
    "#             with autocast():\n",
    "#                 pred = model(preprocess_tensor(normalized_tomogram))\n",
    "#                 loss = seg_loss(pred, segmentation_map)\n",
    "\n",
    "#             valid_loss.append(loss.item())\n",
    "#             tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "#     # 損失値を記録\n",
    "#     train_epoch_loss = np.mean(train_loss)\n",
    "#     valid_epoch_loss = np.mean(valid_loss)\n",
    "#     grand_train_loss.append(train_epoch_loss)\n",
    "#     grand_valid_loss.append(valid_epoch_loss)\n",
    "\n",
    "#     # ベストモデルを保存\n",
    "#     if valid_epoch_loss < best_score or best_score == -100:\n",
    "#         best_score = valid_epoch_loss\n",
    "#         best_model = model.state_dict()\n",
    "#         torch.save(best_model, \"final_model.pth\")\n",
    "#         print(f\"New best model saved with validation loss: {best_score:.4f}\")\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch {epoch + 1}/{CFG.epochs} Summary:\\n\"\n",
    "#         f\"Train Loss: {train_epoch_loss:.4f}, \"\n",
    "#         f\"Valid Loss: {valid_epoch_loss:.4f}, \"\n",
    "#         f\"Best Validation Loss: {best_score:.4f}\"\n",
    "#     )\n",
    "\n",
    "# # train_lossとvalid_lossのプロット\n",
    "# plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "# plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_lossとvalid_lossのプロット\n",
    "\n",
    "# plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "# plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_scoreとvalid_scoreのプロット\n",
    "# plt.plot(grand_train_score, label=\"train_score\")\n",
    "# plt.plot(grand_valid_score, label=\"valid_score\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.random.randn(1, 16, 1, 320, 320)\n",
    "\n",
    "# x[0, :, 0, :0, :0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "# segmentation_map = data[\"segmentation_map\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
