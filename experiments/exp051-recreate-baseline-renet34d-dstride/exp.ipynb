{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from src.inference import inference, inference2pos, create_gt_df\n",
    "from metric import visualize_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [01:26<00:00,  6.50it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "  0%|          | 0/280 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 630, 630])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    "    augmentation=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "# train_nshuffle_dataset = EziiDataset(\n",
    "#     exp_names=CFG.train_exp_names,\n",
    "#     base_dir=\"../../inputs/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.train_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    augmentation=False,\n",
    "    train=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ctfdeconvolved', 'denoised', 'isonetcorrected', 'none', 'wbp'],\n",
       "       dtype='<U15'),\n",
       " array([ 66,  66,  66, 297,  66]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習時のデータパターン\n",
    "\n",
    "z_list = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    z = train_dataset[i][\"zarr_type\"]\n",
    "    z_list.append(z)\n",
    "\n",
    "np.unique(np.array(z_list), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder, num_domains=5).to(\"cuda\")\n",
    "# model.load_state_dict(torch.load(\"./pretrained_model.pth\"))\n",
    "# model.load_state_dict(torch.load(\"./best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 16, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input-test\n",
    "\n",
    "x = torch.randn(2, 16, 1, 64, 64).cuda()\n",
    "model(x, torch.tensor([2, 0]).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"encoder\"と名のつくパラメータは学習しない\n",
    "# for layer, param in model.named_parameters():\n",
    "#     if \"encoder\" in layer:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# サンプルデータ\n",
    "num_classes = len(CFG.particles_name)  # クラス数\n",
    "colors = plt.cm.tab10(\n",
    "    np.arange(len(CFG.particles_name))\n",
    ")  # \"tab10\" カラーマップから色を取得\n",
    "\n",
    "# ListedColormap を作成\n",
    "class_colormap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "# カラーバー付きプロット\n",
    "def plot_with_colormap(data, title, original_tomogram):\n",
    "    masked_data = np.ma.masked_where(data <= 0, data)  # クラス0をマスク\n",
    "    plt.imshow(original_tomogram, cmap=\"gray\")\n",
    "    im = plt.imshow(masked_data, cmap=class_colormap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([6, 16, 320, 320])\n",
      "Augmented shape: torch.Size([6, 16, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# 回転\n",
    "# 3Dテンソルの各軸に対して指定した角度で回転する関数\n",
    "def rotate_3d(tomogram, segmentation_map, angle):\n",
    "    \"\"\"Rotates the 3D tensors tomogram and segmentation_map around the Z-axis.\"\"\"\n",
    "    rotated_tomogram = TF.rotate(tomogram, angle, expand=False)\n",
    "    rotated_segmentation_map = TF.rotate(segmentation_map, angle, expand=False)\n",
    "    return rotated_tomogram, rotated_segmentation_map\n",
    "\n",
    "\n",
    "# 平行移動\n",
    "# 指定された範囲でランダムに平行移動\n",
    "def translate_3d(tomogram, segmentation_map, max_shift):\n",
    "    \"\"\"Translates the 3D tensors by a random shift within max_shift.\"\"\"\n",
    "    shift_x = random.randint(-max_shift, max_shift)\n",
    "    shift_y = random.randint(-max_shift, max_shift)\n",
    "    translated_tomogram = TF.affine(\n",
    "        tomogram, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    translated_segmentation_map = TF.affine(\n",
    "        segmentation_map, angle=0, translate=(shift_x, shift_y), scale=1, shear=0\n",
    "    )\n",
    "    return translated_tomogram, translated_segmentation_map\n",
    "\n",
    "\n",
    "# フリップ\n",
    "# 縦横（上下左右）ランダムフリップ\n",
    "def flip_3d(tomogram, segmentation_map):\n",
    "    \"\"\"Randomly flips the 3D tensors along height or width.\"\"\"\n",
    "    if random.random() > 0.5:  # Horizontal flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-1])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-1])\n",
    "    if random.random() > 0.5:  # Vertical flip\n",
    "        tomogram = torch.flip(tomogram, dims=[-2])\n",
    "        segmentation_map = torch.flip(segmentation_map, dims=[-2])\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# クロッピング\n",
    "# 入力テンソルを中心またはランダムクロップで切り取る\n",
    "def crop_3d(tomogram, segmentation_map, crop_size):\n",
    "    \"\"\"Crops the 3D tensors to the specified crop_size.\"\"\"\n",
    "    _, depth, height, width = tomogram.size()\n",
    "    crop_d, crop_h, crop_w = crop_size\n",
    "\n",
    "    if crop_h > height or crop_w > width:\n",
    "        raise ValueError(\"Crop size cannot be larger than the original size.\")\n",
    "\n",
    "    start_h = random.randint(0, height - crop_h)  # Random starting position for height\n",
    "    start_w = random.randint(0, width - crop_w)  # Random starting position for width\n",
    "\n",
    "    cropped_tomogram = tomogram[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "    cropped_segmentation_map = segmentation_map[\n",
    "        :, :, start_h : start_h + crop_h, start_w : start_w + crop_w\n",
    "    ]\n",
    "\n",
    "    return cropped_tomogram, cropped_segmentation_map\n",
    "\n",
    "\n",
    "# Mixup\n",
    "# 2つのサンプルを線形補間して混合\n",
    "def mixup(tomogram, segmentation_map, alpha=0.4):\n",
    "    \"\"\"Applies mixup augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size = tomogram.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_tomogram = lam * tomogram + (1 - lam) * tomogram[index, :]\n",
    "    mixed_segmentation_map = (\n",
    "        lam * segmentation_map + (1 - lam) * segmentation_map[index, :]\n",
    "    )\n",
    "\n",
    "    return mixed_tomogram, mixed_segmentation_map\n",
    "\n",
    "\n",
    "# Cutmix\n",
    "# ランダム領域を切り取って別のサンプルに貼り付け\n",
    "def cutmix(tomogram, segmentation_map, alpha=1.0):\n",
    "    \"\"\"Applies cutmix augmentation to the batch.\"\"\"\n",
    "    lam = random.betavariate(alpha, alpha)\n",
    "    batch_size, depth, height, width = tomogram.size()\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    cx = random.randint(0, width)\n",
    "    cy = random.randint(0, height)\n",
    "    cw = int(width * (1 - lam))\n",
    "    ch = int(height * (1 - lam))\n",
    "\n",
    "    x1 = max(cx - cw // 2, 0)\n",
    "    x2 = min(cx + cw // 2, width)\n",
    "    y1 = max(cy - ch // 2, 0)\n",
    "    y2 = min(cy + ch // 2, height)\n",
    "\n",
    "    tomogram[:, :, y1:y2, x1:x2] = tomogram[index, :, y1:y2, x1:x2]\n",
    "    segmentation_map[:, :, y1:y2, x1:x2] = segmentation_map[index, :, y1:y2, x1:x2]\n",
    "\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# データ拡張の組み合わせ適用\n",
    "def augment_data(\n",
    "    tomogram,\n",
    "    segmentation_map,\n",
    "    crop_size=(16, 256, 256),\n",
    "    max_shift=10,\n",
    "    rotation_angle=30,\n",
    "    p=0.5,\n",
    "    mixup_alpha=0.4,\n",
    "    cutmix_alpha=1.0,\n",
    "):\n",
    "    \"\"\"Applies a combination of rotation, translation, flipping, cropping, mixup, and cutmix to the inputs with probabilities.\"\"\"\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = rotate_3d(\n",
    "            tomogram,\n",
    "            segmentation_map,\n",
    "            angle=random.uniform(-rotation_angle, rotation_angle),\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = translate_3d(\n",
    "            tomogram, segmentation_map, max_shift=max_shift\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = flip_3d(tomogram, segmentation_map)\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = crop_3d(\n",
    "            tomogram, segmentation_map, crop_size=crop_size\n",
    "        )\n",
    "    if random.random() < p:\n",
    "        tomogram, segmentation_map = mixup(\n",
    "            tomogram, segmentation_map, alpha=mixup_alpha\n",
    "        )\n",
    "    # if random.random() < p:\n",
    "    #     tomogram, segmentation_map = cutmix(\n",
    "    #         tomogram, segmentation_map, alpha=cutmix_alpha\n",
    "    #     )\n",
    "    return tomogram, segmentation_map\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# バッチサイズ6, 深さ16, 高さ320, 幅320のランダムテンソル\n",
    "tomogram = torch.rand((6, 16, 320, 320))\n",
    "segmentation_map = torch.randint(0, 2, (6, 16, 320, 320))  # ラベルは0または1\n",
    "\n",
    "# データ拡張の適用\n",
    "aug_tomogram, aug_segmentation_map = augment_data(tomogram, segmentation_map, p=0.7)\n",
    "print(\"Original shape:\", tomogram.shape)\n",
    "print(\"Augmented shape:\", aug_tomogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoderのパラメータを固定\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    #  weight=torch.tensor([2.0, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    ")\n",
    "# criterion = DiceLoss()\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=CFG.epochs * len(train_loader),\n",
    "    # * batch_size,\n",
    ")\n",
    "scaler = GradScaler()\n",
    "seg_loss = SegmentationLoss(criterion)\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 640, 640])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padf = PadToSize(CFG.resolution)\n",
    "padf(normalized_tomogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 [Training]: 100%|██████████| 280/280 [05:08<00:00,  1.10s/it, loss=0.9000]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.05779614692353842\n",
      "train-epoch-loss:0.9000 valid-beta4-score:0.0578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/150 [Training]: 100%|██████████| 280/280 [05:14<00:00,  1.12s/it, loss=0.3244]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.07596067917783735\n",
      "train-epoch-loss:0.3244 valid-beta4-score:0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/150 [Training]: 100%|██████████| 280/280 [05:11<00:00,  1.11s/it, loss=0.2375]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.09523809523809523\n",
      "train-epoch-loss:0.2375 valid-beta4-score:0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/150 [Training]: 100%|██████████| 280/280 [05:23<00:00,  1.15s/it, loss=0.2026]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.11406206318143695\n",
      "train-epoch-loss:0.2026 valid-beta4-score:0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/150 [Training]: 100%|██████████| 280/280 [05:23<00:00,  1.15s/it, loss=0.2061]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.10476190476190476\n",
      "train-epoch-loss:0.2061 valid-beta4-score:0.1048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/150 [Training]: 100%|██████████| 280/280 [05:15<00:00,  1.13s/it, loss=0.1949]\n",
      "100%|██████████| 15/15 [00:31<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6714285714285715 score 0.1107041107041107\n",
      "train-epoch-loss:0.1949 valid-beta4-score:0.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/150 [Training]: 100%|██████████| 280/280 [05:07<00:00,  1.10s/it, loss=0.1907]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7285714285714286 score 0.10793650793650791\n",
      "train-epoch-loss:0.1907 valid-beta4-score:0.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/150 [Training]: 100%|██████████| 280/280 [05:13<00:00,  1.12s/it, loss=0.1914]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6714285714285715 score 0.10154305624688899\n",
      "train-epoch-loss:0.1914 valid-beta4-score:0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/150 [Training]: 100%|██████████| 280/280 [05:21<00:00,  1.15s/it, loss=0.1917]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.09738632295023274\n",
      "train-epoch-loss:0.1917 valid-beta4-score:0.0974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/150 [Training]: 100%|██████████| 280/280 [05:30<00:00,  1.18s/it, loss=0.1913]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.1101511879049676\n",
      "train-epoch-loss:0.1913 valid-beta4-score:0.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/150 [Training]: 100%|██████████| 280/280 [05:18<00:00,  1.14s/it, loss=0.1734]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.11621384750219106\n",
      "train-epoch-loss:0.1734 valid-beta4-score:0.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/150 [Training]: 100%|██████████| 280/280 [05:23<00:00,  1.16s/it, loss=0.1830]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.10455689124965052\n",
      "train-epoch-loss:0.1830 valid-beta4-score:0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/150 [Training]: 100%|██████████| 280/280 [05:17<00:00,  1.14s/it, loss=0.1769]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.13209802749551702\n",
      "train-epoch-loss:0.1769 valid-beta4-score:0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1728]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.10981366459627329\n",
      "train-epoch-loss:0.1728 valid-beta4-score:0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/150 [Training]: 100%|██████████| 280/280 [05:00<00:00,  1.07s/it, loss=0.1711]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.0640542577241899\n",
      "train-epoch-loss:0.1711 valid-beta4-score:0.0641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/150 [Training]: 100%|██████████| 280/280 [05:09<00:00,  1.11s/it, loss=0.1661]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.09607044289656985\n",
      "train-epoch-loss:0.1661 valid-beta4-score:0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/150 [Training]: 100%|██████████| 280/280 [05:23<00:00,  1.16s/it, loss=0.1727]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.12235550708833151\n",
      "train-epoch-loss:0.1727 valid-beta4-score:0.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1649]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.12305727059825419\n",
      "train-epoch-loss:0.1649 valid-beta4-score:0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/150 [Training]: 100%|██████████| 280/280 [05:21<00:00,  1.15s/it, loss=0.1656]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.12402918333725584\n",
      "train-epoch-loss:0.1656 valid-beta4-score:0.1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/150 [Training]: 100%|██████████| 280/280 [05:23<00:00,  1.16s/it, loss=0.1661]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.27142857142857146 score 0.15740094653507217\n",
      "train-epoch-loss:0.1661 valid-beta4-score:0.1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/150 [Training]: 100%|██████████| 280/280 [05:12<00:00,  1.12s/it, loss=0.1560]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.3285714285714286 score 0.1872463768115942\n",
      "train-epoch-loss:0.1560 valid-beta4-score:0.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/150 [Training]: 100%|██████████| 280/280 [05:20<00:00,  1.14s/it, loss=0.1614]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.44285714285714284 score 0.15907981150604456\n",
      "train-epoch-loss:0.1614 valid-beta4-score:0.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1620]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.44285714285714284 score 0.16642570113869817\n",
      "train-epoch-loss:0.1620 valid-beta4-score:0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/150 [Training]: 100%|██████████| 280/280 [04:53<00:00,  1.05s/it, loss=0.1637]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6142857142857143 score 0.19242426977107963\n",
      "train-epoch-loss:0.1637 valid-beta4-score:0.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/150 [Training]: 100%|██████████| 280/280 [05:08<00:00,  1.10s/it, loss=0.1522]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.5571428571428572 score 0.20637497473611463\n",
      "train-epoch-loss:0.1522 valid-beta4-score:0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/150 [Training]: 100%|██████████| 280/280 [05:09<00:00,  1.10s/it, loss=0.1549]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6142857142857143 score 0.25606020448321004\n",
      "train-epoch-loss:0.1549 valid-beta4-score:0.2561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/150 [Training]: 100%|██████████| 280/280 [05:05<00:00,  1.09s/it, loss=0.1516]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.24841116330478033\n",
      "train-epoch-loss:0.1516 valid-beta4-score:0.2484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/150 [Training]: 100%|██████████| 280/280 [05:01<00:00,  1.08s/it, loss=0.1776]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6714285714285715 score 0.24173219120046543\n",
      "train-epoch-loss:0.1776 valid-beta4-score:0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/150 [Training]: 100%|██████████| 280/280 [05:14<00:00,  1.12s/it, loss=0.1569]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.5 score 0.28056519041809536\n",
      "train-epoch-loss:0.1569 valid-beta4-score:0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/150 [Training]: 100%|██████████| 280/280 [05:20<00:00,  1.15s/it, loss=0.1650]\n",
      "100%|██████████| 15/15 [00:35<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.6142857142857143 score 0.23600907029478457\n",
      "train-epoch-loss:0.1650 valid-beta4-score:0.2360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/150 [Training]: 100%|██████████| 280/280 [05:21<00:00,  1.15s/it, loss=0.1490]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.24881623921517976\n",
      "train-epoch-loss:0.1490 valid-beta4-score:0.2488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/150 [Training]: 100%|██████████| 280/280 [05:04<00:00,  1.09s/it, loss=0.1619]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.20369398580346512\n",
      "train-epoch-loss:0.1619 valid-beta4-score:0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/150 [Training]: 100%|██████████| 280/280 [05:05<00:00,  1.09s/it, loss=0.1528]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.23319508123569996\n",
      "train-epoch-loss:0.1528 valid-beta4-score:0.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/150 [Training]: 100%|██████████| 280/280 [05:14<00:00,  1.12s/it, loss=0.1540]\n",
      "100%|██████████| 15/15 [00:37<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.27372237027478874\n",
      "train-epoch-loss:0.1540 valid-beta4-score:0.2737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/150 [Training]: 100%|██████████| 280/280 [05:12<00:00,  1.12s/it, loss=0.1470]\n",
      "100%|██████████| 15/15 [00:37<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.19846234591542908\n",
      "train-epoch-loss:0.1470 valid-beta4-score:0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/150 [Training]: 100%|██████████| 280/280 [05:20<00:00,  1.14s/it, loss=0.1468]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.9 score 0.2598643440243909\n",
      "train-epoch-loss:0.1468 valid-beta4-score:0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/150 [Training]: 100%|██████████| 280/280 [05:13<00:00,  1.12s/it, loss=0.1554]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7857142857142857 score 0.27888032519670775\n",
      "train-epoch-loss:0.1554 valid-beta4-score:0.2789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/150 [Training]: 100%|██████████| 280/280 [05:17<00:00,  1.13s/it, loss=0.1513]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.7285714285714286 score 0.2684903748733536\n",
      "train-epoch-loss:0.1513 valid-beta4-score:0.2685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/150 [Training]: 100%|██████████| 280/280 [04:43<00:00,  1.01s/it, loss=0.1397]\n",
      "100%|██████████| 15/15 [00:36<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0.8428571428571429 score 0.2850301866288899\n",
      "train-epoch-loss:0.1397 valid-beta4-score:0.2850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/150 [Training]: 100%|██████████| 280/280 [05:07<00:00,  1.10s/it, loss=nan]   \n",
      "100%|██████████| 15/15 [00:33<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:nan valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1422]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1422 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/150 [Training]: 100%|██████████| 280/280 [05:13<00:00,  1.12s/it, loss=0.1589]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1589 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/150 [Training]: 100%|██████████| 280/280 [05:13<00:00,  1.12s/it, loss=0.1432]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1432 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/150 [Training]: 100%|██████████| 280/280 [04:52<00:00,  1.04s/it, loss=0.1454]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1454 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/150 [Training]: 100%|██████████| 280/280 [05:09<00:00,  1.11s/it, loss=0.1470]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1470 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/150 [Training]: 100%|██████████| 280/280 [05:20<00:00,  1.14s/it, loss=0.1392]\n",
      "100%|██████████| 15/15 [00:31<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1392 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1399]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1399 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/150 [Training]: 100%|██████████| 280/280 [05:27<00:00,  1.17s/it, loss=0.1403]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1403 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/150 [Training]: 100%|██████████| 280/280 [05:14<00:00,  1.12s/it, loss=0.1415]\n",
      "100%|██████████| 15/15 [00:31<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1415 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/150 [Training]: 100%|██████████| 280/280 [05:17<00:00,  1.13s/it, loss=0.1346]\n",
      "100%|██████████| 15/15 [00:31<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1346 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/150 [Training]: 100%|██████████| 280/280 [05:14<00:00,  1.12s/it, loss=0.1451]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1451 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/150 [Training]: 100%|██████████| 280/280 [05:02<00:00,  1.08s/it, loss=0.1385]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1385 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/150 [Training]: 100%|██████████| 280/280 [05:12<00:00,  1.11s/it, loss=0.1438]\n",
      "100%|██████████| 15/15 [00:38<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1438 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/150 [Training]: 100%|██████████| 280/280 [05:22<00:00,  1.15s/it, loss=0.1291]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1291 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/150 [Training]: 100%|██████████| 280/280 [05:09<00:00,  1.11s/it, loss=0.1359]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1359 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/150 [Training]: 100%|██████████| 280/280 [05:18<00:00,  1.14s/it, loss=0.1281]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1281 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/150 [Training]: 100%|██████████| 280/280 [05:15<00:00,  1.13s/it, loss=0.1300]\n",
      "100%|██████████| 15/15 [00:34<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1300 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/150 [Training]: 100%|██████████| 280/280 [05:03<00:00,  1.08s/it, loss=0.1359]\n",
      "100%|██████████| 15/15 [00:31<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1359 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/150 [Training]: 100%|██████████| 280/280 [05:22<00:00,  1.15s/it, loss=0.1213]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1213 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/150 [Training]: 100%|██████████| 280/280 [05:07<00:00,  1.10s/it, loss=0.1280]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1280 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/150 [Training]: 100%|██████████| 280/280 [05:19<00:00,  1.14s/it, loss=0.1289]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1289 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/150 [Training]: 100%|██████████| 280/280 [05:12<00:00,  1.12s/it, loss=0.1318]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1318 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1168]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1168 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/150 [Training]: 100%|██████████| 280/280 [05:16<00:00,  1.13s/it, loss=0.1301]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1301 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/150 [Training]: 100%|██████████| 280/280 [05:17<00:00,  1.13s/it, loss=0.1226]\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1226 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/150 [Training]: 100%|██████████| 280/280 [05:15<00:00,  1.13s/it, loss=0.1231]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 0 score -100\n",
      "train-epoch-loss:0.1231 valid-beta4-score:-100.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/150 [Training]: 100%|██████████| 280/280 [05:12<00:00,  1.12s/it, loss=0.1367]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_constant = 0\n",
    "best_score = -100\n",
    "\n",
    "grand_train_loss = []\n",
    "grand_valid_loss = []\n",
    "grand_train_score = []\n",
    "grand_valid_score = []\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Training]\") as tq:\n",
    "        for data in tq:\n",
    "            normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "            segmentation_map = data[\"segmentation_map\"]\n",
    "            zarr_embedding_idx = data[\"zarr_type_embedding_idx\"]\n",
    "\n",
    "            normalized_tomogram = padf(normalized_tomogram)\n",
    "            segmentation_map = padf(segmentation_map)\n",
    "\n",
    "            # データ拡張\n",
    "            normalized_tomogram, segmentation_map = augment_data(\n",
    "                normalized_tomogram, segmentation_map, p=CFG.augmentation_prob\n",
    "            )\n",
    "            normalized_tomogram = normalized_tomogram.cuda()\n",
    "            segmentation_map = segmentation_map.long().cuda()\n",
    "            zarr_embedding_idx = zarr_embedding_idx.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(preprocess_tensor(normalized_tomogram), zarr_embedding_idx)\n",
    "                loss = seg_loss(pred, segmentation_map)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # 確率予測\n",
    "            prob_pred = torch.softmax(pred, dim=1)\n",
    "            tq.set_postfix({\"loss\": f\"{np.mean(train_loss):.4f}\"})\n",
    "\n",
    "    del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # with tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{CFG.epochs} [Validation]\") as tq:\n",
    "    #     for data in tq:\n",
    "    #         normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "    #         segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "    #         zarr_embedding_idx = data[\"zarr_type_embedding_idx\"].cuda()\n",
    "\n",
    "    #         normalized_tomogram = padf(normalized_tomogram)\n",
    "    #         segmentation_map = padf(segmentation_map)\n",
    "\n",
    "    #         with autocast():\n",
    "    #             pred = model(preprocess_tensor(normalized_tomogram), zarr_embedding_idx)\n",
    "    #             loss = seg_loss(pred, segmentation_map)\n",
    "    #         valid_loss.append(loss.item())\n",
    "\n",
    "    #         # 確率予測\n",
    "    #         prob_pred = torch.softmax(pred, dim=1)\n",
    "    #         tq.set_postfix({\"loss\": f\"{np.mean(valid_loss):.4f}\"})\n",
    "\n",
    "    # del normalized_tomogram, segmentation_map, zarr_embedding_idx, pred, loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    # モデルの保存\n",
    "    torch.save(model.state_dict(), \"./pretrained_model.pth\")\n",
    "\n",
    "    # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    train_inferenced_array = {}\n",
    "    train_pred_array = []\n",
    "    train_gt_array = []\n",
    "    valid_inferenced_array = {}\n",
    "    valid_gt_array = []\n",
    "\n",
    "    # for exp_name in tqdm(CFG.train_exp_names):\n",
    "    for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "        # inferenced_array = inference(model, exp_name, train=False)\n",
    "        inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "            model, exp_name, train=False\n",
    "        )\n",
    "        valid_inferenced_array[exp_name] = inferenced_array\n",
    "        base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "        gt_df = create_gt_df(base_dir, [exp_name])\n",
    "        valid_gt_array.append(gt_df)\n",
    "\n",
    "    valid_gt_array = pd.concat(valid_gt_array)\n",
    "\n",
    "    b_constant = 0\n",
    "    b_score = -100\n",
    "    for constant in tqdm(np.linspace(0.1, 0.9, 15)):\n",
    "        valid_pred_array = []\n",
    "        sikii = {\n",
    "            \"apo-ferritin\": constant,\n",
    "            \"beta-amylase\": constant,\n",
    "            \"beta-galactosidase\": constant,\n",
    "            \"ribosome\": constant,\n",
    "            \"thyroglobulin\": constant,\n",
    "            \"virus-like-particle\": constant,\n",
    "        }\n",
    "        for exp_name in [CFG.valid_exp_name]:  # 5つのデータで試す\n",
    "            pred_df = inference2pos(\n",
    "                pred_segmask=valid_inferenced_array[exp_name],\n",
    "                exp_name=exp_name,\n",
    "                sikii_dict=sikii,\n",
    "            )\n",
    "            valid_pred_array.append(pred_df)\n",
    "\n",
    "        valid_pred_array = pd.concat(valid_pred_array)\n",
    "\n",
    "        if len(valid_pred_array) != 0:\n",
    "            score_ = score(\n",
    "                valid_pred_array,\n",
    "                valid_gt_array,\n",
    "                row_id_column_name=\"index\",\n",
    "                distance_multiplier=1.0,\n",
    "                beta=4,\n",
    "            )\n",
    "            if score_ > b_score:\n",
    "                b_score = score_\n",
    "                b_constant = constant\n",
    "\n",
    "        import gc\n",
    "        import torch.cuda as cuda\n",
    "\n",
    "        # del valid_pred_array, valid_gt_array\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "\n",
    "    print(\"constant\", b_constant, \"score\", b_score)\n",
    "\n",
    "    if b_score > best_score:\n",
    "        best_constant = b_constant\n",
    "        best_score = b_score\n",
    "        # best_score = np.mean(valid_mean_scores)\n",
    "        best_model = model.state_dict()\n",
    "        torch.save(best_model, f\"./best_model.pth\")\n",
    "\n",
    "    print(\n",
    "        f\"train-epoch-loss:{np.mean(train_loss):.4f}\",\n",
    "        # f\"valid-epoch-loss:{np.mean(valid_loss):.4f}\",\n",
    "        # f\"train-beta4-score:{np.mean(train_mean_scores):.4f}\",\n",
    "        f\"valid-beta4-score:{b_score:.4f}\",\n",
    "    )\n",
    "\n",
    "    grand_train_loss.append(np.mean(train_loss))\n",
    "    # grand_valid_loss.append(np.mean(valid_loss))\n",
    "    # grand_train_score.append(np.mean(train_mean_scores))\n",
    "    grand_valid_score.append(b_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lossとvalid_lossのプロット\n",
    "\n",
    "plt.plot(grand_train_loss, label=\"train_loss\")\n",
    "plt.plot(grand_valid_loss, label=\"valid_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scoreとvalid_scoreのプロット\n",
    "plt.plot(grand_train_score, label=\"train_score\")\n",
    "plt.plot(grand_valid_score, label=\"valid_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
