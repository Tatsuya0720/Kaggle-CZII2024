{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from metric import visualize_epoch_results\n",
    "from src.utils import save_images\n",
    "from src.metric import score, create_cls_pos, create_cls_pos_sikii, create_df\n",
    "from src.inference import inference, inference2pos\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../inputs/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padf = PadToSize(CFG.resolution)\n",
    "\n",
    "\n",
    "# def last_padding(tomogram, slice_size):\n",
    "#     # tomogram: (tensor)\n",
    "#     b, d, h, w = tomogram.shape\n",
    "#     last_padding = slice_size - d % slice_size\n",
    "#     if last_padding == slice_size:\n",
    "#         return tomogram\n",
    "#     else:\n",
    "#         return torch.cat(\n",
    "#             [tomogram, torch.zeros(b, last_padding, h, w).to(tomogram.device)], dim=1\n",
    "#         )\n",
    "\n",
    "\n",
    "# def preprocess_tensor(tensor):\n",
    "#     batch_size, depth, height, width = tensor.shape\n",
    "#     tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "# def inference(model, exp_name, train=True):\n",
    "#     dataset = EziiDataset(\n",
    "#         exp_names=[exp_name],\n",
    "#         base_dir=\"../../inputs/train/\",\n",
    "#         particles_name=CFG.particles_name,\n",
    "#         resolution=CFG.resolution,\n",
    "#         zarr_type=[\"denoised\"],\n",
    "#         train=train,\n",
    "#         slice=False,\n",
    "#     )\n",
    "#     res_array = CFG.original_img_shape[CFG.resolution]\n",
    "#     pred_array = np.zeros(\n",
    "#         (len(CFG.particles_name) + 1, res_array[0], res_array[1], res_array[2])\n",
    "#     )\n",
    "#     loader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "#     model.eval()\n",
    "#     # tq = tqdm(loader)\n",
    "#     for data in loader:  # 実験データ1つを取り出す\n",
    "#         for i in range(0, data[\"normalized_tomogram\"].shape[1], CFG.slice_):\n",
    "#             normalized_tomogram = data[\"normalized_tomogram\"][:, i : i + CFG.slice_]\n",
    "#             normalized_tomogram = last_padding(normalized_tomogram, CFG.slice_)\n",
    "#             normalized_tomogram = padf(normalized_tomogram)\n",
    "#             normalized_tomogram = preprocess_tensor(normalized_tomogram).to(\"cuda\")\n",
    "#             pred = model(normalized_tomogram)\n",
    "#             prob_pred = (\n",
    "#                 torch.softmax(pred, dim=1).detach().cpu().numpy()\n",
    "#             )  # torch.Size([1, 7, 32, 320, 320])\n",
    "#             range_ = min(i + CFG.slice_, res_array[0])\n",
    "#             hw_pad_diff = prob_pred.shape[-1] - res_array[-1]\n",
    "\n",
    "#             if i >= res_array[0]:\n",
    "#                 continue\n",
    "\n",
    "#             if range_ == res_array[0]:\n",
    "#                 pred_array[:, i:range_] += prob_pred[\n",
    "#                     0, :, : res_array[0] - i, :-hw_pad_diff, :-hw_pad_diff\n",
    "#                 ]\n",
    "#             else:\n",
    "#                 pred_array[:, i:range_] += prob_pred[\n",
    "#                     0, :, :range_, :-hw_pad_diff, :-hw_pad_diff\n",
    "#                 ]\n",
    "\n",
    "#         if train:\n",
    "#             segmentation_map = data[\"segmentation_map\"]\n",
    "#         else:\n",
    "#             segmentation_map = None\n",
    "\n",
    "#         normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "#     # tq.close()\n",
    "\n",
    "#     return pred_array, normalized_tomogram, segmentation_map  # (7, 92, 315, 315)\n",
    "\n",
    "\n",
    "# def inference2pos(pred_segmask, exp_name):\n",
    "#     import cc3d\n",
    "\n",
    "#     cls_pos = []\n",
    "#     Ascale_pos = []\n",
    "#     res2ratio = CFG.resolution2ratio\n",
    "\n",
    "#     for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "#         print(pred_cls, CFG.cls2particles[pred_cls])\n",
    "#         cc, P = cc3d.connected_components(pred_segmask == pred_cls, return_N=True)\n",
    "#         stats = cc3d.statistics(cc)\n",
    "\n",
    "#         for z, y, x in stats[\"centroids\"]:\n",
    "#             Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "#             Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "#             Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "#             cls_pos.append([pred_cls, z, y, x])\n",
    "#             Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "#     pred_original_df = create_df(Ascale_pos, exp_name)\n",
    "\n",
    "#     return pred_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [02:20<00:00,  4.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "  0%|          | 0/16 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 171, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [184, 630, 630] at entry 0 and [200, 630, 630] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# train_nshuffle_loader = DataLoader(\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     train_nshuffle_dataset,\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#     batch_size=1,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     num_workers=CFG.num_workers,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     53\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     54\u001b[0m     valid_dataset,\n\u001b[1;32m     55\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     62\u001b[0m     normalized_tomogram \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_tomogram\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     63\u001b[0m     segmentation_map \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 171, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/tatsuya/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [184, 630, 630] at entry 0 and [200, 630, 630] at entry 1\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    "    augmentation=False,\n",
    "    slice=False,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "# train_nshuffle_dataset = EziiDataset(\n",
    "#     exp_names=CFG.train_exp_names,\n",
    "#     base_dir=\"../../inputs/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.train_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    "    augmentation=False,\n",
    "    train=True,\n",
    "    slice=False,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tqdm(train_loader):\n",
    "    exp_names = data[\"exp_name\"]\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names, normalized_tomogram.shape, segmentation_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc3d\n",
    "\n",
    "all_pred = []\n",
    "\n",
    "for data in tqdm(train_dataset):\n",
    "    exp_name = data[\"exp_name\"]\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    # print(segmentation_map.shape)\n",
    "    gt = segmentation_map  # .numpy()\n",
    "\n",
    "    cls_pos = []\n",
    "    Ascale_pos = []\n",
    "    res2ratio = CFG.resolution2ratio\n",
    "    # exp_name = exp_names[i]\n",
    "\n",
    "    for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "        cc, P = cc3d.connected_components(gt == pred_cls, return_N=True)\n",
    "        stats = cc3d.statistics(cc)\n",
    "\n",
    "        for z, y, x in stats[\"centroids\"][1:]:\n",
    "            Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "            cls_pos.append([pred_cls, z, y, x])\n",
    "            Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "    pred_original_df = create_df(Ascale_pos, exp_name)\n",
    "    all_pred.append(pred_original_df)\n",
    "\n",
    "all_pred = pd.concat(all_pred).reset_index().drop_duplicates(subset=[\"x\", \"y\", \"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_df(base_dir, exp_names):\n",
    "    result_df = None\n",
    "    particle_names = CFG.particles_name\n",
    "\n",
    "    for exp_name in exp_names:\n",
    "        for particle in particle_names:\n",
    "            np_corrds = read_info_json(\n",
    "                base_dir=base_dir, exp_name=exp_name, particle_name=particle\n",
    "            )  # (n, 3)\n",
    "            # 各行にexp_nameとparticle_name追加\n",
    "            particle_df = pd.DataFrame(np_corrds, columns=[\"z\", \"y\", \"x\"])\n",
    "            particle_df[\"experiment\"] = exp_name\n",
    "            particle_df[\"particle_type\"] = particle\n",
    "\n",
    "            if result_df is None:\n",
    "                result_df = particle_df\n",
    "            else:\n",
    "                result_df = pd.concat([result_df, particle_df], axis=0).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "    result_df = result_df.reset_index()\n",
    "    result_df = result_df[[\"index\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "gt_df = create_gt_df(\"../../inputs/train/overlay/ExperimentRuns/\", CFG.train_exp_names)\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(all_pred, gt_df, row_id_column_name=\"index\", distance_multiplier=0.5, beta=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ############### validation ################\n",
    "train_nshuffle_original_tomogram = defaultdict(list)\n",
    "train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "train_cls_pos = defaultdict(list)\n",
    "train_cls_Apos = defaultdict(list)\n",
    "\n",
    "valid_original_tomogram = defaultdict(list)\n",
    "valid_pred_tomogram = defaultdict(list)\n",
    "valid_gt_tomogram = defaultdict(list)\n",
    "valid_cls_pos = defaultdict(list)\n",
    "valid_cls_Apos = defaultdict(list)\n",
    "\n",
    "train_mean_scores = []\n",
    "valid_mean_scores = []\n",
    "\n",
    "# # for exp_name in tqdm(CFG.train_exp_names):\n",
    "# for exp_name in tqdm(CFG.train_exp_names[:5]):  # 5つのデータで試す\n",
    "#     inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "#         model, exp_name, train=True\n",
    "#     )\n",
    "#     train_nshuffle_pred_tomogram[exp_name] = inferenced_array\n",
    "#     train_nshuffle_gt_tomogram[exp_name] = segmentation_map.squeeze(0)\n",
    "#     train_nshuffle_original_tomogram[exp_name] = n_tomogram.squeeze(0)\n",
    "\n",
    "#     mean_score, scores, pred_df, gt_df, pred_cls_pos, pred_Ascale_pos = (\n",
    "#         visualize_epoch_results(\n",
    "#             train_nshuffle_pred_tomogram,\n",
    "#             base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "#             sikii_dict=CFG.initial_sikii,\n",
    "#         )\n",
    "#     )\n",
    "#     train_cls_pos[exp_name] = pred_cls_pos\n",
    "#     train_cls_Apos[exp_name] = pred_Ascale_pos\n",
    "#     train_mean_scores.append(mean_score)\n",
    "# print(\"train_mean_scores\", np.mean(train_mean_scores))\n",
    "\n",
    "for exp_name in tqdm(CFG.valid_exp_names):\n",
    "    inferenced_array, n_tomogram, segmentation_map = inference(\n",
    "        model, exp_name, train=True\n",
    "    )\n",
    "    valid_pred_tomogram[exp_name] = inferenced_array\n",
    "    valid_gt_tomogram[exp_name] = segmentation_map.squeeze(0)\n",
    "    valid_original_tomogram[exp_name] = n_tomogram.squeeze(0)\n",
    "\n",
    "    mean_score, scores, pred_df, gt_df, pred_cls_pos, pred_Ascale_pos = (\n",
    "        visualize_epoch_results(\n",
    "            valid_pred_tomogram,\n",
    "            base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "            sikii_dict=CFG.initial_sikii,\n",
    "        )\n",
    "    )\n",
    "    valid_cls_pos[exp_name] = pred_cls_pos\n",
    "    valid_cls_Apos[exp_name] = pred_Ascale_pos\n",
    "    valid_mean_scores.append(mean_score)\n",
    "print(\"valid_mean_scores\", np.mean(valid_mean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_array, n_tomogram, segmentation_map = inference(model, exp_name, train=True)\n",
    "pred_original_df = inference2pos(\n",
    "    pred_segmask=inferenced_array,\n",
    "    exp_name=exp_name,\n",
    "    sikii_dict=CFG.initial_sikii,\n",
    ")\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference2pos(pred_segmask, exp_name, sikii_dict):\n",
    "    import cc3d\n",
    "\n",
    "    cls_pos = []\n",
    "    Ascale_pos = []\n",
    "    res2ratio = CFG.resolution2ratio\n",
    "\n",
    "    for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "        sikii = sikii_dict[CFG.cls2particles[pred_cls]]\n",
    "        # print(pred_segmask[pred_cls].shape)\n",
    "        cc, P = cc3d.connected_components(pred_segmask[pred_cls] > sikii, return_N=True)\n",
    "        # cc, P = cc3d.connected_components(pred_segmask == pred_cls, return_N=True)\n",
    "        stats = cc3d.statistics(cc)\n",
    "\n",
    "        for z, y, x in stats[\"centroids\"][1:]:\n",
    "            Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "            cls_pos.append([pred_cls, z, y, x])\n",
    "            Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "    pred_original_df = create_df(Ascale_pos, exp_name)\n",
    "\n",
    "    return pred_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(pred_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1.0, beta=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df[\"particle_type\"] == \"apo-ferritin\"].sort_values(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df[gt_df[\"particle_type\"] == \"apo-ferritin\"].sort_values(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "num_classes = len(CFG.particles_name)  # クラス数\n",
    "colors = plt.cm.tab10(\n",
    "    np.arange(len(CFG.particles_name))\n",
    ")  # \"tab10\" カラーマップから色を取得\n",
    "\n",
    "# ListedColormap を作成\n",
    "class_colormap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "def plot_with_colormap(data, title, original_tomogram):\n",
    "    masked_data = np.ma.masked_where(data <= 0, data)  # クラス0をマスク\n",
    "    plt.imshow(original_tomogram, cmap=\"gray\")\n",
    "    im = plt.imshow(masked_data, cmap=class_colormap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    return im\n",
    "\n",
    "\n",
    "def imshow_result(pred, gt, original, index):\n",
    "    # plt.figure(figsize=(20, 5))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plot_with_colormap(\n",
    "        pred[index],\n",
    "        \"Train-Prediction\",\n",
    "        original[index],\n",
    "    )\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    plot_with_colormap(gt[index], \"Gt\", original[index])\n",
    "\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    plt.imshow(original[index], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = \"TS_5_4\"\n",
    "# index = 12\n",
    "# pred = train_nshuffle_pred_tomogram[exp_name].argmax(0)  # (92, 315, 315)\n",
    "# gt = train_nshuffle_gt_tomogram[exp_name]\n",
    "# original = train_nshuffle_original_tomogram[exp_name]\n",
    "\n",
    "# # imshow_result(pred, gt, original, index)\n",
    "\n",
    "# for i in range(42):\n",
    "#     imshow_result(pred, gt, original, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = \"TS_5_4\"\n",
    "# index = 12\n",
    "# pred = train_nshuffle_pred_tomogram[exp_name].argmax(0)  # (92, 315, 315)\n",
    "# gt = train_nshuffle_gt_tomogram[exp_name]\n",
    "# original = train_nshuffle_original_tomogram[exp_name]\n",
    "\n",
    "# # imshow_result(pred, gt, original, index)\n",
    "\n",
    "# for i in range(42):\n",
    "#     imshow_result(pred, gt, original, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = CFG.valid_exp_names[-1]\n",
    "\n",
    "# pred = valid_pred_tomogram[exp_name].argmax(0)\n",
    "# gt = valid_gt_tomogram[exp_name]\n",
    "# original = valid_original_tomogram[exp_name]\n",
    "\n",
    "# for i in range(42):\n",
    "#     imshow_result(pred, gt, original, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = CFG.train_exp_names[-1]\n",
    "\n",
    "# pred_cls_pos = train_cls_pos[exp_name]\n",
    "\n",
    "# exp_name, np.array(pred_cls_pos).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = CFG.valid_exp_names[0]\n",
    "\n",
    "# pred_cls_pos = valid_cls_pos[exp_name]\n",
    "\n",
    "# np.array(pred_cls_pos).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(pred_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1.0, beta=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_df(base_dir, exp_names):\n",
    "    result_df = None\n",
    "    particle_names = CFG.particles_name\n",
    "\n",
    "    for exp_name in exp_names:\n",
    "        for particle in particle_names:\n",
    "            np_corrds = read_info_json(\n",
    "                base_dir=base_dir, exp_name=exp_name, particle_name=particle\n",
    "            )  # (n, 3)\n",
    "            # 各行にexp_nameとparticle_name追加\n",
    "            particle_df = pd.DataFrame(np_corrds, columns=[\"z\", \"y\", \"x\"])\n",
    "            particle_df[\"experiment\"] = exp_name\n",
    "            particle_df[\"particle_type\"] = particle\n",
    "\n",
    "            if result_df is None:\n",
    "                result_df = particle_df\n",
    "            else:\n",
    "                result_df = pd.concat([result_df, particle_df], axis=0).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "    result_df = result_df.reset_index()\n",
    "    result_df = result_df[[\"index\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = CFG.valid_exp_names[0]\n",
    "# pred = valid_pred_tomogram[exp_name].argmax(0)\n",
    "# gt = valid_gt_tomogram[exp_name]\n",
    "# original = valid_original_tomogram[exp_name]\n",
    "import timm\n",
    "\n",
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "exp_name = CFG.train_exp_names[2]\n",
    "pred = train_nshuffle_pred_tomogram[exp_name].argmax(0)\n",
    "gt = train_nshuffle_gt_tomogram[exp_name]\n",
    "original = train_nshuffle_original_tomogram[exp_name]\n",
    "\n",
    "base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "gt_df = create_gt_df(base_dir=base_dir, exp_names=[exp_name])\n",
    "\n",
    "import cc3d\n",
    "\n",
    "cls_pos = []\n",
    "Ascale_pos = []\n",
    "res2ratio = CFG.resolution2ratio\n",
    "\n",
    "for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "    print(pred_cls, CFG.cls2particles[pred_cls])\n",
    "    cc, P = cc3d.connected_components(pred == pred_cls, return_N=True)\n",
    "    stats = cc3d.statistics(cc)\n",
    "\n",
    "    for z, y, x in stats[\"centroids\"]:\n",
    "        Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "        Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "        Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "        cls_pos.append([pred_cls, z, y, x])\n",
    "        Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "pred_original_df = create_df(Ascale_pos, exp_name).drop_duplicates(\n",
    "    subset=[\"x\", \"y\", \"z\"]\n",
    ")\n",
    "\n",
    "score(\n",
    "    pred_original_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1.0, beta=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "exp_name = CFG.valid_exp_names[-1]\n",
    "pred = valid_pred_tomogram[exp_name]\n",
    "gt = valid_gt_tomogram[exp_name]\n",
    "original = valid_original_tomogram[exp_name]\n",
    "\n",
    "base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "gt_df = create_gt_df(base_dir=base_dir, exp_names=[exp_name])\n",
    "\n",
    "\n",
    "for constant in np.linspace(0.2, 0.9, 20):\n",
    "    initial_sikii = {\n",
    "        \"apo-ferritin\": constant,\n",
    "        \"beta-amylase\": constant,\n",
    "        \"beta-galactosidase\": constant,\n",
    "        \"ribosome\": constant,\n",
    "        \"thyroglobulin\": constant,\n",
    "        \"virus-like-particle\": constant,\n",
    "    }\n",
    "\n",
    "    import cc3d\n",
    "\n",
    "    cls_pos = []\n",
    "    Ascale_pos = []\n",
    "    res2ratio = CFG.resolution2ratio\n",
    "\n",
    "    for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "        sikii = initial_sikii[CFG.cls2particles[pred_cls]]\n",
    "        cc, P = cc3d.connected_components(pred[pred_cls] > sikii, return_N=True)\n",
    "        stats = cc3d.statistics(cc)\n",
    "\n",
    "        for z, y, x in stats[\"centroids\"]:\n",
    "            Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "            cls_pos.append([pred_cls, z, y, x])\n",
    "            Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "    pred_original_df = create_df(Ascale_pos, exp_name).drop_duplicates(\n",
    "        subset=[\"x\", \"y\", \"z\"]\n",
    "    )\n",
    "\n",
    "    score_ = score(\n",
    "        pred_original_df,\n",
    "        gt_df,\n",
    "        row_id_column_name=\"index\",\n",
    "        distance_multiplier=1.0,\n",
    "        beta=4,\n",
    "    )\n",
    "\n",
    "    print(sikii, score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "exp_name = CFG.valid_exp_names[0]\n",
    "pred = valid_pred_tomogram[exp_name]\n",
    "gt = valid_gt_tomogram[exp_name]\n",
    "original = valid_original_tomogram[exp_name]\n",
    "\n",
    "base_dir = \"../../inputs/train/overlay/ExperimentRuns/\"\n",
    "gt_df = create_gt_df(base_dir=base_dir, exp_names=[exp_name])\n",
    "\n",
    "\n",
    "for constant in np.linspace(0.2, 0.9, 20):\n",
    "    initial_sikii = {\n",
    "        \"apo-ferritin\": constant,\n",
    "        \"beta-amylase\": constant,\n",
    "        \"beta-galactosidase\": constant,\n",
    "        \"ribosome\": constant,\n",
    "        \"thyroglobulin\": constant,\n",
    "        \"virus-like-particle\": constant,\n",
    "    }\n",
    "\n",
    "    import cc3d\n",
    "\n",
    "    cls_pos = []\n",
    "    Ascale_pos = []\n",
    "    res2ratio = CFG.resolution2ratio\n",
    "\n",
    "    for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "        sikii = initial_sikii[CFG.cls2particles[pred_cls]]\n",
    "        cc, P = cc3d.connected_components(pred[pred_cls] > sikii, return_N=True)\n",
    "        stats = cc3d.statistics(cc)\n",
    "\n",
    "        for z, y, x in stats[\"centroids\"]:\n",
    "            Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "            Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "            cls_pos.append([pred_cls, z, y, x])\n",
    "            Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "    pred_original_df = create_df(Ascale_pos, exp_name).drop_duplicates(\n",
    "        subset=[\"x\", \"y\", \"z\"]\n",
    "    )\n",
    "\n",
    "    score_ = score(\n",
    "        pred_original_df,\n",
    "        gt_df,\n",
    "        row_id_column_name=\"index\",\n",
    "        distance_multiplier=1.0,\n",
    "        beta=4,\n",
    "    )\n",
    "\n",
    "    print(sikii, score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "exp_name = CFG.valid_exp_names[0]\n",
    "inferenced_array, n_tomogram, segmentation_map = inference(model, exp_name, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for constant in np.linspace(0.2, 0.9, 20):\n",
    "    initial_sikii = {\n",
    "        \"apo-ferritin\": constant,\n",
    "        \"beta-amylase\": constant,\n",
    "        \"beta-galactosidase\": constant,\n",
    "        \"ribosome\": constant,\n",
    "        \"thyroglobulin\": constant,\n",
    "        \"virus-like-particle\": constant,\n",
    "    }\n",
    "\n",
    "    pred_original_df = inference2pos(\n",
    "        pred_segmask=inferenced_array,\n",
    "        exp_name=exp_name,\n",
    "        sikii_dict=initial_sikii,\n",
    "    )\n",
    "    gt_df = create_gt_df(\n",
    "        base_dir=\"../../inputs/train/overlay/ExperimentRuns/\", exp_names=[exp_name]\n",
    "    )\n",
    "\n",
    "    s = score(\n",
    "        pred_original_df,\n",
    "        gt_df,\n",
    "        row_id_column_name=\"index\",\n",
    "        distance_multiplier=1.0,\n",
    "        beta=4,\n",
    "    )\n",
    "    print(constant, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from scipy.ndimage.morphology import generate_binary_structure, binary_erosion\n",
    "import matplotlib.pyplot as pp\n",
    "\n",
    "\n",
    "def detect_peaks(image, cls_index):\n",
    "    \"\"\"\n",
    "    Takes an image and detect the peaks usingthe local maximum filter.\n",
    "    Returns a boolean mask of the peaks (i.e. 1 when\n",
    "    the pixel's value is the neighborhood maximum, 0 otherwise)\n",
    "    \"\"\"\n",
    "\n",
    "    # define an 8-connected neighborhood\n",
    "    neighborhood = generate_binary_structure(2, 2)\n",
    "\n",
    "    # apply the local maximum filter; all pixel of maximal value\n",
    "    # in their neighborhood are set to 1\n",
    "    local_max = maximum_filter(image, footprint=neighborhood) == image\n",
    "    # local_max is a mask that contains the peaks we are\n",
    "    # looking for, but also the background.\n",
    "    # In order to isolate the peaks we must remove the background from the mask.\n",
    "\n",
    "    # we create the mask of the background\n",
    "    background = image == 0\n",
    "\n",
    "    # a little technicality: we must erode the background in order to\n",
    "    # successfully subtract it form local_max, otherwise a line will\n",
    "    # appear along the background border (artifact of the local maximum filter)\n",
    "    eroded_background = binary_erosion(\n",
    "        background, structure=neighborhood, border_value=cls_index\n",
    "    )\n",
    "\n",
    "    # we obtain the final mask, containing only peaks,\n",
    "    # by removing the background from the local_max mask (xor operation)\n",
    "    detected_peaks = local_max ^ eroded_background\n",
    "\n",
    "    return detected_peaks\n",
    "\n",
    "\n",
    "# test\n",
    "x = np.linspace(0, 4 * np.pi, 100)\n",
    "y = np.sin(x).reshape(-1, 10)\n",
    "# peaks = detect_peaks(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import (\n",
    "    binary_dilation,\n",
    "    binary_erosion,\n",
    "    binary_opening,\n",
    "    binary_closing,\n",
    ")\n",
    "\n",
    "\n",
    "def apply_morphology(\n",
    "    segmentation: np.ndarray, class_range: tuple[int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    セマンティックセグメンテーション結果にモルフォロジー処理を適用して鮮鋭化します。\n",
    "\n",
    "    Args:\n",
    "        segmentation (np.ndarray): セグメンテーション結果 (2Dまたは3D配列)。\n",
    "        class_range (tuple[int, int]): モルフォロジー処理対象のクラス範囲 (min_class, max_class)。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: モルフォロジー処理後のセグメンテーション結果。\n",
    "    \"\"\"\n",
    "    # 背景クラス (0) は処理対象外\n",
    "    processed_segmentation = np.zeros_like(segmentation)\n",
    "    for cls in range(class_range[0], class_range[1] + 1):\n",
    "        # 特定のクラスのバイナリマスクを作成\n",
    "        binary_mask = segmentation == cls\n",
    "\n",
    "        # モルフォロジー処理 (例: 開操作)\n",
    "        refined_mask = binary_opening(\n",
    "            binary_mask, structure=np.ones((3, 3))\n",
    "        )  # 3x3の構造要素を使用\n",
    "        refined_mask = binary_closing(\n",
    "            refined_mask, structure=np.ones((3, 3))\n",
    "        )  # 閉操作でギャップを埋める\n",
    "\n",
    "        # 処理後のマスクを反映\n",
    "        processed_segmentation[refined_mask] = cls\n",
    "\n",
    "    return processed_segmentation\n",
    "\n",
    "\n",
    "# 使用例\n",
    "# セグメンテーション結果 (例)\n",
    "segmentation_result = np.random.randint(\n",
    "    0, 8, size=(100, 100)\n",
    ")  # ランダムなセグメンテーション結果\n",
    "\n",
    "# クラス1~7を鮮鋭化\n",
    "processed_result = apply_morphology(segmentation_result, class_range=(1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import binary_opening, binary_closing, label\n",
    "\n",
    "\n",
    "def refine_segmentation(\n",
    "    segmentation: np.ndarray, class_range: tuple[int, int], min_size: int = 10\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    小さなクラス同士を分離したまま、セマンティックセグメンテーション結果を鮮鋭化します。\n",
    "\n",
    "    Args:\n",
    "        segmentation (np.ndarray): セグメンテーション結果 (2Dまたは3D配列)。\n",
    "        class_range (tuple[int, int]): 処理対象のクラス範囲 (min_class, max_class)。\n",
    "        min_size (int): 小さい領域を除去する際の最小ピクセルサイズ。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 鮮鋭化されたセグメンテーション結果。\n",
    "    \"\"\"\n",
    "    processed_segmentation = np.zeros_like(segmentation)\n",
    "\n",
    "    for cls in range(class_range[0], class_range[1] + 1):\n",
    "        # 特定クラスのバイナリマスクを作成\n",
    "        binary_mask = segmentation == cls\n",
    "\n",
    "        # 開操作でノイズ除去、閉操作でギャップを埋める\n",
    "        refined_mask = binary_opening(binary_mask, structure=np.ones((1, 1)))\n",
    "        refined_mask = binary_closing(refined_mask, structure=np.ones((1, 1)))\n",
    "\n",
    "        # ラベリングで分離された領域を管理\n",
    "        labeled_mask, num_features = label(refined_mask)\n",
    "\n",
    "        # 小さい領域をフィルタリング\n",
    "        for region in range(1, num_features + 1):\n",
    "            region_mask = labeled_mask == region\n",
    "            if np.sum(region_mask) >= min_size:  # 最小サイズ以上の領域を保持\n",
    "                processed_segmentation[region_mask] = cls\n",
    "\n",
    "    return processed_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = inferenced_array.argmax(0)\n",
    "zeros_array = np.zeros_like(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(zeros_array.shape[0]):\n",
    "    zeros_array[i] = refine_segmentation(seg[i], class_range=(1, 6), min_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 45\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.imshow(inferenced_array.argmax(0)[i])\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(zeros_array[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 画像のサイズ\n",
    "image_size = 100  # 100x100の画像\n",
    "x = np.linspace(0, image_size - 1, image_size)\n",
    "y = np.linspace(0, image_size - 1, image_size)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# 2つの円の中心と半径\n",
    "circle1_center = (40, 50)  # (x, y)座標\n",
    "circle2_center = (60, 50)\n",
    "radius = 20\n",
    "\n",
    "# 2つの円を定義\n",
    "circle1 = ((X - circle1_center[0]) ** 2 + (Y - circle1_center[1]) ** 2) <= radius**2\n",
    "circle2 = ((X - circle2_center[0]) ** 2 + (Y - circle2_center[1]) ** 2) <= radius**2\n",
    "\n",
    "# 円の重なりを画像として作成\n",
    "image = np.zeros((image_size, image_size))\n",
    "image[circle1 | circle2] = 1  # 円がある部分を1にする\n",
    "\n",
    "# プロット\n",
    "plt.imshow(image, cmap=\"viridis\")\n",
    "plt.title(\"Overlapping Circles\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "def mean_pooling(binary_map, kernel_size=3):\n",
    "    \"\"\"\n",
    "    平均プーリングをバイナリマップに適用\n",
    "    :param binary_map: 入力バイナリマップ（2D numpy配列）\n",
    "    :param kernel_size: プーリングカーネルサイズ\n",
    "    :return: 平均プーリング後のマップ\n",
    "    \"\"\"\n",
    "    # 平均プーリングのためのカーネル\n",
    "    kernel = np.ones((kernel_size, kernel_size)) / (kernel_size**2)\n",
    "    print(kernel.shape)\n",
    "    return convolve2d(binary_map, kernel, mode=\"same\")\n",
    "\n",
    "\n",
    "def non_maximum_suppression(soft_mask):\n",
    "    \"\"\"\n",
    "    非最大抑制を適用し、局所最大を見つける\n",
    "    :param soft_mask: 入力マスク（2D numpy配列）\n",
    "    :return: 非最大抑制後のマスク\n",
    "    \"\"\"\n",
    "    max_mask = maximum_filter(soft_mask, size=3)  # 3x3の近傍で局所最大を取得\n",
    "    nms_mask = soft_mask == max_mask  # 局所最大のみを残す\n",
    "    return nms_mask * soft_mask  # 局所最大値以外をゼロにする\n",
    "\n",
    "\n",
    "# 入力画像の生成（2つの円が重なっている）\n",
    "image_size = 40\n",
    "x = np.linspace(0, image_size - 1, image_size)\n",
    "y = np.linspace(0, image_size - 1, image_size)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "circle1_center = (15, 20)\n",
    "circle2_center = (30, 20)\n",
    "radius = 10\n",
    "\n",
    "circle1 = ((X - circle1_center[0]) ** 2 + (Y - circle1_center[1]) ** 2) <= radius**2\n",
    "circle2 = ((X - circle2_center[0]) ** 2 + (Y - circle2_center[1]) ** 2) <= radius**2\n",
    "\n",
    "binary_map = np.zeros((image_size, image_size))\n",
    "binary_map[circle1 | circle2] = 1\n",
    "\n",
    "# MP-NMSの適用\n",
    "iterations = [0, 2, 4, 6, 8]  # 平均プーリングの反復回数\n",
    "kernel_size = 3  # プーリングカーネルサイズ\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, iteration in enumerate(iterations):\n",
    "    temp_map = binary_map.copy()\n",
    "    for _ in range(iteration):\n",
    "        print(temp_map.shape)\n",
    "        temp_map = mean_pooling(temp_map, kernel_size=kernel_size)\n",
    "\n",
    "    # 非最大抑制を適用\n",
    "    nms_map = non_maximum_suppression(temp_map)\n",
    "\n",
    "    # プロット\n",
    "    plt.subplot(2, len(iterations), i + 1)\n",
    "    plt.imshow(temp_map, cmap=\"viridis\")\n",
    "    plt.title(f\"MP num={iteration}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, len(iterations), len(iterations) + i + 1)\n",
    "    plt.imshow(nms_map, cmap=\"viridis\")\n",
    "    plt.title(f\"After NMS (num={iteration})\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 2, 3, 4, 5])  # 配列a\n",
    "v = np.array([0.2, 0.8])  # 配列v\n",
    "\n",
    "np.convolve(a, v, mode=\"same\")  # まずは'same'から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amax_infereced_array = valid_pred_tomogram[exp_name].argmax(0)\n",
    "masked_array = np.zeros_like(amax_infereced_array)\n",
    "\n",
    "z = amax_infereced_array.shape[0]\n",
    "for i in range(z):\n",
    "    iarray = amax_infereced_array[i, :, :]\n",
    "    # iarrayの0以外の部分は1にする\n",
    "    iarray[iarray != 0] = 1\n",
    "\n",
    "    binary_map = iarray\n",
    "    kernel_size = 2\n",
    "    iterations = 3\n",
    "\n",
    "    temp_map = binary_map.copy()\n",
    "    for _ in range(iterations):\n",
    "        temp_map = mean_pooling(temp_map, kernel_size=kernel_size)\n",
    "\n",
    "    nms_map = non_maximum_suppression(temp_map)\n",
    "\n",
    "    masked_array[i] = nms_map\n",
    "\n",
    "    if i == 24:\n",
    "        break\n",
    "\n",
    "masked_array = valid_pred_tomogram[exp_name].argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_segmask = masked_array\n",
    "\n",
    "import cc3d\n",
    "\n",
    "cls_pos = []\n",
    "Ascale_pos = []\n",
    "res2ratio = CFG.resolution2ratio\n",
    "\n",
    "for pred_cls in range(1, len(CFG.particles_name) + 1):\n",
    "    cc, P = cc3d.connected_components(pred_segmask == pred_cls, return_N=True)\n",
    "    stats = cc3d.statistics(cc)\n",
    "\n",
    "    for z, y, x in stats[\"centroids\"][1:]:\n",
    "        Ascale_z = z * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "        Ascale_x = x * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "        Ascale_y = y * res2ratio[CFG.resolution] / res2ratio[\"A\"]\n",
    "\n",
    "        cls_pos.append([pred_cls, z, y, x])\n",
    "        Ascale_pos.append([pred_cls, Ascale_z, Ascale_y, Ascale_x])\n",
    "\n",
    "pred_original_df = create_df(Ascale_pos, exp_name).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(\n",
    "    pred_original_df, gt_df, row_id_column_name=\"index\", distance_multiplier=1.0, beta=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
