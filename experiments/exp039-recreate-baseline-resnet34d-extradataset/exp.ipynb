{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import timm\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import Unet3D\n",
    "from src.utils import save_images, PadToSize\n",
    "from src.metric import (\n",
    "    score,\n",
    "    create_cls_pos,\n",
    "    create_cls_pos_sikii,\n",
    "    create_df,\n",
    "    SegmentationLoss,\n",
    "    DiceLoss,\n",
    ")\n",
    "from metric import visualize_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [01:00<00:00, 26.50it/s]\n",
      "  0%|          | 0/800 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 315, 315])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = EziiDataset(\n",
    "    exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.train_zarr_types,\n",
    "    train=True,\n",
    "    augmentation=True,\n",
    "    slice=True,\n",
    "    pre_read=True,\n",
    ")\n",
    "\n",
    "# train_nshuffle_dataset = EziiDataset(\n",
    "#     exp_names=CFG.train_exp_names,\n",
    "#     base_dir=\"../../inputs/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.train_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "# valid_dataset = EziiDataset(\n",
    "#     exp_names=CFG.valid_exp_names,\n",
    "#     base_dir=\"../../inputs/train/\",\n",
    "#     particles_name=CFG.particles_name,\n",
    "#     resolution=CFG.resolution,\n",
    "#     zarr_type=CFG.valid_zarr_types,\n",
    "#     augmentation=False,\n",
    "#     train=True,\n",
    "# )\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 315, 315])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=CFG.num_workers,\n",
    ")\n",
    "# train_nshuffle_loader = DataLoader(\n",
    "#     train_nshuffle_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=CFG.num_workers,\n",
    "# )\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    normalized_tomogram = data[\"normalized_tomogram\"]\n",
    "    segmentation_map = data[\"segmentation_map\"]\n",
    "    break\n",
    "\n",
    "normalized_tomogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = timm.create_model(\n",
    "    model_name=CFG.model_name,\n",
    "    pretrained=True,\n",
    "    in_chans=3,\n",
    "    num_classes=0,\n",
    "    global_pool=\"\",\n",
    "    features_only=True,\n",
    ")\n",
    "model = Unet3D(encoder=encoder).to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"./pretrained_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"encoder\"と名のつくパラメータは学習しない\n",
    "# for layer, param in model.named_parameters():\n",
    "#     if \"encoder\" in layer:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# サンプルデータ\n",
    "num_classes = len(CFG.particles_name)  # クラス数\n",
    "colors = plt.cm.tab10(\n",
    "    np.arange(len(CFG.particles_name))\n",
    ")  # \"tab10\" カラーマップから色を取得\n",
    "\n",
    "# ListedColormap を作成\n",
    "class_colormap = ListedColormap(colors)\n",
    "\n",
    "\n",
    "# カラーバー付きプロット\n",
    "def plot_with_colormap(data, title, original_tomogram):\n",
    "    masked_data = np.ma.masked_where(data <= 0, data)  # クラス0をマスク\n",
    "    plt.imshow(original_tomogram, cmap=\"gray\")\n",
    "    im = plt.imshow(masked_data, cmap=class_colormap)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay\n",
    ")\n",
    "# criterion = nn.CrossEntropyLoss(\n",
    "#     #  weight=torch.tensor([2.0, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    "# )\n",
    "criterion = DiceLoss()\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,\n",
    "    num_training_steps=CFG.epochs * len(train_loader),\n",
    "    # * batch_size,\n",
    ")\n",
    "seg_loss = SegmentationLoss(criterion)\n",
    "padf = PadToSize(CFG.resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b, c, d, h, w = CFG.batch_size, 1, 96, 320, 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    batch_size, depth, height, width = tensor.shape\n",
    "    tensor = tensor.unsqueeze(2)  # (b, d, h, w) -> (b, d, 1, h, w)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 320, 320])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padf = PadToSize(CFG.resolution)\n",
    "padf(normalized_tomogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_padding(tomogram, slice_size):\n",
    "    # tomogram: (tensor)\n",
    "    b, d, h, w = tomogram.shape\n",
    "    last_padding = slice_size - d % slice_size\n",
    "    if last_padding == slice_size:\n",
    "        return tomogram\n",
    "    else:\n",
    "        return torch.cat(\n",
    "            [tomogram, torch.zeros(b, last_padding, h, w).to(tomogram.device)], dim=1\n",
    "        )\n",
    "\n",
    "\n",
    "def inference(model, exp_name, train=True):\n",
    "    dataset = EziiDataset(\n",
    "        exp_names=[exp_name],\n",
    "        base_dir=\"../../inputs/train/\",\n",
    "        particles_name=CFG.particles_name,\n",
    "        resolution=CFG.resolution,\n",
    "        zarr_type=[\"denoised\"],\n",
    "        train=train,\n",
    "        slice=False,\n",
    "    )\n",
    "    res_array = CFG.original_img_shape[CFG.resolution]\n",
    "    pred_array = np.zeros(\n",
    "        (len(CFG.particles_name) + 1, res_array[0], res_array[1], res_array[2])\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "    model.eval()\n",
    "    # tq = tqdm(loader)\n",
    "    for data in loader:  # 実験データ1つを取り出す\n",
    "        for i in range(0, data[\"normalized_tomogram\"].shape[1], CFG.slice_):\n",
    "            normalized_tomogram = data[\"normalized_tomogram\"][:, i : i + CFG.slice_]\n",
    "            normalized_tomogram = last_padding(normalized_tomogram, CFG.slice_)\n",
    "            normalized_tomogram = padf(normalized_tomogram)\n",
    "            normalized_tomogram = preprocess_tensor(normalized_tomogram).to(\"cuda\")\n",
    "            pred = model(normalized_tomogram)\n",
    "            prob_pred = (\n",
    "                torch.softmax(pred, dim=1).detach().cpu().numpy()\n",
    "            )  # torch.Size([1, 7, 32, 320, 320])\n",
    "            range_ = min(i + CFG.slice_, res_array[0])\n",
    "            hw_pad_diff = prob_pred.shape[-1] - res_array[-1]\n",
    "\n",
    "            if i >= res_array[0]:\n",
    "                continue\n",
    "\n",
    "            if range_ == res_array[0]:\n",
    "                pred_array[:, i:range_] += prob_pred[\n",
    "                    0, :, : res_array[0] - i, :-hw_pad_diff, :-hw_pad_diff\n",
    "                ]\n",
    "            else:\n",
    "                pred_array[:, i:range_] += prob_pred[\n",
    "                    0, :, :range_, :-hw_pad_diff, :-hw_pad_diff\n",
    "                ]\n",
    "            # tq.update()\n",
    "    # tq.close()\n",
    "\n",
    "    return pred_array  # (7, 92, 315, 315)\n",
    "\n",
    "\n",
    "inferenced_array = inference(model, \"TS_6_6\", train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6624469801783561: 100%|██████████| 800/800 [02:17<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.6624469801783561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6405818329751491: 100%|██████████| 800/800 [02:23<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6405818329751491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6321039402484894: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.6321039402484894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.611375813037157: 100%|██████████| 800/800 [02:23<00:00,  5.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.611375813037157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6011395563185215: 100%|██████████| 800/800 [02:17<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.6011395563185215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6090522456914187: 100%|██████████| 800/800 [02:18<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.6090522456914187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6072986172884702: 100%|██████████| 800/800 [02:18<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.6072986172884702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6048934152722358: 100%|██████████| 800/800 [02:20<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.6048934152722358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5969574902951718: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.5969574902951718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.6002933179587125: 100%|██████████| 800/800 [02:20<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.6002933179587125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5963222941756249: 100%|██████████| 800/800 [02:22<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.5963222941756249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5943533488363028: 100%|██████████| 800/800 [02:24<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.5943533488363028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5889377856999636: 100%|██████████| 800/800 [02:18<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.5889377856999636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5827234892547131: 100%|██████████| 800/800 [02:18<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.5827234892547131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5799073057621718: 100%|██████████| 800/800 [02:20<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.5799073057621718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5910176834464074: 100%|██████████| 800/800 [02:23<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 0.5910176834464074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5811186078190803: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 0.5811186078190803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5769034108519554: 100%|██████████| 800/800 [02:19<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 0.5769034108519554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5717871566861867: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 0.5717871566861867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5672702055424452: 100%|██████████| 800/800 [02:18<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 0.5672702055424452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5713388593494892: 100%|██████████| 800/800 [02:18<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 0.5713388593494892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5661281104385852: 100%|██████████| 800/800 [02:18<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss: 0.5661281104385852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5659131701290607: 100%|██████████| 800/800 [02:20<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss: 0.5659131701290607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.561773831397295: 100%|██████████| 800/800 [02:22<00:00,  5.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Loss: 0.561773831397295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5598040994256734: 100%|██████████| 800/800 [02:19<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss: 0.5598040994256734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5591699589043856: 100%|██████████| 800/800 [02:22<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Loss: 0.5591699589043856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5502177231758832: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Loss: 0.5502177231758832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5533889973163605: 100%|██████████| 800/800 [02:20<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Loss: 0.5533889973163605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.5558807776123286: 100%|██████████| 800/800 [02:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Loss: 0.5558807776123286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-loss: 0.557206136641437:  91%|█████████▏| 730/800 [02:13<00:23,  3.00it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m segmentation_map \u001b[38;5;241m=\u001b[39m padf(segmentation_map)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_tomogram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m seg_loss(pred, segmentation_map)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp039-recreate-baseline-resnet34d-extradataset/src/network.py:33\u001b[0m, in \u001b[0;36mUnet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m input2d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (b*d, 3, h, w)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ic(input2d.shape)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[43mencode2d_timm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m last, decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m     41\u001b[0m     feature\u001b[38;5;241m=\u001b[39mencode[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     42\u001b[0m     skip\u001b[38;5;241m=\u001b[39mencode[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m     43\u001b[0m     depth_scaling\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ic(last.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/code/projects/kaggle/CryoET/experiments/exp039-recreate-baseline-resnet34d-extradataset/src/network.py:120\u001b[0m, in \u001b[0;36mencode2d_timm\u001b[0;34m(timm_encoder, input2d, batch_size, depth_scaler)\u001b[0m\n\u001b[1;32m    116\u001b[0m encoded\u001b[38;5;241m.\u001b[39mappend(l2_pooled_f)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# ic(l2_pooled_f.shape)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# ######## layer3 ########\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m layer03_output \u001b[38;5;241m=\u001b[39m \u001b[43mtimm_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml2_pooled_encoder_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# layer03_output => (b*depth//depth_scaling, c, h//8, w//8)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m l3_pooled_f, l3_pooled_encoder_input \u001b[38;5;241m=\u001b[39m aggregate_depth(\n\u001b[1;32m    123\u001b[0m     layer03_output, batch_size, depth_scaler[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    124\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/timm/models/resnet.py:101\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_block(x)\n\u001b[0;32m--> 101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa(x)\n\u001b[1;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/kaggle/lib/python3.9/site-packages/torch/nn/functional.py:1455\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = -100\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    tq = tqdm(train_loader)\n",
    "    for data in train_loader:\n",
    "        normalized_tomogram = data[\"normalized_tomogram\"].cuda()\n",
    "        segmentation_map = data[\"segmentation_map\"].long().cuda()\n",
    "\n",
    "        normalized_tomogram = padf(normalized_tomogram)\n",
    "        segmentation_map = padf(segmentation_map)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(preprocess_tensor(normalized_tomogram))\n",
    "        loss = seg_loss(pred, segmentation_map)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # 確率予測\n",
    "        prob_pred = torch.softmax(pred, dim=1)\n",
    "        tq.update()\n",
    "        tq.set_description(f\"train-loss: {np.mean(train_loss)}\")\n",
    "    tq.close()\n",
    "\n",
    "    print(f\"Epoch {epoch} Loss: {np.mean(train_loss)}\")\n",
    "\n",
    "    # # ############### validation ################\n",
    "    train_nshuffle_original_tomogram = defaultdict(list)\n",
    "    train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "    train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    valid_original_tomogram = defaultdict(list)\n",
    "    valid_pred_tomogram = defaultdict(list)\n",
    "    valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "    train_mean_scores = []\n",
    "    valid_mean_scores = []\n",
    "\n",
    "    # # for exp_name in tqdm(CFG.train_exp_names):\n",
    "    # for exp_name in tqdm(CFG.train_exp_names[:5]):  # 5つのデータで試す\n",
    "    #     inferenced_array = inference(model, exp_name, train=False)\n",
    "    #     train_nshuffle_pred_tomogram[exp_name] = inferenced_array\n",
    "\n",
    "    #     mean_score, scores = visualize_epoch_results(\n",
    "    #         train_nshuffle_pred_tomogram,\n",
    "    #         base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "    #         sikii_dict=CFG.initial_sikii,\n",
    "    #     )\n",
    "    #     train_mean_scores.append(mean_score)\n",
    "    # print(\"train_mean_scores\", np.mean(train_mean_scores))\n",
    "\n",
    "    # for exp_name in tqdm(CFG.valid_exp_names):\n",
    "    #     inferenced_array = inference(model, exp_name, train=False)\n",
    "    #     valid_pred_tomogram[exp_name] = inferenced_array\n",
    "\n",
    "    #     mean_score, scores = visualize_epoch_results(\n",
    "    #         valid_pred_tomogram,\n",
    "    #         base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "    #         sikii_dict=CFG.initial_sikii,\n",
    "    #     )\n",
    "    #     valid_mean_scores.append(mean_score)\n",
    "    # print(\"valid_mean_scores\", np.mean(valid_mean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     inferenced_array \u001b[38;5;241m=\u001b[39m inference(model, exp_name, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m     train_nshuffle_pred_tomogram[exp_name] \u001b[38;5;241m=\u001b[39m inferenced_array\n\u001b[0;32m---> 21\u001b[0m     mean_score, scores \u001b[38;5;241m=\u001b[39m visualize_epoch_results(\n\u001b[1;32m     22\u001b[0m         train_nshuffle_pred_tomogram,\n\u001b[1;32m     23\u001b[0m         base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../inputs/train/overlay/ExperimentRuns/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m         sikii_dict\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39minitial_sikii,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     train_mean_scores\u001b[38;5;241m.\u001b[39mappend(mean_score)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_mean_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(train_mean_scores))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# モデルの保存\n",
    "torch.save(model.state_dict(), \"./pretrained_model.pth\")\n",
    "\n",
    "# ############### validation ################\n",
    "train_nshuffle_original_tomogram = defaultdict(list)\n",
    "train_nshuffle_pred_tomogram = defaultdict(list)\n",
    "train_nshuffle_gt_tomogram = defaultdict(list)\n",
    "\n",
    "valid_original_tomogram = defaultdict(list)\n",
    "valid_pred_tomogram = defaultdict(list)\n",
    "valid_gt_tomogram = defaultdict(list)\n",
    "\n",
    "train_mean_scores = []\n",
    "valid_mean_scores = []\n",
    "\n",
    "# for exp_name in tqdm(CFG.train_exp_names):\n",
    "for exp_name in tqdm(CFG.train_exp_names):  # 5つのデータで試す\n",
    "    inferenced_array = inference(model, exp_name, train=False)\n",
    "    train_nshuffle_pred_tomogram[exp_name] = inferenced_array\n",
    "\n",
    "    mean_score, scores = visualize_epoch_results(\n",
    "        train_nshuffle_pred_tomogram,\n",
    "        base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "        sikii_dict=CFG.initial_sikii,\n",
    "    )\n",
    "    train_mean_scores.append(mean_score)\n",
    "print(\"train_mean_scores\", np.mean(train_mean_scores))\n",
    "\n",
    "for exp_name in tqdm(CFG.valid_exp_names):\n",
    "    inferenced_array = inference(model, exp_name, train=False)\n",
    "    valid_pred_tomogram[exp_name] = inferenced_array\n",
    "\n",
    "    mean_score, scores = visualize_epoch_results(\n",
    "        valid_pred_tomogram,\n",
    "        base_dir=\"../../inputs/train/overlay/ExperimentRuns/\",\n",
    "        sikii_dict=CFG.initial_sikii,\n",
    "    )\n",
    "    valid_mean_scores.append(mean_score)\n",
    "print(\"valid_mean_scores\", np.mean(valid_mean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_nshuffle_pred_tomogram[\"TS_5_4\"].argmax(0), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
