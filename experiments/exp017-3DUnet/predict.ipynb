{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.config import CFG\n",
    "from src.dataloader import (\n",
    "    read_zarr,\n",
    "    read_info_json,\n",
    "    scale_coordinates,\n",
    "    create_dataset,\n",
    "    create_segmentation_map,\n",
    "    EziiDataset,\n",
    "    drop_padding,\n",
    ")\n",
    "from src.network import UNet_2D, aug\n",
    "from src.utils import save_images\n",
    "from src.metric import score, create_cls_pos, create_cls_pos_sikii, create_df\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../inputs/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TS_86_3', 'denoised'), ('TS_6_6', 'denoised')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = EziiDataset(\n",
    "    exp_names=CFG.valid_exp_names,\n",
    "    # exp_names=CFG.train_exp_names,\n",
    "    base_dir=\"../../inputs/train/static\",\n",
    "    particles_name=CFG.particles_name,\n",
    "    resolution=CFG.resolution,\n",
    "    zarr_type=CFG.valid_zarr_types,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for row in tqdm(valid_loader):\n",
    "    normalized_tomogram = row[\"normalized_tomogram\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36468520154670364: : 184it [00:09, 20.13it/s]                    \n"
     ]
    }
   ],
   "source": [
    "model = UNet_2D().to(\"cuda\")\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\"))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor([0.5, 32, 32, 32, 32, 32, 32]).to(\"cuda\")\n",
    ")\n",
    "# criterion = DiceLoss()\n",
    "\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "batch_size = 4\n",
    "\n",
    "valid_loss = []\n",
    "valid_pred_tomogram = defaultdict(list)\n",
    "valid_gt_tomogram = defaultdict(list)\n",
    "model.eval()\n",
    "tq = tqdm(range(len(valid_loader) * normalized_tomogram.shape[0]))\n",
    "for data in valid_loader:\n",
    "    exp_name = data[\"exp_name\"][0]\n",
    "    tomogram = data[\"normalized_tomogram\"].to(\"cuda\")\n",
    "    segmentation_map = data[\"segmentation_map\"].to(\"cuda\").long()\n",
    "\n",
    "    for i in range(tomogram.shape[1]):\n",
    "        input_ = tomogram[:, i].unsqueeze(0)\n",
    "        gt = segmentation_map[:, i]\n",
    "\n",
    "        output = model(input_)\n",
    "        loss = criterion(output, gt)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "        tq.set_description(f\"Loss: {np.mean(valid_loss)}\")\n",
    "        tq.update(1)\n",
    "\n",
    "        valid_pred_tomogram[exp_name].append(output.cpu().detach().numpy())\n",
    "        valid_gt_tomogram[exp_name].append(gt.cpu().detach().numpy())\n",
    "tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TS_86_3', 'TS_6_6'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred_tomogram.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments: ['TS_86_3', 'TS_6_6']\n",
      "####################### valid-experiments: TS_86_3 #######################\n",
      "experiments: TS_86_3, score: 0.14467114162413539\n",
      "####################### valid-experiments: TS_6_6 #######################\n",
      "experiments: TS_6_6, score: 0.21106372794782077\n",
      "CV: 0.17786743478597808\n"
     ]
    }
   ],
   "source": [
    "# 各実験のスコアを計算\n",
    "cv_scores = []\n",
    "\n",
    "experiments = list(valid_pred_tomogram.keys())\n",
    "print(f\"experiments: {experiments}\")\n",
    "\n",
    "for exp_name in experiments:\n",
    "    print(\n",
    "        f\"####################### valid-experiments: {exp_name} #######################\"\n",
    "    )\n",
    "    # pred\n",
    "    # pred_tomogram = np.array(valid_pred_tomogram[exp_name]).argmax(2).squeeze(1)\n",
    "    # pred_tomogram = drop_padding(pred_tomogram, CFG.resolution)\n",
    "    # pred_cls_pos, pred_Ascale_pos = create_cls_pos(pred_tomogram)\n",
    "    # pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "    # pred_df = pred_df.reset_index()\n",
    "\n",
    "    constant = 0.5\n",
    "    sikii_dict = {\n",
    "        \"apo-ferritin\": constant,\n",
    "        \"beta-amylase\": constant,\n",
    "        \"beta-galactosidase\": constant,\n",
    "        \"ribosome\": constant,\n",
    "        \"thyroglobulin\": constant,\n",
    "        \"virus-like-particle\": constant,\n",
    "    }\n",
    "\n",
    "    # multi-cls-pred\n",
    "    pred_tomogram = np.array(valid_pred_tomogram[exp_name]).squeeze(1)\n",
    "    pred_tomogram = drop_padding(pred_tomogram, CFG.resolution)\n",
    "    pred_tomogram = np.exp(pred_tomogram) / np.exp(pred_tomogram).sum(1)[:, None]\n",
    "    pred_cls_pos, pred_Ascale_pos = create_cls_pos_sikii(\n",
    "        pred_tomogram, sikii_dict=sikii_dict\n",
    "    )\n",
    "    pred_df = create_df(pred_Ascale_pos, exp_name)\n",
    "    pred_df = pred_df.drop_duplicates(subset=[\"x\", \"y\", \"z\"], keep=\"first\")\n",
    "    pred_df = pred_df.reset_index()\n",
    "\n",
    "    # gt\n",
    "    gt_tomogram = np.array(valid_gt_tomogram[exp_name]).squeeze(1)\n",
    "    gt_tomogram = drop_padding(gt_tomogram, CFG.resolution)\n",
    "    gt_cls_pos, gt_Ascale_pos = create_cls_pos(gt_tomogram)\n",
    "    gt_df = create_df(gt_Ascale_pos, exp_name)\n",
    "\n",
    "    gt_df = gt_df.reset_index()\n",
    "\n",
    "    score_ = score(\n",
    "        solution=pred_df,\n",
    "        submission=gt_df,\n",
    "        row_id_column_name=\"index\",\n",
    "        distance_multiplier=0.5,\n",
    "        beta=4,\n",
    "    )\n",
    "    print(f\"experiments: {exp_name}, score: {score_}\")\n",
    "    cv_scores.append(score_)\n",
    "\n",
    "print(f\"CV: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
